#!/usr/bin/env python
# coding: utf-8
"""
prepare_cicids_dataset.py
=========================
å°† CIC-IDS2017 Payload-Bytes æ•°æ®è½¬æ¢ä¸ºä½œè€… Classification_Dataset æœŸæœ›çš„æ ¼å¼

è¾“å…¥: Payload_Bytes_File_*.parquet (å¤šä¸ªåˆ†ç‰‡)
è¾“å‡º: train.parquet, val.parquet, test.parquet

è¾“å‡ºæ ¼å¼:
- question: str, å›ºå®šé—®é¢˜
- context: str, åå…­è¿›åˆ¶ payload (æ¯4å­—ç¬¦ç”¨ç©ºæ ¼åˆ†éš”)
- class: int, æ•°å€¼æ ‡ç­¾
- type_q: str, ç±»åˆ«åç§°
"""

import os
import sys
import glob
import argparse
import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm
from sklearn.model_selection import train_test_split

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))


def bytes_to_hex(byte_array: np.ndarray, format_type: str = 'every4') -> list:
    """
    å°†å­—èŠ‚æ•°ç»„è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
    
    Args:
        byte_array: shape (n_samples, n_bytes)
        format_type: 'every4' æˆ– 'every2'
    
    Returns:
        hex_strings: åå…­è¿›åˆ¶å­—ç¬¦ä¸²åˆ—è¡¨
    """
    hex_strings = []
    for row in tqdm(byte_array, desc="è½¬æ¢åå…­è¿›åˆ¶", leave=False):
        hex_str = ''.join(f'{int(b):02x}' for b in row)
        if format_type == 'every4':
            hex_str = ' '.join(hex_str[i:i+4] for i in range(0, len(hex_str), 4))
        elif format_type == 'every2':
            hex_str = ' '.join(hex_str[i:i+2] for i in range(0, len(hex_str), 2))
        hex_strings.append(hex_str.strip())
    return hex_strings


def load_and_merge_parquet(data_dir: Path, pattern: str = "Payload_Bytes_File_*.parquet"):
    """åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰ parquet åˆ†ç‰‡"""
    parquet_files = sorted(glob.glob(str(data_dir / pattern)))
    if not parquet_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°åŒ¹é… {pattern} çš„æ–‡ä»¶ï¼Œç›®å½•: {data_dir}")
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®åˆ†ç‰‡")
    
    dfs = []
    for f in tqdm(parquet_files, desc="åŠ è½½åˆ†ç‰‡"):
        df_part = pd.read_parquet(f)
        dfs.append(df_part)
    
    df_full = pd.concat(dfs, ignore_index=True)
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ! æ€»æ ·æœ¬æ•°: {len(df_full):,}")
    return df_full


def get_label_column(df: pd.DataFrame) -> str:
    """è‡ªåŠ¨æ£€æµ‹æ ‡ç­¾åˆ—"""
    candidates = ['attack_label', 'Label', 'label', 'class']
    for col in candidates:
        if col in df.columns:
            return col
    
    # å°è¯•æ¨¡ç³ŠåŒ¹é…
    for col in df.columns:
        if 'label' in col.lower() or 'attack' in col.lower():
            return col
    
    raise ValueError("æœªæ‰¾åˆ°æ ‡ç­¾åˆ—")


def convert_dataset(
    df: pd.DataFrame,
    max_bytes: int = 64,
    format_type: str = 'every4',
    question: str = "Classify the network packet",
    sample_size: int = None,
    seed: int = 42
):
    """
    å°†åŸå§‹æ•°æ®è½¬æ¢ä¸ºä½œè€…æœŸæœ›çš„æ ¼å¼
    
    Args:
        df: åŸå§‹ DataFrame
        max_bytes: ä½¿ç”¨çš„æœ€å¤§ payload å­—èŠ‚æ•°
        format_type: åå…­è¿›åˆ¶æ ¼å¼ ('every4' æˆ– 'every2')
        question: å›ºå®šé—®é¢˜æ–‡æœ¬
        sample_size: é‡‡æ ·å¤§å° (None è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®)
        seed: éšæœºç§å­
    """
    # è·å–æ ‡ç­¾åˆ—
    label_col = get_label_column(df)
    print(f"ğŸ·ï¸ æ ‡ç­¾åˆ—: {label_col}")
    
    # é‡‡æ ·ï¼ˆå¦‚æœ‰å¿…è¦ï¼‰
    if sample_size and sample_size < len(df):
        print(f"ğŸ“Š åˆ†å±‚é‡‡æ ·: {sample_size:,} æ ·æœ¬")
        df = df.groupby(label_col, group_keys=False).apply(
            lambda x: x.sample(
                min(len(x), max(1, sample_size // df[label_col].nunique())),
                random_state=seed
            )
        )
        print(f"   é‡‡æ ·å: {len(df):,} æ ·æœ¬")
    
    # è·å– payload åˆ—
    payload_cols = sorted(
        [c for c in df.columns if c.startswith('payload_byte_')],
        key=lambda x: int(x.split('_')[-1])
    )[:max_bytes]
    
    if not payload_cols:
        raise ValueError("æœªæ‰¾åˆ° payload_byte_* åˆ—")
    
    print(f"ğŸ“¦ ä½¿ç”¨ {len(payload_cols)} ä¸ª payload å­—èŠ‚åˆ—")
    
    # æå–å­—èŠ‚æ•°æ®
    X_bytes = df[payload_cols].values.astype(np.uint8)
    y_labels = df[label_col].values
    
    # è½¬æ¢ä¸ºåå…­è¿›åˆ¶
    print(f"ğŸ”„ è½¬æ¢ä¸ºåå…­è¿›åˆ¶ (æ ¼å¼: {format_type})...")
    contexts = bytes_to_hex(X_bytes, format_type=format_type)
    
    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
    unique_labels = sorted(df[label_col].unique())
    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
    print(f"ğŸ“‹ ç±»åˆ«æ•°: {len(unique_labels)}")
    for label, idx in label_to_id.items():
        count = (y_labels == label).sum()
        print(f"   {label} -> {idx} ({count:,} æ ·æœ¬)")
    
    # æ„å»ºè¾“å‡º DataFrame
    result = pd.DataFrame({
        'question': question,
        'context': contexts,
        'class': [label_to_id[label] for label in y_labels],
        'type_q': y_labels
    })
    
    return result, label_to_id


def split_and_save(
    df: pd.DataFrame,
    output_dir: Path,
    test_size: float = 0.2,
    val_size: float = 0.2,
    seed: int = 42
):
    """åˆ’åˆ†æ•°æ®é›†å¹¶ä¿å­˜"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # åˆ’åˆ†è®­ç»ƒé›†å’Œä¸´æ—¶é›†
    train_df, temp_df = train_test_split(
        df, test_size=(test_size + val_size),
        stratify=df['class'], random_state=seed
    )
    
    # åˆ’åˆ†éªŒè¯é›†å’Œæµ‹è¯•é›†
    relative_val_size = val_size / (test_size + val_size)
    val_df, test_df = train_test_split(
        temp_df, test_size=(1 - relative_val_size),
        stratify=temp_df['class'], random_state=seed
    )
    
    # ä¿å­˜
    train_path = output_dir / "train.parquet"
    val_path = output_dir / "val.parquet"
    test_path = output_dir / "test.parquet"
    
    train_df.to_parquet(train_path, index=False)
    val_df.to_parquet(val_path, index=False)
    test_df.to_parquet(test_path, index=False)
    
    print(f"\nâœ… æ•°æ®é›†å·²ä¿å­˜:")
    print(f"   è®­ç»ƒé›†: {train_path} ({len(train_df):,} æ ·æœ¬)")
    print(f"   éªŒè¯é›†: {val_path} ({len(val_df):,} æ ·æœ¬)")
    print(f"   æµ‹è¯•é›†: {test_path} ({len(test_df):,} æ ·æœ¬)")
    
    return train_path, val_path, test_path


def main():
    parser = argparse.ArgumentParser(description="è½¬æ¢ CIC-IDS2017 æ•°æ®ä¸ºä½œè€…æ ¼å¼")
    parser.add_argument(
        "--input_dir", type=str,
        default="../data/CIC-IDS2017/Payload-Bytes",
        help="è¾“å…¥æ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--output_dir", type=str,
        default="../data/CIC-IDS2017/Classification",
        help="è¾“å‡ºæ•°æ®ç›®å½•"
    )
    parser.add_argument(
        "--max_bytes", type=int, default=64,
        help="ä½¿ç”¨çš„æœ€å¤§ payload å­—èŠ‚æ•°"
    )
    parser.add_argument(
        "--format", type=str, default="every4",
        choices=["every4", "every2", "noSpace"],
        help="åå…­è¿›åˆ¶æ ¼å¼"
    )
    parser.add_argument(
        "--sample_size", type=int, default=None,
        help="é‡‡æ ·å¤§å° (None è¡¨ç¤ºä½¿ç”¨å…¨éƒ¨æ•°æ®)"
    )
    parser.add_argument(
        "--seed", type=int, default=42,
        help="éšæœºç§å­"
    )
    parser.add_argument(
        "--dry_run", action="store_true",
        help="ä»…éªŒè¯æ•°æ®æ ¼å¼ï¼Œä¸ä¿å­˜"
    )
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("CIC-IDS2017 æ•°æ®æ ¼å¼è½¬æ¢")
    print("=" * 60)
    
    # åŠ è½½æ•°æ®
    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        sys.exit(1)
    
    df = load_and_merge_parquet(input_dir)
    
    # è½¬æ¢æ ¼å¼
    result_df, label_map = convert_dataset(
        df,
        max_bytes=args.max_bytes,
        format_type=args.format,
        sample_size=args.sample_size,
        seed=args.seed
    )
    
    # å±•ç¤ºç¤ºä¾‹
    print(f"\nğŸ“ æ•°æ®ç¤ºä¾‹:")
    print(result_df.head(3).to_string())
    
    if args.dry_run:
        print("\nğŸ” Dry run æ¨¡å¼ï¼Œè·³è¿‡ä¿å­˜")
        return
    
    # åˆ’åˆ†å¹¶ä¿å­˜
    split_and_save(result_df, args.output_dir, seed=args.seed)
    
    # ä¿å­˜æ ‡ç­¾æ˜ å°„
    import json
    label_map_path = Path(args.output_dir) / "label_map.json"
    with open(label_map_path, 'w') as f:
        json.dump(label_map, f, indent=2)
    print(f"   æ ‡ç­¾æ˜ å°„: {label_map_path}")
    
    print("\nğŸ‰ è½¬æ¢å®Œæˆ!")


if __name__ == "__main__":
    main()
