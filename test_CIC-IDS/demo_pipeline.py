#!/usr/bin/env python
"""
PCAP_encoder ç«¯åˆ°ç«¯æ¼”ç¤ºè„šæœ¬
===========================

æ­¤è„šæœ¬æ¼”ç¤º PCAP_encoder çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼š
1. æ•°æ®å‡†å¤‡ï¼šåŠ è½½æˆ–ç”Ÿæˆæµ‹è¯•æ•°æ®
2. æ–‡æœ¬æ„å»ºï¼šå°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºåå…­è¿›åˆ¶ä¸Šä¸‹æ–‡
3. åˆ†è¯ç¼–ç ï¼šä½¿ç”¨ T5 åˆ†è¯å™¨å¤„ç†è¾“å…¥
4. æ¨¡å‹æ¨ç†ï¼šé€šè¿‡ T5 ç¼–ç å™¨è·å–è¡¨ç¤º
5. åˆ†ç±»é¢„æµ‹ï¼šä½¿ç”¨çº¿æ€§åˆ†ç±»å¤´è¿›è¡Œé¢„æµ‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python demo_pipeline.py [--use-pretrained]

é€‰é¡¹ï¼š
    --use-pretrained  ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼ˆéœ€è¦ weights.pth æ–‡ä»¶ï¼‰
"""

import argparse
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional

import numpy as np
import pandas as pd
import torch
from torch import nn


def print_banner():
    """æ‰“å°æ¬¢è¿æ¨ªå¹…"""
    print("=" * 70)
    print("ğŸ”¬ PCAP_encoder ç«¯åˆ°ç«¯æ¼”ç¤ºè„šæœ¬")
    print("=" * 70)
    print()
    print("æœ¬è„šæœ¬å°†æ¼”ç¤ºå®Œæ•´çš„ç½‘ç»œæµé‡è¡¨ç¤ºå­¦ä¹ æµç¨‹ã€‚")
    print("æ¯ä¸ªæ­¥éª¤éƒ½ä¼šè¯¦ç»†å±•ç¤ºè¾“å…¥å’Œè¾“å‡ºã€‚")
    print()


def wait_for_user(step: int, total: int, message: str):
    """ç­‰å¾…ç”¨æˆ·æŒ‰å›è½¦ç»§ç»­"""
    print(f"\n{'â”€' * 70}")
    print(f"[Step {step}/{total}] {message}")
    print("â”€" * 70)
    input(">>> æŒ‰ Enter é”®ç»§ç»­...")
    print()


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="PCAP_encoder ç«¯åˆ°ç«¯æ¼”ç¤º")
    parser.add_argument(
        "--use-pretrained",
        action="store_true",
        default=True,  # é»˜è®¤ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        help="ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼ˆé»˜è®¤å¯ç”¨ï¼Œéœ€è¦ models/weights.pthï¼‰"
    )
    parser.add_argument(
        "--no-pretrained",
        action="store_true",
        help="ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–"
    )
    parser.add_argument(
        "--data",
        type=Path,
        default=None,
        help="æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆParquet æ ¼å¼ï¼‰"
    )
    parser.add_argument(
        "--n-samples",
        type=int,
        default=10,
        help="æ¼”ç¤ºä½¿ç”¨çš„æ ·æœ¬æ•°é‡"
    )
    parser.add_argument(
        "--model-name",
        type=str,
        default=None,  # è‡ªåŠ¨æ£€æµ‹ï¼šæœ‰é¢„è®­ç»ƒæƒé‡ç”¨ t5-baseï¼Œå¦åˆ™ç”¨ t5-small
        help="T5 æ¨¡å‹åç§°ï¼ˆé»˜è®¤è‡ªåŠ¨æ£€æµ‹ï¼šé¢„è®­ç»ƒç”¨ t5-baseï¼Œæ¼”ç¤ºç”¨ t5-smallï¼‰"
    )
    args = parser.parse_args()
    # å¤„ç†äº’æ–¥é€‰é¡¹
    if args.no_pretrained:
        args.use_pretrained = False
    
    # è‡ªåŠ¨æ£€æµ‹æ¨¡å‹åç§°
    if args.model_name is None:
        script_dir = Path(__file__).resolve().parent
        weights_path = script_dir.parent / "models" / "weights.pth"
        if args.use_pretrained and weights_path.exists():
            # PCAP_encoder é¢„è®­ç»ƒæƒé‡åŸºäº t5-base (éšè—ç»´åº¦=768, 12å±‚)
            args.model_name = "t5-base"
        else:
            args.model_name = "t5-small"
    
    return args


def check_dependencies() -> bool:
    """æ£€æŸ¥å¿…è¦çš„ä¾èµ–æ˜¯å¦å·²å®‰è£…"""
    print("ğŸ” æ£€æŸ¥ä¾èµ–...")
    
    missing = []
    
    try:
        import transformers
        print(f"   âœ… transformers {transformers.__version__}")
    except ImportError:
        missing.append("transformers")
        print("   âŒ transformers æœªå®‰è£…")
    
    try:
        import torch
        print(f"   âœ… torch {torch.__version__}")
    except ImportError:
        missing.append("torch")
        print("   âŒ torch æœªå®‰è£…")
    
    try:
        import pandas
        print(f"   âœ… pandas {pandas.__version__}")
    except ImportError:
        missing.append("pandas")
        print("   âŒ pandas æœªå®‰è£…")
    
    if missing:
        print(f"\nâš ï¸  ç¼ºå°‘ä¾èµ–: {', '.join(missing)}")
        print("è¯·è¿è¡Œ: pip install " + " ".join(missing))
        return False
    
    return True


def load_or_generate_data(data_path: Optional[Path], n_samples: int) -> pd.DataFrame:
    """åŠ è½½ç°æœ‰æ•°æ®æˆ–ç”Ÿæˆæ¼”ç¤ºæ•°æ®"""
    
    if data_path and data_path.exists():
        print(f"ğŸ“‚ ä»æ–‡ä»¶åŠ è½½æ•°æ®: {data_path}")
        df = pd.read_parquet(data_path)
        if len(df) > n_samples:
            df = df.sample(n=n_samples, random_state=42)
        print(f"   å·²åŠ è½½ {len(df)} æ¡è®°å½•")
    else:
        print("ğŸ”§ ç”Ÿæˆæ¼”ç¤ºæ•°æ®...")
        # ç”Ÿæˆç®€å•çš„æ¼”ç¤ºæ•°æ®
        np.random.seed(42)
        
        attack_types = ["BENIGN", "FTP-Patator", "SSH-Patator"]
        labels = np.random.choice(attack_types, size=n_samples, p=[0.5, 0.3, 0.2])
        
        # ç”Ÿæˆ 32 å­—èŠ‚çš„è½½è·
        payload_data = np.random.randint(0, 256, size=(n_samples, 32))
        
        columns = {f"payload_byte_{i+1}": payload_data[:, i] for i in range(32)}
        columns["attack_label"] = labels
        
        df = pd.DataFrame(columns)
        print(f"   å·²ç”Ÿæˆ {len(df)} æ¡æ¼”ç¤ºè®°å½•")
    
    return df


def build_text_fields(df: pd.DataFrame, question: str = "Classify the network packet") -> Tuple[List[str], List[str], np.ndarray, List[str]]:
    """
    å°†æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
    
    Returns:
        questions: é—®é¢˜åˆ—è¡¨
        contexts: ä¸Šä¸‹æ–‡åˆ—è¡¨ï¼ˆåå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼‰
        labels: æ ‡ç­¾æ•°ç»„
        label_names: æ ‡ç­¾åç§°åˆ—è¡¨
    """
    print("ğŸ”„ å°†è½½è·å­—èŠ‚è½¬æ¢ä¸ºåå…­è¿›åˆ¶ä¸Šä¸‹æ–‡...")
    
    # è·å–æ‰€æœ‰ payload åˆ—
    payload_cols = sorted(
        [c for c in df.columns if c.startswith("payload_byte_")],
        key=lambda x: int(x.split("_")[-1])
    )
    
    # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
    byte_array = df[payload_cols].to_numpy(dtype=np.uint16)
    contexts = []
    for row in byte_array:
        hex_str = "".join(f"{int(b):02x}" for b in row)
        # æ¯4å­—ç¬¦åˆ†ç»„ï¼ˆPCAP_encoder æ ¼å¼ï¼‰
        hex_grouped = " ".join(hex_str[i:i+4] for i in range(0, len(hex_str), 4))
        contexts.append(hex_grouped)
    
    # æ„é€ é—®é¢˜
    questions = [question] * len(contexts)
    
    # æ ‡ç­¾ç¼–ç 
    labels, uniques = pd.factorize(df["attack_label"], sort=True)
    
    print(f"   é—®é¢˜æ¨¡æ¿: \"{question}\"")
    print(f"   ä¸Šä¸‹æ–‡é•¿åº¦: {len(contexts[0])} å­—ç¬¦")
    print(f"   æ ‡ç­¾æ˜ å°„: {dict(enumerate(uniques))}")
    
    return questions, contexts, labels.astype(np.int64), uniques.tolist()


def show_sample_conversion(df: pd.DataFrame, questions: List[str], contexts: List[str], n: int = 2):
    """å±•ç¤ºæ ·æœ¬è½¬æ¢çš„è¯¦ç»†è¿‡ç¨‹"""
    print("ğŸ“‹ æ ·æœ¬è½¬æ¢è¯¦æƒ…:")
    print()
    
    payload_cols = sorted(
        [c for c in df.columns if c.startswith("payload_byte_")],
        key=lambda x: int(x.split("_")[-1])
    )[:8]  # åªå–å‰8å­—èŠ‚
    
    for i in range(min(n, len(df))):
        row = df.iloc[i]
        print(f"   â”Œâ”€ æ ·æœ¬ {i+1} â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
        print(f"   â”‚ æ ‡ç­¾: {row['attack_label']}")
        
        bytes_vals = [int(row[col]) for col in payload_cols]
        print(f"   â”‚ åŸå§‹å­—èŠ‚ (å‰8): {bytes_vals}")
        
        print(f"   â”‚ é—®é¢˜: {questions[i][:50]}...")
        print(f"   â”‚ ä¸Šä¸‹æ–‡: {contexts[i][:60]}...")
        print(f"   â””{'â”€' * 55}")
        print()


def tokenize_inputs(
    questions: List[str],
    contexts: List[str],
    tokenizer,
    max_length: int = 128
) -> Dict[str, torch.Tensor]:
    """ä½¿ç”¨ T5 åˆ†è¯å™¨ç¼–ç è¾“å…¥"""
    print("ğŸ”¤ åˆ†è¯ç¼–ç ...")
    
    # T5 çš„è¾“å…¥æ ¼å¼ï¼šquestion + context
    inputs = [f"question: {q} context: {c}" for q, c in zip(questions, contexts)]
    
    encoded = tokenizer(
        inputs,
        padding=True,
        truncation=True,
        max_length=max_length,
        return_tensors="pt"
    )
    
    print(f"   è¾“å…¥åºåˆ—æ•°: {len(inputs)}")
    print(f"   Token åºåˆ—å½¢çŠ¶: {encoded['input_ids'].shape}")
    print(f"   æœ€å¤§åºåˆ—é•¿åº¦: {encoded['input_ids'].shape[1]}")
    
    # å±•ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬çš„åˆ†è¯ç»“æœ
    print()
    print("   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„åˆ†è¯ç»“æœ:")
    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0][:20])
    print(f"   Tokens (å‰20): {tokens}")
    
    return encoded


def load_model(model_name: str, weights_path: Optional[Path], device: str) -> Tuple[nn.Module, int]:
    """åŠ è½½ T5 ç¼–ç å™¨"""
    from transformers import T5ForConditionalGeneration
    
    print(f"ğŸ¤– åŠ è½½ T5 æ¨¡å‹: {model_name}")
    
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    
    if weights_path and weights_path.exists():
        print(f"   åŠ è½½é¢„è®­ç»ƒæƒé‡: {weights_path}")
        try:
            state = torch.load(weights_path, map_location="cpu", weights_only=True)
        except TypeError:
            state = torch.load(weights_path, map_location="cpu")
        model.load_state_dict(state, strict=False)
        print("   âœ… æƒé‡åŠ è½½æˆåŠŸ")
    else:
        print("   âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆä»…ç”¨äºæ¼”ç¤ºæµç¨‹ï¼‰")
    
    encoder = model.encoder.to(device)
    
    # å†»ç»“å‚æ•°
    for p in encoder.parameters():
        p.requires_grad = False
    
    hidden_size = model.config.d_model
    print(f"   ç¼–ç å™¨éšè—ç»´åº¦: {hidden_size}")
    print(f"   ç¼–ç å™¨å±‚æ•°: {model.config.num_layers}")
    print(f"   è®¾å¤‡: {device}")
    
    return encoder, hidden_size


def encode_and_classify(
    encoder: nn.Module,
    head: nn.Module,
    encodings: Dict[str, torch.Tensor],
    labels: np.ndarray,
    device: str
) -> Tuple[np.ndarray, np.ndarray]:
    """é€šè¿‡ç¼–ç å™¨å’Œåˆ†ç±»å¤´è¿›è¡Œæ¨ç†"""
    print("ğŸ§  ç¼–ç  + åˆ†ç±»æ¨ç†...")
    
    input_ids = encodings["input_ids"].to(device)
    attention_mask = encodings["attention_mask"].to(device)
    
    with torch.no_grad():
        # ç¼–ç 
        print("   1. é€šè¿‡ T5 ç¼–ç å™¨...")
        outputs = encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
        hidden = outputs.last_hidden_state[:, 0, :]  # å–ç¬¬ä¸€ä¸ª token çš„è¡¨ç¤º
        print(f"      éšè—è¡¨ç¤ºå½¢çŠ¶: {hidden.shape}")
        
        # åˆ†ç±»
        print("   2. é€šè¿‡çº¿æ€§åˆ†ç±»å¤´...")
        logits = head(hidden)
        print(f"      Logits å½¢çŠ¶: {logits.shape}")
        
        # é¢„æµ‹
        probs = torch.softmax(logits, dim=1).cpu().numpy()
        preds = probs.argmax(axis=1)
    
    return preds, probs


def show_predictions(
    df: pd.DataFrame,
    labels: np.ndarray,
    preds: np.ndarray,
    probs: np.ndarray,
    label_names: List[str]
):
    """å±•ç¤ºé¢„æµ‹ç»“æœ"""
    print("ğŸ“Š é¢„æµ‹ç»“æœ:")
    print()
    print(f"   {'æ ·æœ¬':^6} â”‚ {'çœŸå®æ ‡ç­¾':^15} â”‚ {'é¢„æµ‹æ ‡ç­¾':^15} â”‚ {'ç½®ä¿¡åº¦':^10} â”‚ {'æ­£ç¡®':^6}")
    print(f"   {'â”€'*6}â”€â”¼â”€{'â”€'*15}â”€â”¼â”€{'â”€'*15}â”€â”¼â”€{'â”€'*10}â”€â”¼â”€{'â”€'*6}")
    
    correct = 0
    for i in range(len(labels)):
        true_label = label_names[labels[i]]
        pred_label = label_names[preds[i]]
        confidence = probs[i].max() * 100
        is_correct = "âœ…" if labels[i] == preds[i] else "âŒ"
        if labels[i] == preds[i]:
            correct += 1
        
        print(f"   {i+1:^6} â”‚ {true_label:^15} â”‚ {pred_label:^15} â”‚ {confidence:^10.1f}% â”‚ {is_correct:^6}")
    
    accuracy = correct / len(labels) * 100
    print(f"   {'â”€'*6}â”€â”´â”€{'â”€'*15}â”€â”´â”€{'â”€'*15}â”€â”´â”€{'â”€'*10}â”€â”´â”€{'â”€'*6}")
    print(f"\n   å‡†ç¡®ç‡: {accuracy:.1f}% ({correct}/{len(labels)})")
    
    # æ³¨æ„äº‹é¡¹
    print()
    print("   ğŸ“ æ³¨æ„:")
    print("   - å¦‚æœæœªä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå‡†ç¡®ç‡æ¥è¿‘éšæœºçŒœæµ‹æ˜¯æ­£å¸¸çš„")
    print("   - è¿™åªæ˜¯æ¼”ç¤ºæµç¨‹ï¼Œä¸ä»£è¡¨çœŸå®æ¨¡å‹æ€§èƒ½")


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print_banner()
    
    # Step 0: æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        sys.exit(1)
    
    # å¯¼å…¥ï¼ˆåœ¨ä¾èµ–æ£€æŸ¥åï¼‰
    from transformers import T5TokenizerFast
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"\nğŸ–¥ï¸  è¿è¡Œè®¾å¤‡: {device}")
    
    # Step 1: åŠ è½½/ç”Ÿæˆæ•°æ®
    wait_for_user(1, 5, "åŠ è½½æˆ–ç”Ÿæˆæµ‹è¯•æ•°æ®")
    df = load_or_generate_data(args.data, args.n_samples)
    print()
    print(df.head())
    
    # Step 2: æ„å»ºæ–‡æœ¬å­—æ®µ
    wait_for_user(2, 5, "å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼")
    questions, contexts, labels, label_names = build_text_fields(df)
    print()
    show_sample_conversion(df, questions, contexts)
    
    # Step 3: åˆ†è¯
    wait_for_user(3, 5, "ä½¿ç”¨ T5 åˆ†è¯å™¨ç¼–ç ")
    print(f"â³ æ­£åœ¨åŠ è½½åˆ†è¯å™¨ ({args.model_name})...")
    tokenizer = T5TokenizerFast.from_pretrained(args.model_name)
    encodings = tokenize_inputs(questions, contexts, tokenizer)
    
    # Step 4: åŠ è½½æ¨¡å‹
    wait_for_user(4, 5, "åŠ è½½ T5 ç¼–ç å™¨å’Œåˆ†ç±»å¤´")
    
    # é»˜è®¤å°è¯•åŠ è½½é¢„è®­ç»ƒæƒé‡
    script_dir = Path(__file__).resolve().parent
    weights_path = script_dir.parent / "models" / "weights.pth"
    
    if not args.use_pretrained:
        # ç”¨æˆ·æ˜ç¡®ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        print("âš ï¸  ç”¨æˆ·é€‰æ‹©ä¸ä½¿ç”¨é¢„è®­ç»ƒæƒé‡")
        weights_path = None
    elif not weights_path.exists():
        print(f"âš ï¸  æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶: {weights_path}")
        print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
        weights_path = None
    else:
        print(f"âœ… æ‰¾åˆ°é¢„è®­ç»ƒæƒé‡: {weights_path}")
    
    encoder, hidden_size = load_model(args.model_name, weights_path, device)
    
    # åˆ›å»ºåˆ†ç±»å¤´
    num_classes = len(label_names)
    head = nn.Linear(hidden_size, num_classes).to(device)
    print(f"   åˆ†ç±»å¤´: Linear({hidden_size} -> {num_classes})")
    
    # Step 5: æ¨ç†
    wait_for_user(5, 5, "è¿è¡Œæ¨ç†å¹¶æŸ¥çœ‹é¢„æµ‹ç»“æœ")
    preds, probs = encode_and_classify(encoder, head, encodings, labels, device)
    print()
    show_predictions(df, labels, preds, probs, label_names)
    
    # å®Œæˆ
    print()
    print("=" * 70)
    print("ğŸ‰ æ¼”ç¤ºå®Œæˆ!")
    print("=" * 70)
    print()
    print("ğŸ’¡ åç»­å»ºè®®:")
    print("   1. ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¿è¡Œ: python demo_pipeline.py --use-pretrained")
    print("   2. ä½¿ç”¨æ›´å¤šæ•°æ®: python demo_pipeline.py --n-samples 50")
    print("   3. æŸ¥çœ‹å®Œæ•´è¯„ä¼°è„šæœ¬: eval_with_encoder_head.py")


if __name__ == "__main__":
    main()
