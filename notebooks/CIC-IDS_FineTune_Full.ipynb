{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1feda431",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒé…ç½®ä¸è·¯å¾„è®¾ç½®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3435dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ é¡¹ç›®æ ¹ç›®å½•: /home/test/ybk/nids/encoder/PCAP_encoder\n",
      "ğŸ“Œ å½“å‰ä½¿ç”¨çš„ GPU ç´¢å¼•: 4,5,6,7\n",
      "ğŸ“¦ PyTorch ç‰ˆæœ¬: 2.2.2+cu118\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ğŸš€ æŒ‡å®šä½¿ç”¨ 4, 5, 6, 7 å››å¼  GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6,7\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ğŸ› ï¸ è®¾ç½®é¡¹ç›®æ ¹ç›®å½•\n",
    "PROJECT_ROOT = Path(\"/home/test/ybk/nids/encoder/PCAP_encoder\")\n",
    "os.chdir(PROJECT_ROOT)\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "print(f\"ğŸ“‚ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}\")\n",
    "# âš ï¸ æ³¨æ„ï¼šä¸ºäº†å¤šå¡å¹¶è¡Œï¼Œæ­¤å¤„ä¸¥ç¦è°ƒç”¨ torch.cuda.is_available() æˆ–ä»»ä½•è§¦ç¢° GPU çš„æ“ä½œ\n",
    "print(f\"ğŸ“Œ å½“å‰ä½¿ç”¨çš„ GPU ç´¢å¼•: {os.environ.get('CUDA_VISIBLE_DEVICES')}\")\n",
    "print(f\"ğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85003efd",
   "metadata": {},
   "source": [
    "## 2. å¯¼å…¥æ ¸å¿ƒæ¨¡å—\n",
    "\n",
    "æˆ‘ä»¬å°†ç›´æ¥è°ƒç”¨åŸä½œè€…å®šä¹‰çš„ `Core` æ¨¡å—ï¼Œç¡®ä¿å®éªŒé€»è¾‘ä¸è®ºæ–‡ä¸€è‡´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2596d8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ!\n"
     ]
    }
   ],
   "source": [
    "from Core.classes.dataset_for_classification import Classification_Dataset\n",
    "from Core.classes.classification_model import Classification_model\n",
    "from Core.classes.tokenizer import QA_Tokenizer_T5\n",
    "from Core.classes.logger import TrainingExperimentLogger\n",
    "\n",
    "print(\"âœ… æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0b80d7",
   "metadata": {},
   "source": [
    "## 3. å®éªŒå‚æ•°é…ç½®\n",
    "\n",
    "åœ¨è¿™é‡Œæˆ‘ä»¬é…ç½®å¾®è°ƒçš„å…³é”®å‚æ•°ã€‚æ³¨æ„ï¼šä¸ºäº†è·å¾—æ›´å¥½çš„æ•ˆæœï¼Œæˆ‘ä»¬**è§£å†»äº†ç¼–ç å™¨**å¹¶é™ä½äº†å­¦ä¹ ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0d9568a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ å®éªŒæ¨¡å¼: å…¨é‡å¾®è°ƒ (Unfrozen)\n",
      "ğŸ”— å­¦ä¹ ç‡: 2e-05\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# ğŸ“‹ æ ¸å¿ƒé…ç½® - é’ˆå¯¹ NIDS å¾®è°ƒä¼˜åŒ–\n",
    "# ============================================================\n",
    "\n",
    "# 1. è·¯å¾„é…ç½®\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"CIC-IDS2017\" / \"Classification\"\n",
    "TRAINING_DATA = str(DATA_DIR / \"train.parquet\")\n",
    "VALIDATION_DATA = str(DATA_DIR / \"val.parquet\")\n",
    "TESTING_DATA = str(DATA_DIR / \"test.parquet\")\n",
    "\n",
    "# é¢„è®­ç»ƒæƒé‡è·¯å¾„ (weights.pth æ‰€åœ¨ç›®å½•)\n",
    "PRETRAINED_MODEL_PATH = str(PROJECT_ROOT / \"models\")\n",
    "\n",
    "# 2. è®­ç»ƒç­–ç•¥ (å®Œå…¨è§£å†»æ¨¡å¼)\n",
    "FIX_ENCODER = False      # âŒ False è¡¨ç¤ºè§£å†»ç¼–ç å™¨è¿›è¡Œå…¨é‡å¾®è°ƒ\n",
    "LEARNING_RATE = 2e-5     # ğŸ“‰ è§£å†»æ—¶ä½¿ç”¨æå°çš„å­¦ä¹ ç‡ (2e-5)\n",
    "EPOCHS = 15              # è®­ç»ƒè½®æ•°\n",
    "BATCH_SIZE = 16          # æ‰¹æ¬¡å¤§å° (æ ¹æ®æ˜¾å­˜è°ƒæ•´)\n",
    "LOSS_TYPE = \"weighted\"   # âš–ï¸ ä½¿ç”¨åŠ æƒæŸå¤±å¤„ç†ç±»åˆ«ä¸å¹³è¡¡\n",
    "\n",
    "# 3. æ¨¡å‹æ¶æ„\n",
    "MODEL_NAME = \"t5-base\"   # å¿…é¡»ä¸é¢„è®­ç»ƒæƒé‡åŒ¹é…\n",
    "TOKENIZER_NAME = \"t5-base\"\n",
    "BOTTLENECK = \"mean\"      # æå–ç‰¹å¾çš„æ–¹å¼\n",
    "MAX_QST_LENGTH = 512\n",
    "MAX_ANS_LENGTH = 32\n",
    "SEED = 42\n",
    "\n",
    "print(f\"ğŸš€ å®éªŒæ¨¡å¼: {'å…¨é‡å¾®è°ƒ (Unfrozen)' if not FIX_ENCODER else 'çº¿æ€§æ¢æµ‹ (Frozen)'}\")\n",
    "print(f\"ğŸ”— å­¦ä¹ ç‡: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619c8363",
   "metadata": {},
   "source": [
    "## 4. æ•°æ®é¢„å¤„ç†\n",
    "\n",
    "å°†åŸå§‹çš„ CIC-IDS2017 å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯è¯»çš„åå…­è¿›åˆ¶æ–‡æœ¬æ ¼å¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dca6872d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ æ­£åœ¨ä» /home/test/ybk/nids/encoder/PCAP_encoder/data/CIC-IDS2017/Payload-Bytes åŠ è½½å‰ 5 ä¸ªåˆ†ç‰‡...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ åŠ è½½ Parquet åˆ†ç‰‡: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [01:03<00:00, 12.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… åŸå§‹æ•°æ®åŠ è½½å®Œæˆï¼Œå…± 8378784 æ¡è®°å½•ã€‚\n"
     ]
    }
   ],
   "source": [
    "def bytes_to_hex(byte_array: np.ndarray, format_type: str = 'every4') -> list:\n",
    "    \"\"\"å°†å­—èŠ‚æ•°ç»„è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²\"\"\"\n",
    "    hex_strings = []\n",
    "    for row in tqdm(byte_array, desc=\"ğŸ”„ è½¬æ¢åå…­è¿›åˆ¶\", leave=False):\n",
    "        hex_str = ''.join(f'{int(b):02x}' for b in row)\n",
    "        if format_type == 'every4':\n",
    "            hex_str = ' '.join(hex_str[i:i+4] for i in range(0, len(hex_str), 4))\n",
    "        hex_strings.append(hex_str.strip())\n",
    "    return hex_strings\n",
    "\n",
    "def load_raw_nids_data(input_dir, num_files=5):\n",
    "    \"\"\"\n",
    "    ğŸš€ æ…¢é€Ÿæ­¥éª¤ï¼šä»ç£ç›˜åŠ è½½åŸå§‹ Parquet åˆ†ç‰‡åˆ°å†…å­˜\n",
    "    \"\"\"\n",
    "    input_dir = Path(input_dir)\n",
    "    files = sorted(glob.glob(str(input_dir / \"Payload_Bytes_File_*.parquet\")))\n",
    "    \n",
    "    print(f\"â³ æ­£åœ¨ä» {input_dir} åŠ è½½å‰ {num_files} ä¸ªåˆ†ç‰‡...\")\n",
    "    dfs = []\n",
    "    for f in tqdm(files[:num_files], desc=\"ğŸ“‚ åŠ è½½ Parquet åˆ†ç‰‡\"):\n",
    "        dfs.append(pd.read_parquet(f))\n",
    "    \n",
    "    df_all = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"âœ… åŸå§‹æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(df_all)} æ¡è®°å½•ã€‚\")\n",
    "    return df_all\n",
    "\n",
    "# 1. æ‰§è¡ŒåŠ è½½ (è¿™ä¸€æ­¥åªéœ€è¦è¿è¡Œä¸€æ¬¡)\n",
    "RAW_PAYLOAD_DIR = PROJECT_ROOT / \"data\" / \"CIC-IDS2017\" / \"Payload-Bytes\"\n",
    "df_raw = load_raw_nids_data(RAW_PAYLOAD_DIR, num_files=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab367b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š åŸå§‹ç±»åˆ«åˆ†å¸ƒ:\n",
      "attack_label\n",
      "BENIGN                        7727363\n",
      "DoS Hulk                       592302\n",
      "DoS GoldenEye                   23923\n",
      "Heartbleed                      20920\n",
      "Web Attack â€“ Brute Force         9437\n",
      "Web Attack â€“ XSS                 2798\n",
      "DoS Slowhttptest                 2001\n",
      "Web Attack â€“ Sql Injection         26\n",
      "DoS slowloris                      14\n",
      "Name: count, dtype: Int64\n",
      "âš–ï¸ å¹³è¡¡åæ€»æ ·æœ¬æ•°: 19998\n",
      "ğŸ”„ æ­£åœ¨è½¬æ¢ 19998 æ¡æ•°æ®çš„å­—èŠ‚ä¸ºåå…­è¿›åˆ¶æ–‡æœ¬...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...\n",
      "âœ… å¹³è¡¡æ•°æ®å¤„ç†å®Œæˆï¼\n",
      "ğŸ“Š æœ€ç»ˆè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "type_q\n",
      "Heartbleed                    1556\n",
      "Web Attack â€“ Sql Injection    1556\n",
      "DoS GoldenEye                 1556\n",
      "Web Attack â€“ XSS              1555\n",
      "DoS Hulk                      1555\n",
      "BENIGN                        1555\n",
      "DoS slowloris                 1555\n",
      "Web Attack â€“ Brute Force      1555\n",
      "DoS Slowhttptest              1555\n",
      "Name: count, dtype: Int64\n"
     ]
    }
   ],
   "source": [
    "def process_and_save_nids_data(df, output_dir, sample_size=20000):\n",
    "    \"\"\"\n",
    "    âš¡ æ”¹è¿›ç‰ˆï¼šå¹³è¡¡é‡‡æ ·ç­–ç•¥\n",
    "    \"\"\"\n",
    "    output_dir = Path(output_dir)\n",
    "    \n",
    "    # 1. ç»Ÿè®¡åŸå§‹åˆ†å¸ƒ\n",
    "    print(f\"ğŸ“Š åŸå§‹ç±»åˆ«åˆ†å¸ƒ:\\n{df['attack_label'].value_counts()}\")\n",
    "    \n",
    "    # 2. å¹³è¡¡é‡‡æ ·é€»è¾‘\n",
    "    # ç›®æ ‡ï¼šæ¯ä¸ªç±»åˆ«å°½é‡è¾¾åˆ° target_per_class æ¡æ ·æœ¬\n",
    "    target_per_class = sample_size // df['attack_label'].nunique()\n",
    "    balanced_dfs = []\n",
    "    \n",
    "    for label in df['attack_label'].unique():\n",
    "        df_group = df[df['attack_label'] == label]\n",
    "        if len(df_group) >= target_per_class:\n",
    "            # å¤šæ•°ç±»ï¼šä¸‹é‡‡æ ·\n",
    "            balanced_dfs.append(df_group.sample(n=target_per_class, random_state=SEED))\n",
    "        else:\n",
    "            # å°‘æ•°ç±»ï¼šè¿‡é‡‡æ · (é‡å¤åˆ©ç”¨ç°æœ‰æ ·æœ¬)\n",
    "            balanced_dfs.append(df_group.sample(n=target_per_class, replace=True, random_state=SEED))\n",
    "    \n",
    "    df_balanced = pd.concat(balanced_dfs, ignore_index=True)\n",
    "    print(f\"âš–ï¸ å¹³è¡¡åæ€»æ ·æœ¬æ•°: {len(df_balanced)}\")\n",
    "\n",
    "    # 3. æ ‡ç­¾ç¼–ç \n",
    "    labels, uniques = pd.factorize(df_balanced['attack_label'], sort=True)\n",
    "    df_balanced['class'] = labels.astype(np.int64)\n",
    "    \n",
    "    # 4. æå– Payload å¹¶è½¬æ¢åå…­è¿›åˆ¶\n",
    "    print(f\"ğŸ”„ æ­£åœ¨è½¬æ¢ {len(df_balanced)} æ¡æ•°æ®çš„å­—èŠ‚ä¸ºåå…­è¿›åˆ¶æ–‡æœ¬...\")\n",
    "    payload_cols = [f\"payload_byte_{i}\" for i in range(1, 65)]\n",
    "    X_bytes = df_balanced[payload_cols].fillna(0).values.astype(np.uint8)\n",
    "    contexts = bytes_to_hex(X_bytes)\n",
    "    \n",
    "    # 5. æ„é€ æœ€ç»ˆ DataFrame\n",
    "    new_df = pd.DataFrame({\n",
    "        'question': 'Classify the network packet',\n",
    "        'context': contexts,\n",
    "        'class': df_balanced['class'].values,\n",
    "        'type_q': df_balanced['attack_label'].values\n",
    "    })\n",
    "    \n",
    "    # 6. åˆ’åˆ†æ•°æ®é›† (æ­¤æ—¶æ¯ä¸ªç±»éƒ½æœ‰è¶³å¤Ÿæ ·æœ¬ï¼Œä¸ä¼šæŠ¥é”™)\n",
    "    print(f\"âœ‚ï¸ æ­£åœ¨åˆ’åˆ†æ•°æ®é›†...\")\n",
    "    train_df, temp_df = train_test_split(new_df, test_size=0.3, random_state=SEED, stratify=new_df['class'])\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=SEED, stratify=temp_df['class'])\n",
    "    \n",
    "    # 7. ä¿å­˜\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    train_df.to_parquet(output_dir / \"train.parquet\", index=False)\n",
    "    val_df.to_parquet(output_dir / \"val.parquet\", index=False)\n",
    "    test_df.to_parquet(output_dir / \"test.parquet\", index=False)\n",
    "    \n",
    "    print(f\"âœ… å¹³è¡¡æ•°æ®å¤„ç†å®Œæˆï¼\")\n",
    "    print(f\"ğŸ“Š æœ€ç»ˆè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\\n{train_df['type_q'].value_counts()}\")\n",
    "\n",
    "# æ‰§è¡Œå¤„ç†ï¼šå»ºè®®å¢åŠ  sample_size åˆ° 20000 ä»¥ä¸Š\n",
    "process_and_save_nids_data(df_raw, DATA_DIR, sample_size=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f1566",
   "metadata": {},
   "source": [
    "## 5. åˆå§‹åŒ–å®éªŒç»„ä»¶\n",
    "\n",
    "æˆ‘ä»¬å°†é€æ­¥åˆå§‹åŒ–åˆ†è¯å™¨ã€æ•°æ®é›†å’Œæ—¥å¿—è®°å½•å™¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ea8e116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®éªŒé…ç½®å·²æ›´æ–°ä¸ºå¤šå¡æ¨¡å¼!\n"
     ]
    }
   ],
   "source": [
    "# 1. æ„å»ºé…ç½®å­—å…¸\n",
    "opts = {\n",
    "    \"identifier\": f\"nids_finetune_lr{LEARNING_RATE}_unfrozen_multiGPU\",\n",
    "    \"experiment\": \"CIC-IDS2017_NIDS\",\n",
    "    \"task\": \"supervised\",\n",
    "    \"clean_start\": True,\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"tokenizer_name\": TOKENIZER_NAME,\n",
    "    \"finetuned_path_model\": PRETRAINED_MODEL_PATH,\n",
    "    \"bottleneck\": BOTTLENECK,\n",
    "    \"pkt_repr_dim\": 768,\n",
    "    \"use_pkt_reduction\": False,\n",
    "    \"lr\": LEARNING_RATE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE, # æ¯å¼ å¡çš„ batch size\n",
    "    \"seed\": SEED,\n",
    "    \"loss\": LOSS_TYPE,\n",
    "    \"fix_encoder\": FIX_ENCODER,\n",
    "    \"training_data\": TRAINING_DATA,\n",
    "    \"validation_data\": VALIDATION_DATA,\n",
    "    \"testing_data\": TESTING_DATA,\n",
    "    \"percentage\": 100,\n",
    "    \"max_qst_length\": MAX_QST_LENGTH,\n",
    "    \"max_ans_length\": MAX_ANS_LENGTH,\n",
    "    \"input_format\": \"every4\",\n",
    "    \"output_path\": str(PROJECT_ROOT / \"results\"),\n",
    "    \"log_level\": \"info\",\n",
    "    \"use_cuda\": True, # ğŸš€ ç›´æ¥è®¾ä¸º Trueï¼Œä¸¥ç¦åœ¨æ­¤å¤„è°ƒç”¨ torch.cuda.is_available()\n",
    "    \"gpu\": \"cuda\", \n",
    "}\n",
    "\n",
    "print(\"âœ… å®éªŒé…ç½®å·²æ›´æ–°ä¸ºå¤šå¡æ¨¡å¼!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abeb639",
   "metadata": {},
   "source": [
    "## 7. æ‰§è¡Œåˆ†å¸ƒå¼å¾®è°ƒè®­ç»ƒ\n",
    "\n",
    "âš ï¸ **é‡è¦æç¤º**: \n",
    "1. **å¿…é¡»é‡å¯ Kernel**: ç”±äº `notebook_launcher` è¦æ±‚åœ¨å¯åŠ¨å‰ä¸èƒ½æœ‰ä»»ä½• `Accelerator` å®ä¾‹è¢«åˆå§‹åŒ–ï¼Œè¯·ç‚¹å‡»ä¸Šæ–¹èœå•çš„ **\"Restart\"**ã€‚\n",
    "2. **è·³è¿‡å…¨å±€åŠ è½½**: æˆ‘ä»¬å·²ç»åˆ é™¤äº†ä¹‹å‰çš„â€œåŠ è½½æ•°æ®ä¸æ¨¡å‹â€å•å…ƒæ ¼ï¼Œå› ä¸ºåœ¨å¤šå¡æ¨¡å¼ä¸‹ï¼Œæ¯ä¸ªè¿›ç¨‹å¿…é¡»åœ¨è‡ªå·±çš„ç©ºé—´å†…ç‹¬ç«‹åˆå§‹åŒ–æ¨¡å‹ã€‚\n",
    "3. **ç›´æ¥è¿è¡Œ**: é‡å¯åï¼Œä¾æ¬¡è¿è¡Œâ€œç¯å¢ƒé…ç½®â€ã€â€œå®éªŒå‚æ•°â€ã€â€œæ•°æ®é¢„å¤„ç†â€å’Œâ€œæ„å»ºé…ç½®å­—å…¸â€ï¼Œç„¶åç›´æ¥è¿è¡Œæœ¬å•å…ƒæ ¼ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbc23e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on 4 GPUs.\n",
      "ğŸ“¢ ä¸»è¿›ç¨‹å¯åŠ¨ï¼Œæ­£åœ¨ä¸º 4 å¼ æ˜¾å¡å‡†å¤‡ç¯å¢ƒ...\n",
      "\n",
      "############### Running experiment t5-base_standard-tokenizer_CIC-IDS2017_NIDSnids_finetune_lr2e-05_unfrozen_multiGPU_task-supervised_lr-2e-05_epochs-15_batch-16 for supervised ###############\n",
      "\n",
      "ğŸš€ å¼€å§‹ NIDS å¾®è°ƒè®­ç»ƒ (æ¨¡å¼: å…¨é‡å¾®è°ƒ)\n",
      "   - æ€» Batch Size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:05<00:00,  7.67it/s]/s]\n",
      "  7%|â–‹         | 218/3052 [01:11<2:03:36,  2.62s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.42it/s]/s]  \n",
      " 14%|â–ˆâ–        | 436/3052 [02:14<1:37:07,  2.23s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.36it/s]/s]  \n",
      " 21%|â–ˆâ–ˆâ–       | 654/3052 [03:31<1:32:11,  2.31s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:05<00:00,  8.11it/s]/s]  \n",
      " 29%|â–ˆâ–ˆâ–Š       | 872/3052 [04:47<1:34:32,  2.60s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.32it/s]t/s] \n",
      " 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 1090/3052 [06:05<1:04:40,  1.98s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.53it/s]t/s]  \n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 1308/3052 [07:06<56:59,  1.96s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.44it/s]t/s]  \n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1526/3052 [08:08<50:14,  1.98s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:06<00:00,  7.09it/s]t/s]  \n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 1744/3052 [09:24<55:18,  2.54s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.51it/s]t/s]  \n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1962/3052 [10:25<35:48,  1.97s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.47it/s]t/s]\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 2180/3052 [11:42<28:27,  1.96s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.38it/s]t/s]\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 2398/3052 [12:58<21:34,  1.98s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.33it/s]t/s]\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 2616/3052 [13:59<14:26,  1.99s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.58it/s]t/s]\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 2834/3052 [15:00<07:00,  1.93s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:05<00:00,  9.16it/s]t/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3052/3052 [16:02<00:00,  2.04s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.51it/s]    \n",
      "3270it [17:03,  1.94s/it]Deleting 1 checkpoints to make room for new checkpoint.\n",
      "Removed shared tensor {'lm_head.weight'} while saving. This should be OK, but check by verifying that you don't receive any warning while reloading\n",
      "3270it [17:06,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End training...\n",
      "Start testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46/46 [00:04<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2026-01-04 21:05:40,977] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 954104 via signal SIGTERM\n",
      "[2026-01-04 21:05:40,980] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 954106 via signal SIGTERM\n",
      "[2026-01-04 21:05:40,981] torch.distributed.elastic.multiprocessing.api: [WARNING] Closing process 954163 via signal SIGTERM\n"
     ]
    }
   ],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "def train_distributed():\n",
    "    \"\"\"\n",
    "    ğŸš€ å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒåŒ…è£…å‡½æ•°\n",
    "    \"\"\"\n",
    "    # 1. åˆå§‹åŒ–æ—¥å¿— (å†…éƒ¨ä¼šåˆ›å»º Accelerator)\n",
    "    local_logger = TrainingExperimentLogger(opts)\n",
    "    \n",
    "    # åªåœ¨ä¸»è¿›ç¨‹æ‰“å°åˆå§‹åŒ–ä¿¡æ¯\n",
    "    if local_logger.accelerator.is_main_process:\n",
    "        print(f\"ğŸ“¢ ä¸»è¿›ç¨‹å¯åŠ¨ï¼Œæ­£åœ¨ä¸º {local_logger.accelerator.num_processes} å¼ æ˜¾å¡å‡†å¤‡ç¯å¢ƒ...\")\n",
    "    \n",
    "    local_logger.start_experiment(opts)\n",
    "    \n",
    "    # 2. åˆå§‹åŒ–åˆ†è¯å™¨\n",
    "    local_tokenizer = QA_Tokenizer_T5(opts)\n",
    "    \n",
    "    # 3. åŠ è½½æ•°æ®é›†\n",
    "    local_dataset_trainval = Classification_Dataset(opts, local_tokenizer)\n",
    "    local_dataset_trainval.load_dataset(\"Train\", TRAINING_DATA, \"every4\", VALIDATION_DATA)\n",
    "    \n",
    "    local_dataset_test = Classification_Dataset(opts, local_tokenizer)\n",
    "    local_dataset_test.load_dataset(\"Test\", TESTING_DATA, \"every4\")\n",
    "    \n",
    "    # 4. åˆå§‹åŒ–æ¨¡å‹\n",
    "    local_model = Classification_model(opts, local_tokenizer, local_dataset_trainval, local_dataset_test)\n",
    "    \n",
    "    if local_logger.accelerator.is_main_process:\n",
    "        print(f\"ğŸš€ å¼€å§‹ NIDS å¾®è°ƒè®­ç»ƒ (æ¨¡å¼: {'å…¨é‡å¾®è°ƒ' if not FIX_ENCODER else 'ä»…è®­ç»ƒåˆ†ç±»å¤´'})\")\n",
    "        print(f\"   - æ€» Batch Size: {opts['batch_size'] * local_logger.accelerator.num_processes}\")\n",
    "    \n",
    "    try:\n",
    "        local_model.run(local_logger, opts)\n",
    "        if local_logger.accelerator.is_main_process:\n",
    "            print(\"\\nğŸ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼\")\n",
    "    except Exception as e:\n",
    "        # æ‰“å°é”™è¯¯å †æ ˆä»¥æ–¹ä¾¿è°ƒè¯•\n",
    "        import traceback\n",
    "        print(f\"\\nâŒ è¿›ç¨‹ {local_logger.accelerator.process_index} å‡ºé”™: {e}\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        local_logger.end_experiment()\n",
    "\n",
    "# ğŸš€ å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ\n",
    "# æ³¨æ„ï¼šè¿è¡Œæ­¤å•å…ƒæ ¼å‰å¿…é¡»å…ˆ Restart Kernelï¼Œç¡®ä¿æ²¡æœ‰å…¨å±€ Accelerator å®ä¾‹\n",
    "# ä½¿ç”¨ use_port å‚æ•°æŒ‡å®šä¸€ä¸ªæœªè¢«å ç”¨çš„ç«¯å£\n",
    "notebook_launcher(train_distributed, num_processes=4, use_port=\"29501\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64094279",
   "metadata": {},
   "source": [
    "## 8. ç»“æœåˆ†æ\n",
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œç»“æœå°†ä¿å­˜åœ¨ `results/` ç›®å½•ä¸‹ã€‚ä½ å¯ä»¥æŸ¥çœ‹æ··æ·†çŸ©é˜µå’Œ F1 åˆ†æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0abea65d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ å®éªŒç»“æœæ ¹ç›®å½•: /home/test/ybk/nids/encoder/PCAP_encoder/results/supervised/t5-base_standard-tokenizer/CIC-IDS2017_NIDSnids_finetune_lr2e-05_unfrozen_multiGPU/task-supervised_lr-2e-05_epochs-15_batch-16/seed_42\n",
      "âœ… æœ€ä½³æƒé‡è·¯å¾„: /home/test/ybk/nids/encoder/PCAP_encoder/results/supervised/t5-base_standard-tokenizer/CIC-IDS2017_NIDSnids_finetune_lr2e-05_unfrozen_multiGPU/task-supervised_lr-2e-05_epochs-15_batch-16/seed_42/best_model/weights.pth\n",
      "âš–ï¸ æƒé‡å¤§å°: 2.28 MB\n",
      "\n",
      "ğŸ“Š è¯„ä¼°æŒ‡æ ‡ (æ¥è‡ª CIC-IDS2017_NIDSnids_finetune_lr2e-05_unfrozen_multiGPU.json):\n",
      "   - æµ‹è¯•é›†å‡†ç¡®ç‡ (Accuracy): 84.92%\n",
      "   - Macro F1 Score: 0.8512\n",
      "   - Micro F1 Score: 0.8492\n",
      "   - æµ‹è¯•è€—æ—¶: 4.83s\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# 1. åŠ¨æ€æŸ¥æ‰¾æƒé‡æ–‡ä»¶è·¯å¾„ (åŸä½œè€…ä»£ç ç”Ÿæˆçš„è·¯å¾„éå¸¸æ·±)\n",
    "# ç»“æ„: results/supervised/t5-base_standard-tokenizer/å®éªŒID/å‚æ•°/seed_42/best_model/weights.pth\n",
    "search_pattern = str(PROJECT_ROOT / \"results\" / \"supervised\" / \"*\" / (opts['experiment'] + opts['identifier']) / \"*\" / f\"seed_{SEED}\" / \"best_model\" / \"weights.pth\")\n",
    "found_weights = glob.glob(search_pattern)\n",
    "\n",
    "if found_weights:\n",
    "    best_weight_path = Path(found_weights[0])\n",
    "    results_dir = best_weight_path.parent.parent\n",
    "    print(f\"ğŸ“‚ å®éªŒç»“æœæ ¹ç›®å½•: {results_dir}\")\n",
    "    print(f\"âœ… æœ€ä½³æƒé‡è·¯å¾„: {best_weight_path}\")\n",
    "    print(f\"âš–ï¸ æƒé‡å¤§å°: {best_weight_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    \n",
    "    # 2. è¯»å–è¯„ä¼°ç»“æœ (JSON æ ¼å¼)\n",
    "    eval_file = PROJECT_ROOT / \"evaluation\" / f\"{opts['experiment']}{opts['identifier']}.json\"\n",
    "    if eval_file.exists():\n",
    "        print(f\"\\nğŸ“Š è¯„ä¼°æŒ‡æ ‡ (æ¥è‡ª {eval_file.name}):\")\n",
    "        with open(eval_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # æœ€åä¸€è¡Œä¸ºæµ‹è¯•é›†ç»“æœï¼Œä¹‹å‰çš„è¡Œä¸ºæ¯ä¸ª epoch çš„éªŒè¯ç»“æœ\n",
    "            test_results = json.loads(lines[-1])\n",
    "            \n",
    "            print(f\"   - æµ‹è¯•é›†å‡†ç¡®ç‡ (Accuracy): {test_results.get('accuracy', 0)*100:.2f}%\")\n",
    "            print(f\"   - Macro F1 Score: {test_results.get('f1_score_macro', 0):.4f}\")\n",
    "            print(f\"   - Micro F1 Score: {test_results.get('f1_score_micro', 0):.4f}\")\n",
    "            print(f\"   - æµ‹è¯•è€—æ—¶: {test_results.get('test_time', 0):.2f}s\")\n",
    "    else:\n",
    "        print(f\"âš ï¸ æœªæ‰¾åˆ°è¯„ä¼° JSON æ–‡ä»¶: {eval_file}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æœªæ‰¾åˆ°æƒé‡æ–‡ä»¶ã€‚è¯·æ£€æŸ¥ï¼š\")\n",
    "    print(f\"   1. æœç´¢æ¨¡å¼æ˜¯å¦æ­£ç¡®: {search_pattern}\")\n",
    "    print(f\"   2. è®­ç»ƒæ˜¯å¦å› æ˜¾å­˜ä¸è¶³ç­‰åŸå› ä¸­é€”å´©æºƒã€‚\")\n",
    "    # å°è¯•åˆ—å‡º results ç›®å½•çœ‹çœ‹ç»“æ„\n",
    "    print(\"\\nå½“å‰ results ç›®å½•ç»“æ„:\")\n",
    "    os.system(f\"ls -R {PROJECT_ROOT / 'results' / 'supervised'} | head -n 20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb40699",
   "metadata": {},
   "source": [
    "## 9. å¯è§†åŒ–åˆ†æï¼šæ··æ·†çŸ©é˜µä¸åˆ†ç±»æŠ¥å‘Š\n",
    "\n",
    "ä¸ºäº†æ›´ç›´è§‚åœ°è¯„ä¼°æ¨¡å‹åœ¨å„ä¸ªç±»åˆ«ä¸Šçš„è¡¨ç°ï¼Œæˆ‘ä»¬ç»˜åˆ¶æ··æ·†çŸ©é˜µå¹¶æ‰“å°è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Šã€‚è¿™æœ‰åŠ©äºè¯†åˆ«æ¨¡å‹æœ€å®¹æ˜“æ··æ·†çš„æ”»å‡»ç±»å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1fc7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. è·å–ç±»åˆ«æ˜ å°„ (ä»æµ‹è¯•é›†æ–‡ä»¶ä¸­è¯»å–)\n",
    "test_df_for_labels = pd.read_parquet(TESTING_DATA)\n",
    "# å»ºç«‹ class -> type_q çš„æ˜ å°„\n",
    "class_to_label = test_df_for_labels.drop_duplicates('class').sort_values('class')[['class', 'type_q']]\n",
    "label_names = class_to_label['type_q'].tolist()\n",
    "\n",
    "print(f\"ğŸ·ï¸ ç±»åˆ«æ˜ å°„: {dict(zip(class_to_label['class'], class_to_label['type_q']))}\")\n",
    "\n",
    "# 2. åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç† (å•å¡æ¨¡å¼å³å¯)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}\")\n",
    "\n",
    "# åˆå§‹åŒ–åˆ†è¯å™¨\n",
    "viz_tokenizer = QA_Tokenizer_T5(opts)\n",
    "\n",
    "# åˆå§‹åŒ–æ•°æ®é›†\n",
    "viz_dataset_test = Classification_Dataset(opts, viz_tokenizer)\n",
    "viz_dataset_test.load_dataset(\"Test\", TESTING_DATA, \"every4\")\n",
    "viz_test_loader = DataLoader(viz_dataset_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# åˆå§‹åŒ–æ¨¡å‹åŒ…è£…å™¨\n",
    "viz_model_wrapper = Classification_model(opts, viz_tokenizer, None, viz_dataset_test)\n",
    "\n",
    "# åŠ è½½æœ€ä½³æƒé‡\n",
    "# æ³¨æ„ï¼šbest_weight_path æ˜¯åœ¨ä¸Šä¸€å•å…ƒæ ¼ä¸­å®šä¹‰çš„\n",
    "if 'best_weight_path' in locals():\n",
    "    print(f\"ğŸ“¥ æ­£åœ¨åŠ è½½æƒé‡: {best_weight_path}\")\n",
    "    viz_model_wrapper.classification_head.load_state_dict(torch.load(best_weight_path))\n",
    "    viz_model_wrapper.classification_head.to(device)\n",
    "    viz_model_wrapper.custom_model.to(device)\n",
    "    viz_model_wrapper.classification_head.eval()\n",
    "    viz_model_wrapper.custom_model.eval()\n",
    "\n",
    "    # 3. æ‰§è¡Œæ¨ç†\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, batch in tqdm(viz_test_loader, desc=\"ğŸ” æ­£åœ¨ç”Ÿæˆé¢„æµ‹\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label_class'].to(device) # æ³¨æ„ï¼šDataset ä¸­ label çš„ key æ˜¯ label_class\n",
    "            \n",
    "            # å‰å‘ä¼ æ’­é€»è¾‘ (å‚è€ƒ Classification_model.validation_batch)\n",
    "            model_outputs = viz_model_wrapper.custom_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # æ ¹æ® bottleneck ç±»å‹æå–ç‰¹å¾ (è¿™é‡Œå‡è®¾æ˜¯ mean æˆ–é»˜è®¤å– [:, 0, :])\n",
    "            if opts['bottleneck'] == 'mean':\n",
    "                # å¦‚æœæ˜¯ meanï¼ŒModelWithBottleneck å·²ç»å¤„ç†å¥½äº†è¿”å›\n",
    "                packet_representation = model_outputs\n",
    "            else:\n",
    "                packet_representation = model_outputs[:, 0, :]\n",
    "            \n",
    "            label_prob, _ = viz_model_wrapper.classification_head(packet_representation)\n",
    "            \n",
    "            preds = torch.argmax(label_prob, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # 4. ç»˜å›¾\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=label_names, yticklabels=label_names)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix - CIC-IDS2017 NIDS')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 5. æ‰“å°è¯¦ç»†æŠ¥å‘Š\n",
    "    print(\"\\nğŸ“ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=label_names))\n",
    "else:\n",
    "    print(\"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° best_weight_pathï¼Œè¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªå•å…ƒæ ¼ã€‚\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcapencoder",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
