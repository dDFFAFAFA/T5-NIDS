#!/usr/bin/env python
# coding: utf-8

# # PCAP_encoder å®Œæ•´å®éªŒæµç¨‹
# 
# æœ¬notebookæ¼”ç¤ºPCAP_encoderçš„å®Œæ•´å®éªŒé“¾è·¯ï¼š
# 1. æ•°æ®åŠ è½½ä¸åˆå¹¶ï¼ˆ18ä¸ªåˆ†ç‰‡ï¼‰
# 2. æ•°æ®é¢„å¤„ç†ä¸å¯è§†åŒ–
# 3. åŠ è½½é¢„è®­ç»ƒT5æ¨¡å‹
# 4. ç‰¹å¾æå–ä¸ç¼–ç 
# 5. åˆ†ç±»å™¨è®­ç»ƒä¸è¯„ä¼°
# 6. ç»“æœå¯è§†åŒ–

# ## 1. ç¯å¢ƒé…ç½®ä¸ä¾èµ–å¯¼å…¥

import os
import sys
import glob
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('../')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from tqdm.auto import tqdm
from typing import List, Tuple, Dict

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score,
    confusion_matrix, classification_report, roc_curve, auc
)
from sklearn.preprocessing import label_binarize

# è®¾ç½®ç»˜å›¾é£æ ¼
plt.style.use('seaborn-v0_8-whitegrid')
plt.rcParams['figure.figsize'] = [12, 8]
plt.rcParams['font.size'] = 12
plt.rcParams['axes.labelsize'] = 14
plt.rcParams['axes.titlesize'] = 16

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
print(f"ğŸ“¦ PyTorch ç‰ˆæœ¬: {torch.__version__}")

# è®¾ç½®éšæœºç§å­
SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)

print("âœ… ç¯å¢ƒé…ç½®å®Œæˆ!")

# ## 2. æ•°æ®åŠ è½½ä¸åˆå¹¶

# æ•°æ®è·¯å¾„
DATA_DIR = Path("/home/test/ybk/nids/encoder/PCAP_encoder/data/CIC-IDS2017/Payload-Bytes")
WEIGHTS_PATH = Path("../models/weights.pth")

# æŸ¥æ‰¾æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶
parquet_files = sorted(glob.glob(str(DATA_DIR / "Payload_Bytes_File_*.parquet")))
print(f"ğŸ“‚ æ‰¾åˆ° {len(parquet_files)} ä¸ªæ•°æ®åˆ†ç‰‡:")
for f in parquet_files[:5]:
    print(f"   - {Path(f).name}")
if len(parquet_files) > 5:
    print(f"   ... è¿˜æœ‰ {len(parquet_files) - 5} ä¸ªæ–‡ä»¶")

# åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰åˆ†ç‰‡
print("\nâ³ åŠ è½½æ•°æ®ä¸­...")
dfs = []
for f in tqdm(parquet_files, desc="åŠ è½½åˆ†ç‰‡"):
    df_part = pd.read_parquet(f)
    dfs.append(df_part)

df_full = pd.concat(dfs, ignore_index=True)
print(f"\nâœ… æ•°æ®åŠ è½½å®Œæˆ!")
print(f"   æ€»æ ·æœ¬æ•°: {len(df_full):,}")
print(f"   åˆ—æ•°: {len(df_full.columns)}")
print(f"   å†…å­˜å ç”¨: {df_full.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

# ## 3. æ•°æ®æ¢ç´¢ä¸å¯è§†åŒ–

# æŸ¥çœ‹æ•°æ®ç»“æ„
print("ğŸ“‹ æ•°æ®åˆ—ä¿¡æ¯:")
print(df_full.columns.tolist()[:20])
print(f"... å…± {len(df_full.columns)} åˆ—")

# è·å–æ ‡ç­¾åˆ—
if 'attack_label' in df_full.columns:
    label_col = 'attack_label'
elif 'Label' in df_full.columns:
    label_col = 'Label'
else:
    # å°è¯•æ‰¾åˆ°æ ‡ç­¾åˆ—
    for col in df_full.columns:
        if 'label' in col.lower() or 'attack' in col.lower():
            label_col = col
            break
    else:
        label_col = df_full.columns[-1]  # ä½¿ç”¨æœ€åä¸€åˆ—

print(f"\nğŸ·ï¸ æ ‡ç­¾åˆ—: {label_col}")

# æ ‡ç­¾åˆ†å¸ƒ
label_counts = df_full[label_col].value_counts()
print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
for label, count in label_counts.items():
    pct = count / len(df_full) * 100
    print(f"   {label}: {count:,} ({pct:.2f}%)")

# å¯è§†åŒ–æ ‡ç­¾åˆ†å¸ƒ
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# æŸ±çŠ¶å›¾
ax1 = axes[0]
colors = plt.cm.viridis(np.linspace(0, 0.8, len(label_counts)))
bars = ax1.bar(range(len(label_counts)), label_counts.values, color=colors)
ax1.set_xticks(range(len(label_counts)))
ax1.set_xticklabels(label_counts.index, rotation=45, ha='right')
ax1.set_xlabel('æ”»å‡»ç±»å‹')
ax1.set_ylabel('æ ·æœ¬æ•°é‡')
ax1.set_title('å„ç±»åˆ«æ ·æœ¬æ•°é‡åˆ†å¸ƒ')
for bar, count in zip(bars, label_counts.values):
    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, 
             f'{count:,}', ha='center', va='bottom', fontsize=10)

# é¥¼å›¾
ax2 = axes[1]
ax2.pie(label_counts.values, labels=label_counts.index, autopct='%1.1f%%',
        colors=colors, startangle=90)
ax2.set_title('å„ç±»åˆ«æ ·æœ¬æ¯”ä¾‹')

plt.tight_layout()
plt.savefig('../docs/label_distribution.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… æ ‡ç­¾åˆ†å¸ƒå›¾å·²ä¿å­˜!")

# ## 4. æ•°æ®é‡‡æ ·ä¸é¢„å¤„ç†

# ç”±äºæ•°æ®é‡å¯èƒ½å¾ˆå¤§ï¼Œè¿›è¡Œé‡‡æ ·
SAMPLE_SIZE = 10000  # é‡‡æ ·æ•°é‡ï¼Œå¯æ ¹æ®å†…å­˜è°ƒæ•´
MAX_BYTES = 64  # ä½¿ç”¨çš„æœ€å¤§å­—èŠ‚æ•°

print(f"\nğŸ”„ æ•°æ®é‡‡æ ·...")
print(f"   é‡‡æ ·å¤§å°: {SAMPLE_SIZE:,}")
print(f"   ä½¿ç”¨å­—èŠ‚æ•°: {MAX_BYTES}")

# åˆ†å±‚é‡‡æ ·
df_sampled = df_full.groupby(label_col, group_keys=False).apply(
    lambda x: x.sample(min(len(x), SAMPLE_SIZE // len(label_counts)), random_state=SEED)
)
print(f"   é‡‡æ ·åæ ·æœ¬æ•°: {len(df_sampled):,}")

# é‡‡æ ·åçš„æ ‡ç­¾åˆ†å¸ƒ
sampled_counts = df_sampled[label_col].value_counts()
print(f"\nğŸ“Š é‡‡æ ·åæ ‡ç­¾åˆ†å¸ƒ:")
for label, count in sampled_counts.items():
    pct = count / len(df_sampled) * 100
    print(f"   {label}: {count:,} ({pct:.2f}%)")

# è·å– payload åˆ—
payload_cols = sorted(
    [c for c in df_sampled.columns if c.startswith('payload_byte_')],
    key=lambda x: int(x.split('_')[-1])
)[:MAX_BYTES]

print(f"\nğŸ“¦ Payload åˆ—æ•°: {len(payload_cols)}")

# æå–å­—èŠ‚æ•°æ®å’Œæ ‡ç­¾
X_bytes = df_sampled[payload_cols].values.astype(np.uint8)
y_labels = df_sampled[label_col].values

print(f"   å­—èŠ‚æ•°æ®å½¢çŠ¶: {X_bytes.shape}")
print(f"   æ ‡ç­¾æ•°æ®å½¢çŠ¶: {y_labels.shape}")

# å¯è§†åŒ–éƒ¨åˆ†å­—èŠ‚æ•°æ®
fig, axes = plt.subplots(3, 1, figsize=(15, 10))

# ä¸åŒç±»åˆ«çš„å­—èŠ‚æ¨¡å¼
unique_labels = np.unique(y_labels)[:3]  # å–å‰3ä¸ªç±»åˆ«
for i, label in enumerate(unique_labels):
    ax = axes[i]
    idx = np.where(y_labels == label)[0][:5]  # æ¯ä¸ªç±»åˆ«å–5ä¸ªæ ·æœ¬
    for j, sample_idx in enumerate(idx):
        ax.plot(X_bytes[sample_idx], alpha=0.7, label=f'æ ·æœ¬ {j+1}')
    ax.set_title(f'ç±»åˆ«: {label}')
    ax.set_xlabel('å­—èŠ‚ä½ç½®')
    ax.set_ylabel('å­—èŠ‚å€¼')
    ax.legend(loc='upper right')
    ax.set_xlim(0, MAX_BYTES)

plt.tight_layout()
plt.savefig('../docs/byte_patterns.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… å­—èŠ‚æ¨¡å¼å›¾å·²ä¿å­˜!")

# ## 5. è½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼

def bytes_to_hex(byte_array: np.ndarray, format_type: str = 'every4') -> List[str]:
    """å°†å­—èŠ‚æ•°ç»„è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²"""
    hex_strings = []
    for row in tqdm(byte_array, desc="è½¬æ¢åå…­è¿›åˆ¶"):
        hex_str = ''.join(f'{int(b):02x}' for b in row)
        if format_type == 'every4':
            hex_str = ' '.join(hex_str[i:i+4] for i in range(0, len(hex_str), 4))
        elif format_type == 'every2':
            hex_str = ' '.join(hex_str[i:i+2] for i in range(0, len(hex_str), 2))
        hex_strings.append(hex_str)
    return hex_strings

print("\nğŸ”„ è½¬æ¢ä¸ºåå…­è¿›åˆ¶æ ¼å¼...")
contexts = bytes_to_hex(X_bytes, format_type='every4')

print(f"\nğŸ“ è½¬æ¢ç»“æœç¤ºä¾‹:")
for i in range(3):
    print(f"   æ ·æœ¬ {i+1} ({y_labels[i]}):")
    print(f"   åŸå§‹å­—èŠ‚: {X_bytes[i][:8].tolist()}")
    print(f"   åå…­è¿›åˆ¶: {contexts[i][:50]}...")
    print()

# ## 6. æ ‡ç­¾ç¼–ç 

# åˆ›å»ºæ ‡ç­¾æ˜ å°„
unique_labels_all = np.unique(y_labels)
label_to_id = {label: idx for idx, label in enumerate(unique_labels_all)}
id_to_label = {idx: label for label, idx in label_to_id.items()}

y_encoded = np.array([label_to_id[label] for label in y_labels])
num_classes = len(unique_labels_all)

print(f"ğŸ·ï¸ æ ‡ç­¾ç¼–ç :")
for label, idx in label_to_id.items():
    print(f"   {label} -> {idx}")
print(f"\n   ç±»åˆ«æ€»æ•°: {num_classes}")

# ## 7. åŠ è½½ T5 æ¨¡å‹å’Œé¢„è®­ç»ƒæƒé‡

from transformers import T5ForConditionalGeneration, T5TokenizerFast

MODEL_NAME = "t5-base"

print(f"\nâ³ åŠ è½½ T5 æ¨¡å‹: {MODEL_NAME}...")
tokenizer = T5TokenizerFast.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)

# åŠ è½½é¢„è®­ç»ƒæƒé‡
if WEIGHTS_PATH.exists():
    print(f"ğŸ“¥ åŠ è½½é¢„è®­ç»ƒæƒé‡: {WEIGHTS_PATH}")
    try:
        state_dict = torch.load(WEIGHTS_PATH, map_location='cpu', weights_only=True)
        model.load_state_dict(state_dict, strict=False)
        print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸ!")
    except Exception as e:
        print(f"âš ï¸ æƒé‡åŠ è½½å¤±è´¥: {e}")
        print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")
else:
    print(f"âš ï¸ æœªæ‰¾åˆ°é¢„è®­ç»ƒæƒé‡: {WEIGHTS_PATH}")
    print("   å°†ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡")

# æå–ç¼–ç å™¨å¹¶å†»ç»“
encoder = model.encoder.to(device)
for param in encoder.parameters():
    param.requires_grad = False
encoder.eval()

hidden_size = model.config.d_model
print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
print(f"   ç¼–ç å™¨éšè—ç»´åº¦: {hidden_size}")
print(f"   ç¼–ç å™¨å±‚æ•°: {model.config.num_layers}")
print(f"   æ³¨æ„åŠ›å¤´æ•°: {model.config.num_heads}")

# ## 8. æ„å»ºé—®ç­”æ ¼å¼è¾“å…¥å¹¶åˆ†è¯

QUESTION = "Classify the network packet"
MAX_LENGTH = 512

print(f"\nğŸ”¤ æ„å»ºæ¨¡å‹è¾“å…¥...")
print(f"   é—®é¢˜æ¨¡æ¿: '{QUESTION}'")
print(f"   æœ€å¤§é•¿åº¦: {MAX_LENGTH}")

# æ„å»ºè¾“å…¥æ–‡æœ¬
input_texts = [f"question: {QUESTION} context: {ctx}" for ctx in contexts]

print(f"\nğŸ“ è¾“å…¥ç¤ºä¾‹:")
print(f"   {input_texts[0][:100]}...")

# åˆ†è¯
print(f"\nâ³ åˆ†è¯ç¼–ç ä¸­...")
encodings = tokenizer(
    input_texts,
    padding=True,
    truncation=True,
    max_length=MAX_LENGTH,
    return_tensors="pt"
)

print(f"âœ… åˆ†è¯å®Œæˆ!")
print(f"   input_ids å½¢çŠ¶: {encodings['input_ids'].shape}")
print(f"   attention_mask å½¢çŠ¶: {encodings['attention_mask'].shape}")

# å±•ç¤ºåˆ†è¯ç»“æœ
print(f"\nğŸ“ åˆ†è¯ç»“æœç¤ºä¾‹:")
sample_tokens = tokenizer.convert_ids_to_tokens(encodings['input_ids'][0][:30])
print(f"   Tokens (å‰30): {sample_tokens}")

# ## 9. æ•°æ®é›†åˆ’åˆ†

print(f"\nğŸ”„ åˆ’åˆ†æ•°æ®é›†...")
X_train_idx, X_temp_idx, y_train, y_temp = train_test_split(
    np.arange(len(y_encoded)), y_encoded,
    test_size=0.4, stratify=y_encoded, random_state=SEED
)

X_val_idx, X_test_idx, y_val, y_test = train_test_split(
    X_temp_idx, y_temp,
    test_size=0.5, stratify=y_temp, random_state=SEED
)

print(f"   è®­ç»ƒé›†: {len(X_train_idx):,} æ ·æœ¬ ({len(X_train_idx)/len(y_encoded)*100:.1f}%)")
print(f"   éªŒè¯é›†: {len(X_val_idx):,} æ ·æœ¬ ({len(X_val_idx)/len(y_encoded)*100:.1f}%)")
print(f"   æµ‹è¯•é›†: {len(X_test_idx):,} æ ·æœ¬ ({len(X_test_idx)/len(y_encoded)*100:.1f}%)")

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
BATCH_SIZE = 32

def create_loader(indices, shuffle=False):
    input_ids = encodings['input_ids'][indices]
    attention_mask = encodings['attention_mask'][indices]
    labels = torch.tensor(y_encoded[indices], dtype=torch.long)
    dataset = TensorDataset(input_ids, attention_mask, labels)
    return DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=shuffle)

train_loader = create_loader(X_train_idx, shuffle=True)
val_loader = create_loader(X_val_idx, shuffle=False)
test_loader = create_loader(X_test_idx, shuffle=False)

print(f"\nğŸ“¦ DataLoader åˆ›å»ºå®Œæˆ:")
print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
print(f"   éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
print(f"   æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_loader)}")

# ## 10. ç‰¹å¾æå–

@torch.no_grad()
def extract_features(loader, encoder, device, bottleneck='mean'):
    """ä½¿ç”¨ç¼–ç å™¨æå–ç‰¹å¾"""
    encoder.eval()
    all_features = []
    all_labels = []
    
    for batch in tqdm(loader, desc="æå–ç‰¹å¾"):
        input_ids = batch[0].to(device)
        attention_mask = batch[1].to(device)
        labels = batch[2]
        
        outputs = encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)
        hidden_states = outputs.last_hidden_state
        
        if bottleneck == 'mean':
            features = hidden_states.mean(dim=1)
        elif bottleneck == 'first':
            features = hidden_states[:, 0, :]
        elif bottleneck == 'last':
            features = hidden_states[:, -1, :]
        else:
            features = hidden_states.mean(dim=1)
        
        all_features.append(features.cpu())
        all_labels.append(labels)
    
    return torch.cat(all_features), torch.cat(all_labels)

print("\nâ³ æå–ç‰¹å¾ä¸­...")
print("   ä½¿ç”¨ç“¶é¢ˆå±‚: mean pooling")

train_features, train_labels = extract_features(train_loader, encoder, device)
val_features, val_labels = extract_features(val_loader, encoder, device)
test_features, test_labels = extract_features(test_loader, encoder, device)

print(f"\nâœ… ç‰¹å¾æå–å®Œæˆ!")
print(f"   è®­ç»ƒé›†ç‰¹å¾: {train_features.shape}")
print(f"   éªŒè¯é›†ç‰¹å¾: {val_features.shape}")
print(f"   æµ‹è¯•é›†ç‰¹å¾: {test_features.shape}")

# å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ (ä½¿ç”¨ t-SNE æˆ– PCA)
from sklearn.decomposition import PCA

print("\nğŸ¨ å¯è§†åŒ–ç‰¹å¾åˆ†å¸ƒ (PCA)...")
pca = PCA(n_components=2, random_state=SEED)
test_features_2d = pca.fit_transform(test_features.numpy())

fig, ax = plt.subplots(figsize=(12, 10))
scatter = ax.scatter(
    test_features_2d[:, 0], test_features_2d[:, 1],
    c=test_labels.numpy(), cmap='viridis',
    alpha=0.6, s=30
)
plt.colorbar(scatter, label='ç±»åˆ«')
ax.set_xlabel('PC1')
ax.set_ylabel('PC2')
ax.set_title('T5 ç¼–ç å™¨ç‰¹å¾ (PCA é™ç»´)')

# æ·»åŠ ç±»åˆ«æ ‡æ³¨
for i, label in enumerate(unique_labels_all[:5]):  # åªæ ‡æ³¨å‰5ä¸ªç±»åˆ«
    idx = np.where(test_labels.numpy() == i)[0]
    if len(idx) > 0:
        center = test_features_2d[idx].mean(axis=0)
        ax.annotate(label, center, fontsize=12, fontweight='bold',
                   ha='center', va='center',
                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))

plt.tight_layout()
plt.savefig('../docs/feature_visualization.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… ç‰¹å¾åˆ†å¸ƒå›¾å·²ä¿å­˜!")

# ## 11. è®­ç»ƒåˆ†ç±»å™¨

print("\nğŸ‹ï¸ è®­ç»ƒåˆ†ç±»å™¨...")

# å®šä¹‰åˆ†ç±»å™¨
class Classifier(nn.Module):
    def __init__(self, input_dim, num_classes, hidden_dim=256):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.dropout = nn.Dropout(0.3)
        self.fc2 = nn.Linear(hidden_dim, num_classes)
    
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

classifier = Classifier(hidden_size, num_classes).to(device)
optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

# è®­ç»ƒå‚æ•°
EPOCHS = 30

# ç§»åŠ¨ç‰¹å¾åˆ°è®¾å¤‡
train_features = train_features.to(device)
train_labels = train_labels.to(device)
val_features = val_features.to(device)
val_labels = val_labels.to(device)

# è®­ç»ƒå¾ªç¯
history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}
best_val_acc = 0
best_epoch = 0

for epoch in range(EPOCHS):
    # è®­ç»ƒ
    classifier.train()
    optimizer.zero_grad()
    
    logits = classifier(train_features)
    train_loss = criterion(logits, train_labels)
    train_loss.backward()
    optimizer.step()
    
    train_preds = logits.argmax(dim=1)
    train_acc = (train_preds == train_labels).float().mean().item()
    
    # éªŒè¯
    classifier.eval()
    with torch.no_grad():
        val_logits = classifier(val_features)
        val_loss = criterion(val_logits, val_labels)
        val_preds = val_logits.argmax(dim=1)
        val_acc = (val_preds == val_labels).float().mean().item()
    
    # è®°å½•å†å²
    history['train_loss'].append(train_loss.item())
    history['val_loss'].append(val_loss.item())
    history['train_acc'].append(train_acc)
    history['val_acc'].append(val_acc)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        best_epoch = epoch
        torch.save(classifier.state_dict(), '../models/best_classifier.pth')
    
    if (epoch + 1) % 5 == 0:
        print(f"Epoch {epoch+1:3d}/{EPOCHS}: "
              f"Train Loss={train_loss.item():.4f}, Acc={train_acc:.4f} | "
              f"Val Loss={val_loss.item():.4f}, Acc={val_acc:.4f}")

print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
print(f"   æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (Epoch {best_epoch + 1})")

# å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Loss æ›²çº¿
ax1 = axes[0]
ax1.plot(history['train_loss'], label='è®­ç»ƒæŸå¤±', linewidth=2)
ax1.plot(history['val_loss'], label='éªŒè¯æŸå¤±', linewidth=2)
ax1.axvline(best_epoch, color='r', linestyle='--', alpha=0.5, label=f'æœ€ä½³æ¨¡å‹ (Epoch {best_epoch+1})')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.set_title('è®­ç»ƒå’ŒéªŒè¯æŸå¤±')
ax1.legend()
ax1.grid(True, alpha=0.3)

# Accuracy æ›²çº¿
ax2 = axes[1]
ax2.plot(history['train_acc'], label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
ax2.plot(history['val_acc'], label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
ax2.axvline(best_epoch, color='r', linestyle='--', alpha=0.5, label=f'æœ€ä½³æ¨¡å‹ (Epoch {best_epoch+1})')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.set_title('è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡')
ax2.legend()
ax2.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../docs/training_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… è®­ç»ƒæ›²çº¿å›¾å·²ä¿å­˜!")

# ## 12. æµ‹è¯•è¯„ä¼°

print("\nğŸ“Š æµ‹è¯•é›†è¯„ä¼°...")

# åŠ è½½æœ€ä½³æ¨¡å‹
classifier.load_state_dict(torch.load('../models/best_classifier.pth', weights_only=True))
classifier.eval()

# æµ‹è¯•
test_features = test_features.to(device)
test_labels = test_labels.to(device)

with torch.no_grad():
    test_logits = classifier(test_features)
    test_probs = F.softmax(test_logits, dim=1)
    test_preds = test_logits.argmax(dim=1)

test_preds_np = test_preds.cpu().numpy()
test_labels_np = test_labels.cpu().numpy()
test_probs_np = test_probs.cpu().numpy()

# è®¡ç®—æŒ‡æ ‡
accuracy = accuracy_score(test_labels_np, test_preds_np)
f1_macro = f1_score(test_labels_np, test_preds_np, average='macro')
f1_weighted = f1_score(test_labels_np, test_preds_np, average='weighted')
precision = precision_score(test_labels_np, test_preds_np, average='macro')
recall = recall_score(test_labels_np, test_preds_np, average='macro')

print(f"\nğŸ“ˆ æµ‹è¯•é›†æŒ‡æ ‡:")
print(f"   å‡†ç¡®ç‡ (Accuracy):     {accuracy*100:.2f}%")
print(f"   F1 Score (Macro):      {f1_macro*100:.2f}%")
print(f"   F1 Score (Weighted):   {f1_weighted*100:.2f}%")
print(f"   ç²¾ç¡®ç‡ (Precision):    {precision*100:.2f}%")
print(f"   å¬å›ç‡ (Recall):       {recall*100:.2f}%")

# åˆ†ç±»æŠ¥å‘Š
print(f"\nğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
target_names = [id_to_label[i] for i in range(num_classes)]
print(classification_report(test_labels_np, test_preds_np, target_names=target_names))

# ## 13. å¯è§†åŒ–åˆ†ç±»ç»“æœ

# æ··æ·†çŸ©é˜µ
print("\nğŸ¨ ç»˜åˆ¶æ··æ·†çŸ©é˜µ...")
cm = confusion_matrix(test_labels_np, test_preds_np)
cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]

fig, axes = plt.subplots(1, 2, figsize=(20, 8))

# åŸå§‹æ··æ·†çŸ©é˜µ
ax1 = axes[0]
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax1,
            xticklabels=target_names, yticklabels=target_names)
ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾')
ax1.set_ylabel('çœŸå®æ ‡ç­¾')
ax1.set_title('æ··æ·†çŸ©é˜µ (æ•°é‡)')
plt.setp(ax1.get_xticklabels(), rotation=45, ha='right')
plt.setp(ax1.get_yticklabels(), rotation=0)

# å½’ä¸€åŒ–æ··æ·†çŸ©é˜µ
ax2 = axes[1]
sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues', ax=ax2,
            xticklabels=target_names, yticklabels=target_names)
ax2.set_xlabel('é¢„æµ‹æ ‡ç­¾')
ax2.set_ylabel('çœŸå®æ ‡ç­¾')
ax2.set_title('æ··æ·†çŸ©é˜µ (å½’ä¸€åŒ–)')
plt.setp(ax2.get_xticklabels(), rotation=45, ha='right')
plt.setp(ax2.get_yticklabels(), rotation=0)

plt.tight_layout()
plt.savefig('../docs/confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜!")

# ## 14. ROC æ›²çº¿

print("\nğŸ¨ ç»˜åˆ¶ ROC æ›²çº¿...")

# äºŒå€¼åŒ–æ ‡ç­¾
y_test_bin = label_binarize(test_labels_np, classes=range(num_classes))

# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„ ROC æ›²çº¿
fpr = dict()
tpr = dict()
roc_auc = dict()

for i in range(num_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], test_probs_np[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# è®¡ç®—å¾®å¹³å‡ ROC
fpr["micro"], tpr["micro"], _ = roc_curve(y_test_bin.ravel(), test_probs_np.ravel())
roc_auc["micro"] = auc(fpr["micro"], tpr["micro"])

# ç»˜åˆ¶ ROC æ›²çº¿
fig, ax = plt.subplots(figsize=(12, 10))

# ç»˜åˆ¶å¾®å¹³å‡ ROC
ax.plot(fpr["micro"], tpr["micro"],
        label=f'å¾®å¹³å‡ ROC (AUC = {roc_auc["micro"]:.3f})',
        color='deeppink', linestyle=':', linewidth=3)

# ç»˜åˆ¶æ¯ä¸ªç±»åˆ«çš„ ROC
colors = plt.cm.viridis(np.linspace(0, 0.8, num_classes))
for i, color in enumerate(colors):
    if i < len(target_names):
        ax.plot(fpr[i], tpr[i], color=color, linewidth=2,
                label=f'{target_names[i]} (AUC = {roc_auc[i]:.3f})')

# ç»˜åˆ¶å¯¹è§’çº¿
ax.plot([0, 1], [0, 1], 'k--', linewidth=1)

ax.set_xlim([0.0, 1.0])
ax.set_ylim([0.0, 1.05])
ax.set_xlabel('å‡é˜³ç‡ (False Positive Rate)')
ax.set_ylabel('çœŸé˜³ç‡ (True Positive Rate)')
ax.set_title('å¤šåˆ†ç±» ROC æ›²çº¿')
ax.legend(loc='lower right', fontsize=10)
ax.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('../docs/roc_curves.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… ROC æ›²çº¿å›¾å·²ä¿å­˜!")

# ## 15. æ¯ç±»åˆ«æ€§èƒ½å¯¹æ¯”

print("\nğŸ¨ ç»˜åˆ¶å„ç±»åˆ«æ€§èƒ½å¯¹æ¯”...")

# è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
per_class_metrics = {
    'accuracy': [],
    'precision': [],
    'recall': [],
    'f1': []
}

for i in range(num_classes):
    # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    class_mask = test_labels_np == i
    if class_mask.sum() > 0:
        class_acc = (test_preds_np[class_mask] == i).mean()
        per_class_metrics['accuracy'].append(class_acc)
    else:
        per_class_metrics['accuracy'].append(0)

# ä½¿ç”¨ sklearn è®¡ç®— per-class æŒ‡æ ‡
per_class_precision = precision_score(test_labels_np, test_preds_np, average=None, zero_division=0)
per_class_recall = recall_score(test_labels_np, test_preds_np, average=None, zero_division=0)
per_class_f1 = f1_score(test_labels_np, test_preds_np, average=None, zero_division=0)

per_class_metrics['precision'] = per_class_precision.tolist()
per_class_metrics['recall'] = per_class_recall.tolist()
per_class_metrics['f1'] = per_class_f1.tolist()

# ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾
fig, ax = plt.subplots(figsize=(14, 8))

x = np.arange(num_classes)
width = 0.2

bars1 = ax.bar(x - 1.5*width, per_class_metrics['accuracy'], width, label='Accuracy', color='steelblue')
bars2 = ax.bar(x - 0.5*width, per_class_metrics['precision'], width, label='Precision', color='coral')
bars3 = ax.bar(x + 0.5*width, per_class_metrics['recall'], width, label='Recall', color='seagreen')
bars4 = ax.bar(x + 1.5*width, per_class_metrics['f1'], width, label='F1-Score', color='orchid')

ax.set_xlabel('ç±»åˆ«')
ax.set_ylabel('åˆ†æ•°')
ax.set_title('å„ç±»åˆ«æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”')
ax.set_xticks(x)
ax.set_xticklabels(target_names, rotation=45, ha='right')
ax.legend()
ax.set_ylim(0, 1.1)
ax.grid(True, alpha=0.3, axis='y')

# æ·»åŠ æ•°å€¼æ ‡æ³¨
for bars in [bars1, bars2, bars3, bars4]:
    for bar in bars:
        height = bar.get_height()
        if height > 0.1:
            ax.annotate(f'{height:.2f}',
                       xy=(bar.get_x() + bar.get_width() / 2, height),
                       xytext=(0, 3), textcoords="offset points",
                       ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.savefig('../docs/per_class_metrics.png', dpi=150, bbox_inches='tight')
plt.show()

print("âœ… å„ç±»åˆ«æ€§èƒ½å›¾å·²ä¿å­˜!")

# ## 16. é¢„æµ‹ç»“æœå±•ç¤º

print("\nğŸ“‹ é¢„æµ‹ç»“æœç¤ºä¾‹:")
print("-" * 80)

# éšæœºé€‰æ‹© 10 ä¸ªæ ·æœ¬å±•ç¤º
np.random.seed(SEED)
sample_indices = np.random.choice(len(test_labels_np), size=10, replace=False)

correct_count = 0
for idx in sample_indices:
    true_label = target_names[test_labels_np[idx]]
    pred_label = target_names[test_preds_np[idx]]
    confidence = test_probs_np[idx].max() * 100
    is_correct = "âœ…" if true_label == pred_label else "âŒ"
    if true_label == pred_label:
        correct_count += 1
    
    print(f"æ ·æœ¬ {idx:4d}: çœŸå®={true_label:15s} | é¢„æµ‹={pred_label:15s} | ç½®ä¿¡åº¦={confidence:5.1f}% | {is_correct}")

print("-" * 80)
print(f"ç¤ºä¾‹å‡†ç¡®ç‡: {correct_count}/10 = {correct_count/10*100:.1f}%")

# ## 17. å®éªŒæ€»ç»“

print("\n" + "="*80)
print("ğŸ“Š å®éªŒæ€»ç»“")
print("="*80)

print(f"""
ğŸ”¬ å®éªŒé…ç½®:
   - æ•°æ®é›†: CIC-IDS2017 Payload-Bytes
   - æ•°æ®åˆ†ç‰‡: {len(parquet_files)} ä¸ªæ–‡ä»¶
   - é‡‡æ ·å¤§å°: {SAMPLE_SIZE:,}
   - Payload å­—èŠ‚æ•°: {MAX_BYTES}
   - ç±»åˆ«æ•°: {num_classes}

ğŸ§  æ¨¡å‹é…ç½®:
   - ç¼–ç å™¨: {MODEL_NAME}
   - éšè—ç»´åº¦: {hidden_size}
   - ç“¶é¢ˆå±‚: mean pooling
   - åˆ†ç±»å™¨: 2å±‚ MLP ({hidden_size} -> 256 -> {num_classes})

ğŸ“ˆ æœ€ç»ˆæ€§èƒ½:
   - æµ‹è¯•å‡†ç¡®ç‡: {accuracy*100:.2f}%
   - F1 Score (Macro): {f1_macro*100:.2f}%
   - F1 Score (Weighted): {f1_weighted*100:.2f}%
   - ç²¾ç¡®ç‡: {precision*100:.2f}%
   - å¬å›ç‡: {recall*100:.2f}%

ğŸ’¾ ä¿å­˜çš„æ–‡ä»¶:
   - åˆ†ç±»å™¨æƒé‡: ../models/best_classifier.pth
   - æ ‡ç­¾åˆ†å¸ƒå›¾: ../docs/label_distribution.png
   - å­—èŠ‚æ¨¡å¼å›¾: ../docs/byte_patterns.png
   - ç‰¹å¾å¯è§†åŒ–: ../docs/feature_visualization.png
   - è®­ç»ƒæ›²çº¿å›¾: ../docs/training_curves.png
   - æ··æ·†çŸ©é˜µå›¾: ../docs/confusion_matrix.png
   - ROC æ›²çº¿å›¾: ../docs/roc_curves.png
   - ç±»åˆ«æ€§èƒ½å›¾: ../docs/per_class_metrics.png
""")

print("ğŸ‰ å®éªŒå®Œæˆ!")
