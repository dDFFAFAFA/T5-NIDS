# %% [markdown]
# # PCAP-Encoder NIDS å¾®è°ƒå®Œæ•´æµç¨‹ (ä½¿ç”¨ä½œè€…ç»„ä»¶)
# 
# æœ¬ Notebook ä½¿ç”¨ä½œè€…æä¾›çš„æ ¸å¿ƒç»„ä»¶è¿›è¡Œ NIDS åˆ†ç±»å¾®è°ƒï¼š
# - `Classification_Dataset`: æ•°æ®åŠ è½½
# - `Classification_model`: è®­ç»ƒæµç¨‹
# - `ModelWithBottleneck`: æ¨¡å‹å°è£…
# 
# **ä½¿ç”¨æ–¹æ³•**: åœ¨ VS Code ä¸­ä½¿ç”¨ "Run Cell" é€ä¸ªå•å…ƒæ ¼è¿è¡Œï¼Œæˆ–åœ¨ Jupyter ä¸­æ‰“å¼€

# %% [markdown]
# ## 1. ç¯å¢ƒé…ç½®

# %%
import os
import sys
import glob
import json
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
from pathlib import Path
from tqdm.auto import tqdm

import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path("/Users/changye/Desktop/æœŸåˆŠ/æ¨¡å—åº“/Debunk_Traffic_Representation-master/code/PCAP_encoder")
os.chdir(PROJECT_ROOT)
sys.path.insert(0, str(PROJECT_ROOT))

print(f"ğŸ“‚ é¡¹ç›®æ ¹ç›®å½•: {PROJECT_ROOT}")
print(f"ğŸ–¥ï¸ è®¾å¤‡: {'cuda' if torch.cuda.is_available() else 'cpu'}")
print(f"ğŸ“¦ PyTorch: {torch.__version__}")

# %% [markdown]
# ## 2. å¯¼å…¥ä½œè€…çš„æ ¸å¿ƒæ¨¡å—

# %%
# å¯¼å…¥ä½œè€…çš„æ ¸å¿ƒç»„ä»¶
from Core.classes.dataset_for_classification import Classification_Dataset
from Core.classes.classification_model import Classification_model
from Core.classes.tokenizer import QA_Tokenizer_T5
from Core.classes.logger import TrainingExperimentLogger

print("âœ… ä½œè€…æ ¸å¿ƒæ¨¡å—å¯¼å…¥æˆåŠŸ!")
print("   - Classification_Dataset: æ•°æ®é›†ç±»")
print("   - Classification_model: è®­ç»ƒç®¡ç†ç±»")
print("   - QA_Tokenizer_T5: åˆ†è¯å™¨")
print("   - TrainingExperimentLogger: æ—¥å¿—è®°å½•å™¨")

# %% [markdown]
# ## 3. é…ç½®å‚æ•°
# 
# æ ¹æ®éœ€è¦ä¿®æ”¹ä»¥ä¸‹é…ç½®:

# %%
# ============================================================
# ğŸ“‹ å®éªŒé…ç½® - æ ¹æ®éœ€è¦ä¿®æ”¹è¿™é‡Œ
# ============================================================

# æ•°æ®è·¯å¾„ (ç»å¯¹è·¯å¾„)
DATA_DIR = PROJECT_ROOT / "data" / "CIC-IDS2017" / "Classification"
TRAINING_DATA = str(DATA_DIR / "train.parquet")
VALIDATION_DATA = str(DATA_DIR / "val.parquet")
TESTING_DATA = str(DATA_DIR / "test.parquet")

# é¢„è®­ç»ƒæƒé‡è·¯å¾„
PRETRAINED_MODEL_PATH = str(PROJECT_ROOT / "models" / "pretrained")

# è®­ç»ƒå‚æ•°
EPOCHS = 20
BATCH_SIZE = 24
LEARNING_RATE = 0.001  # å†»ç»“ç¼–ç å™¨ç”¨ 0.001, è§£å†»ç”¨ 0.00001
SEED = 42

# æ˜¯å¦å†»ç»“ç¼–ç å™¨
FIX_ENCODER = True  # True = å†»ç»“, False = è§£å†»å¾®è°ƒ

# æ¨¡å‹é…ç½®
MODEL_NAME = "T5-base"
TOKENIZER_NAME = "T5-base"
BOTTLENECK = "mean"  # mean, first, last, attention
MAX_QST_LENGTH = 512
MAX_ANS_LENGTH = 32

# å…¶ä»–
LOSS_TYPE = "normal"  # normal æˆ– weighted (å¤„ç†ç±»åˆ«ä¸å¹³è¡¡)
PERCENTAGE = 100  # ä½¿ç”¨æ•°æ®çš„ç™¾åˆ†æ¯” [1, 100]
LOG_LEVEL = "info"
OUTPUT_PATH = str(PROJECT_ROOT / "results")

# æ‰“å°é…ç½®
print("=" * 60)
print("ğŸ“‹ å®éªŒé…ç½®")
print("=" * 60)
print(f"ğŸ“‚ è®­ç»ƒæ•°æ®: {TRAINING_DATA}")
print(f"ğŸ“‚ éªŒè¯æ•°æ®: {VALIDATION_DATA}")
print(f"ğŸ“‚ æµ‹è¯•æ•°æ®: {TESTING_DATA}")
print(f"ğŸ“‚ é¢„è®­ç»ƒæƒé‡: {PRETRAINED_MODEL_PATH}")
print(f"ğŸ”§ å­¦ä¹ ç‡: {LEARNING_RATE}")
print(f"ğŸ”§ Epochs: {EPOCHS}")
print(f"ğŸ”§ Batch Size: {BATCH_SIZE}")
print(f"ğŸ”§ ç¼–ç å™¨çŠ¶æ€: {'å†»ç»“' if FIX_ENCODER else 'è§£å†»'}")
print(f"ğŸ”§ Bottleneck: {BOTTLENECK}")
print("=" * 60)

# %% [markdown]
# ## 4. æ•°æ®å‡†å¤‡ (å¯é€‰)
# 
# å¦‚æœè¿˜æ²¡æœ‰è½¬æ¢æ•°æ®ï¼Œè¿è¡Œæ­¤å•å…ƒæ ¼

# %%
def bytes_to_hex(byte_array: np.ndarray, format_type: str = 'every4') -> list:
    """å°†å­—èŠ‚æ•°ç»„è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²"""
    hex_strings = []
    for row in tqdm(byte_array, desc="è½¬æ¢åå…­è¿›åˆ¶", leave=False):
        hex_str = ''.join(f'{int(b):02x}' for b in row)
        if format_type == 'every4':
            hex_str = ' '.join(hex_str[i:i+4] for i in range(0, len(hex_str), 4))
        elif format_type == 'every2':
            hex_str = ' '.join(hex_str[i:i+2] for i in range(0, len(hex_str), 2))
        hex_strings.append(hex_str.strip())
    return hex_strings


def prepare_dataset_if_needed(
    input_dir: Path,
    output_dir: Path,
    max_bytes: int = 64,
    format_type: str = 'every4',
    sample_size: int = None
):
    """å¦‚æœæ•°æ®ä¸å­˜åœ¨ï¼Œåˆ™å‡†å¤‡æ•°æ®"""
    output_dir = Path(output_dir)
    train_file = output_dir / "train.parquet"
    
    if train_file.exists():
        print(f"âœ… æ•°æ®å·²å­˜åœ¨: {output_dir}")
        return True
    
    print(f"â³ æ•°æ®ä¸å­˜åœ¨ï¼Œå¼€å§‹è½¬æ¢...")
    input_dir = Path(input_dir)
    
    # åŠ è½½åŸå§‹æ•°æ®
    parquet_files = sorted(glob.glob(str(input_dir / "Payload_Bytes_File_*.parquet")))
    if not parquet_files:
        print(f"âŒ æœªæ‰¾åˆ°åŸå§‹æ•°æ®æ–‡ä»¶: {input_dir}")
        return False
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(parquet_files)} ä¸ªåˆ†ç‰‡")
    dfs = [pd.read_parquet(f) for f in tqdm(parquet_files, desc="åŠ è½½åˆ†ç‰‡")]
    df = pd.concat(dfs, ignore_index=True)
    print(f"âœ… åŠ è½½å®Œæˆ: {len(df):,} æ ·æœ¬")
    
    # è·å–æ ‡ç­¾åˆ—
    label_col = None
    for col in ['attack_label', 'Label', 'label']:
        if col in df.columns:
            label_col = col
            break
    if not label_col:
        print("âŒ æœªæ‰¾åˆ°æ ‡ç­¾åˆ—")
        return False
    
    # é‡‡æ ·
    if sample_size and sample_size < len(df):
        df = df.groupby(label_col, group_keys=False).apply(
            lambda x: x.sample(min(len(x), max(1, sample_size // df[label_col].nunique())), random_state=SEED)
        )
        print(f"ğŸ“Š é‡‡æ ·å: {len(df):,} æ ·æœ¬")
    
    # è·å– payload åˆ—
    payload_cols = sorted(
        [c for c in df.columns if c.startswith('payload_byte_')],
        key=lambda x: int(x.split('_')[-1])
    )[:max_bytes]
    
    # è½¬æ¢
    X_bytes = df[payload_cols].values.astype(np.uint8)
    y_labels = df[label_col].values
    contexts = bytes_to_hex(X_bytes, format_type=format_type)
    
    # æ ‡ç­¾æ˜ å°„
    unique_labels = sorted(df[label_col].unique())
    label_to_id = {label: idx for idx, label in enumerate(unique_labels)}
    
    # æ„å»º DataFrame
    result = pd.DataFrame({
        'question': 'Classify the network packet',
        'context': contexts,
        'class': [label_to_id[label] for label in y_labels],
        'type_q': y_labels
    })
    
    # åˆ’åˆ†
    train_df, temp_df = train_test_split(result, test_size=0.4, stratify=result['class'], random_state=SEED)
    val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['class'], random_state=SEED)
    
    # ä¿å­˜
    output_dir.mkdir(parents=True, exist_ok=True)
    train_df.to_parquet(output_dir / "train.parquet", index=False)
    val_df.to_parquet(output_dir / "val.parquet", index=False)
    test_df.to_parquet(output_dir / "test.parquet", index=False)
    
    with open(output_dir / "label_map.json", 'w') as f:
        json.dump(label_to_id, f, indent=2)
    
    print(f"âœ… æ•°æ®å·²ä¿å­˜:")
    print(f"   è®­ç»ƒé›†: {len(train_df):,}")
    print(f"   éªŒè¯é›†: {len(val_df):,}")
    print(f"   æµ‹è¯•é›†: {len(test_df):,}")
    return True

# %%
# æ£€æŸ¥å¹¶å‡†å¤‡æ•°æ®
# ä¿®æ”¹ RAW_DATA_DIR ä¸ºä½ çš„åŸå§‹ Payload-Bytes æ•°æ®ç›®å½•
RAW_DATA_DIR = PROJECT_ROOT / "data" / "CIC-IDS2017" / "Payload-Bytes"

prepare_dataset_if_needed(
    input_dir=RAW_DATA_DIR,
    output_dir=DATA_DIR,
    max_bytes=64,
    format_type='every4',
    sample_size=10000  # è®¾ä¸º None ä½¿ç”¨å…¨éƒ¨æ•°æ®
)

# %% [markdown]
# ## 5. æ£€æŸ¥æ•°æ®æ ¼å¼

# %%
# æ£€æŸ¥è½¬æ¢åçš„æ•°æ®
if Path(TRAINING_DATA).exists():
    train_df = pd.read_parquet(TRAINING_DATA)
    print(f"ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(train_df):,}")
    print(f"ğŸ“‹ åˆ—: {train_df.columns.tolist()}")
    print(f"\nğŸ” æ•°æ®æ ·ä¾‹:")
    print(train_df.head(3).to_string())
    print(f"\nğŸ“Š ç±»åˆ«åˆ†å¸ƒ:")
    print(train_df['type_q'].value_counts())
else:
    print(f"âŒ è®­ç»ƒæ•°æ®ä¸å­˜åœ¨: {TRAINING_DATA}")
    print("   è¯·å…ˆè¿è¡Œä¸Šä¸€ä¸ªå•å…ƒæ ¼å‡†å¤‡æ•°æ®")

# %% [markdown]
# ## 6. æ„å»ºé…ç½®å­—å…¸ (æ¨¡æ‹Ÿå‘½ä»¤è¡Œå‚æ•°)

# %%
# æ„å»º opts å­—å…¸ (ä½œè€…çš„ä»£ç ä½¿ç”¨è¿™ä¸ªæ ¼å¼)
opts = {
    # å®éªŒæ ‡è¯†
    "identifier": f"cicids_notebook_lr{LEARNING_RATE}_{'frozen' if FIX_ENCODER else 'unfrozen'}",
    "experiment": "CIC-IDS2017_NIDS",
    "task": "supervised",
    "clean_start": True,
    
    # æ¨¡å‹é…ç½®
    "model_name": MODEL_NAME,
    "tokenizer_name": TOKENIZER_NAME,
    "finetuned_path_model": PRETRAINED_MODEL_PATH,
    "bottleneck": BOTTLENECK,
    "pkt_repr_dim": 768,
    
    # è®­ç»ƒå‚æ•°
    "lr": LEARNING_RATE,
    "epochs": EPOCHS,
    "batch_size": BATCH_SIZE,
    "seed": SEED,
    "loss": LOSS_TYPE,
    "fix_encoder": FIX_ENCODER,
    
    # æ•°æ®å‚æ•°
    "training_data": TRAINING_DATA,
    "validation_data": VALIDATION_DATA,
    "testing_data": TESTING_DATA,
    "percentage": PERCENTAGE,
    "max_qst_length": MAX_QST_LENGTH,
    "max_ans_length": MAX_ANS_LENGTH,
    "input_format": "every4",
    
    # å…¶ä»–
    "output_path": OUTPUT_PATH,
    "log_level": LOG_LEVEL,
    "gpu": "0,",
    "use_cuda": torch.cuda.is_available(),
}

print("âœ… é…ç½®å­—å…¸æ„å»ºå®Œæˆ!")
for key, value in opts.items():
    print(f"   {key}: {value}")

# %% [markdown]
# ## 7. åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨

# %%
# åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
print("â³ åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨...")
logger = TrainingExperimentLogger(opts)
logger.start_experiment(opts)
print(f"âœ… æ—¥å¿—è®°å½•å™¨åˆå§‹åŒ–å®Œæˆ!")
print(f"   å®éªŒID: {opts['identifier']}")

# %% [markdown]
# ## 8. åˆå§‹åŒ–åˆ†è¯å™¨

# %%
# åˆå§‹åŒ–åˆ†è¯å™¨
print("â³ åˆå§‹åŒ–åˆ†è¯å™¨...")
tokenizer_obj = QA_Tokenizer_T5(opts)
print(f"âœ… åˆ†è¯å™¨åˆå§‹åŒ–å®Œæˆ!")
print(f"   æ¨¡å‹: {opts['tokenizer_name']}")

# %% [markdown]
# ## 9. åŠ è½½æ•°æ®é›†

# %%
# åŠ è½½è®­ç»ƒ/éªŒè¯æ•°æ®é›†
print("â³ åŠ è½½è®­ç»ƒæ•°æ®é›†...")
dataset_trainval = Classification_Dataset(opts, tokenizer_obj)
dataset_trainval.load_dataset(
    "Train",
    opts["training_data"],
    opts['input_format'],
    opts["validation_data"],
    opts["percentage"]
)
print(f"âœ… è®­ç»ƒ/éªŒè¯æ•°æ®é›†åŠ è½½å®Œæˆ!")
print(f"   è®­ç»ƒé›†å¤§å°: {dataset_trainval.size_train}")
print(f"   éªŒè¯é›†å¤§å°: {dataset_trainval.size_val}")

# %%
# åŠ è½½æµ‹è¯•æ•°æ®é›†
print("â³ åŠ è½½æµ‹è¯•æ•°æ®é›†...")
dataset_test = Classification_Dataset(opts, tokenizer_obj)
dataset_test.load_dataset("Test", opts["testing_data"], opts['input_format'])
print(f"âœ… æµ‹è¯•æ•°æ®é›†åŠ è½½å®Œæˆ!")
print(f"   æµ‹è¯•é›†å¤§å°: {len(dataset_test)}")

# %%
# æ£€æŸ¥æ•°æ®æ ·ä¾‹
print("\nğŸ” æ•°æ®æ ·ä¾‹æ£€æŸ¥:")
sample_idx, sample_data = dataset_trainval[0]
print(f"   ç´¢å¼•: {sample_idx}")
print(f"   input_ids shape: {sample_data['input_ids'].shape}")
print(f"   attention_mask shape: {sample_data['attention_mask'].shape}")
print(f"   label_class: {sample_data['label_class']}")

# %% [markdown]
# ## 10. åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹

# %%
# åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹
print("â³ åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹...")
model_obj = Classification_model(opts, tokenizer_obj, dataset_trainval, dataset_test)
print(f"âœ… åˆ†ç±»æ¨¡å‹åˆå§‹åŒ–å®Œæˆ!")

# %% [markdown]
# ## 11. å¼€å§‹è®­ç»ƒ
# 
# âš ï¸ è¿™ä¸€æ­¥ä¼šå¼€å§‹å®é™…è®­ç»ƒï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´

# %%
# å¼€å§‹è®­ç»ƒ
print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
print("=" * 60)
model_obj.run(logger, opts)
print("=" * 60)
print("âœ… è®­ç»ƒå®Œæˆ!")

# %% [markdown]
# ## 12. ç»“æŸå®éªŒ

# %%
# ç»“æŸå®éªŒ
logger.end_experiment()
print("ğŸ‰ å®éªŒç»“æŸ!")

# %% [markdown]
# ## 13. ç»“æœåˆ†æ (å¯é€‰)
# 
# å¦‚æœè®­ç»ƒå®Œæˆåæƒ³æŸ¥çœ‹ç»“æœ:

# %%
# æŸ¥çœ‹ç»“æœç›®å½•
results_dir = Path(OUTPUT_PATH) / opts['experiment'] / opts['identifier']
if results_dir.exists():
    print(f"ğŸ“‚ ç»“æœç›®å½•: {results_dir}")
    for f in results_dir.iterdir():
        print(f"   - {f.name}")
else:
    print(f"âš ï¸ ç»“æœç›®å½•ä¸å­˜åœ¨: {results_dir}")

# %% [markdown]
# ---
# ## å¤‡æ³¨
# 
# ### å¦‚æœè®­ç»ƒå‡ºé”™ï¼Œæ£€æŸ¥ä»¥ä¸‹å‡ ç‚¹:
# 1. æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®
# 2. é¢„è®­ç»ƒæƒé‡è·¯å¾„æ˜¯å¦æ­£ç¡®
# 3. GPU å†…å­˜æ˜¯å¦è¶³å¤Ÿ (å¯å°è¯•å‡å° BATCH_SIZE)
# 4. æŸ¥çœ‹å…·ä½“é”™è¯¯ä¿¡æ¯ï¼Œå®šä½é—®é¢˜

# %%
# è°ƒè¯•: æ‰‹åŠ¨æ£€æŸ¥æ¨¡å‹å®šä¹‰
print("ğŸ” è°ƒè¯•ä¿¡æ¯:")
print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"   GPU: {torch.cuda.get_device_name(0)}")
    print(f"   GPU å†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
print(f"   é¢„è®­ç»ƒæƒé‡è·¯å¾„å­˜åœ¨: {Path(PRETRAINED_MODEL_PATH).exists()}")
print(f"   è®­ç»ƒæ•°æ®å­˜åœ¨: {Path(TRAINING_DATA).exists()}")
