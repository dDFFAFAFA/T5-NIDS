# Part 2: test_CIC-IDS æµ‹è¯•æ¨¡å—å®Œæ•´é“¾è·¯

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ¨¡å—æ¶æ„](#æ¨¡å—æ¶æ„)
- [è„šæœ¬ 1: demo_minimal_data.py](#è„šæœ¬-1-demo_minimal_datapy)
- [è„šæœ¬ 2: demo_pipeline.py](#è„šæœ¬-2-demo_pipelinepy)
- [è„šæœ¬ 3: eval_with_encoder_head.py](#è„šæœ¬-3-eval_with_encoder_headpy)
- [è„šæœ¬ 4: evaluate_cic_ids.py](#è„šæœ¬-4-evaluate_cic_idspy)
- [è„šæœ¬ 5: data_downloads.py](#è„šæœ¬-5-data_downloadspy)
- [è„šæœ¬ 6: inspect_labels.py](#è„šæœ¬-6-inspect_labelspy)
- [å®Œæ•´å·¥ä½œæµç¨‹](#å®Œæ•´å·¥ä½œæµç¨‹)

---

## æ¦‚è¿°

`test_CIC-IDS` ç›®å½•åŒ…å«äº†ä¸€ç³»åˆ—ç”¨äºæµ‹è¯•å’Œè¯„ä¼° PCAP_encoder æ¨¡å‹çš„è„šæœ¬ã€‚è¿™äº›è„šæœ¬ä¸»è¦ç”¨äºï¼š

1. **ç”Ÿæˆæ¼”ç¤ºæ•°æ®**: æ— éœ€ä¸‹è½½å¤§å‹æ•°æ®é›†å³å¯å¿«é€ŸéªŒè¯æµç¨‹
2. **ç«¯åˆ°ç«¯æ¼”ç¤º**: å±•ç¤ºä»æ•°æ®åˆ°é¢„æµ‹çš„å®Œæ•´æµç¨‹
3. **æ¨¡å‹è¯„ä¼°**: ä½¿ç”¨é¢„è®­ç»ƒæƒé‡è¯„ä¼°æ¨¡å‹æ€§èƒ½
4. **æ•°æ®å¤„ç†**: ä¸‹è½½å’Œæ£€æŸ¥ CIC-IDS2017 æ•°æ®é›†

### ç›®æ ‡æ•°æ®é›†

**CIC-IDS2017** (Canadian Institute for Cybersecurity - Intrusion Detection System 2017)
- ç½‘ç»œå…¥ä¾µæ£€æµ‹æ•°æ®é›†
- åŒ…å«æ­£å¸¸æµé‡å’Œå¤šç§æ”»å‡»ç±»å‹
- æ•°æ®æ ¼å¼: Parquet æ–‡ä»¶ï¼ŒåŒ…å« Payload å­—èŠ‚å’Œæ”»å‡»æ ‡ç­¾

---

## æ¨¡å—æ¶æ„

```mermaid
graph TB
    subgraph "æ•°æ®å‡†å¤‡"
        A1[demo_minimal_data.py] --> A2[ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®]
        A3[data_downloads.py] --> A4[ä¸‹è½½ CIC-IDS2017]
        A5[inspect_labels.py] --> A6[æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ]
    end
    
    subgraph "æ¨¡å‹æ¼”ç¤º"
        B1[demo_pipeline.py] --> B2[ç«¯åˆ°ç«¯æµç¨‹æ¼”ç¤º]
    end
    
    subgraph "æ¨¡å‹è¯„ä¼°"
        C1[eval_with_encoder_head.py] --> C2[ç¼–ç å™¨+åˆ†ç±»å¤´è¯„ä¼°]
        C3[evaluate_cic_ids.py] --> C4[CIC-IDS å®Œæ•´è¯„ä¼°]
    end
    
    A2 --> B2
    A4 --> C2
    A4 --> C4
    
    style A2 fill:#e1f5ff
    style B2 fill:#fff4e1
    style C2 fill:#e8f5e9
    style C4 fill:#f3e5f5
```

---

## è„šæœ¬ 1: demo_minimal_data.py

### åŠŸèƒ½

ç”Ÿæˆæœ€å°é‡çš„æ¨¡æ‹Ÿç½‘ç»œæµé‡æ•°æ®ï¼Œç”¨äºå¿«é€ŸéªŒè¯ PCAP_encoder çš„å·¥ä½œæµç¨‹ï¼Œæ— éœ€ä¸‹è½½ä»»ä½•å¤–éƒ¨æ•°æ®é›†ã€‚

### è¾“å…¥

**æ— éœ€å¤–éƒ¨è¾“å…¥**ï¼Œæ‰€æœ‰æ•°æ®éƒ½æ˜¯æœ¬åœ°åˆæˆçš„ã€‚

**é…ç½®å‚æ•°**:
```python
n_samples = 50      # ç”Ÿæˆçš„æ ·æœ¬æ•°é‡
n_bytes = 64        # æ¯ä¸ªæ ·æœ¬çš„å­—èŠ‚æ•°
seed = 42           # éšæœºç§å­
```

### å¤„ç†æµç¨‹

```mermaid
graph LR
    A[é…ç½®å‚æ•°] --> B[ç”Ÿæˆæ ‡ç­¾]
    B --> C[ç”Ÿæˆè½½è·å­—èŠ‚]
    C --> D[æ„å»º DataFrame]
    D --> E[å±•ç¤ºæ‘˜è¦]
    E --> F[å±•ç¤ºè½¬æ¢ç¤ºä¾‹]
    F --> G[ä¿å­˜ Parquet]
    
    style A fill:#e1f5ff
    style G fill:#fff4e1
```

#### æ­¥éª¤ 1: ç”Ÿæˆæ ‡ç­¾

**å‡½æ•°**: `generate_synthetic_traffic()`

```python
# å®šä¹‰æ”»å‡»ç±»å‹
attack_types = ["BENIGN", "FTP-Patator", "SSH-Patator"]

# æŒ‰æ¯”ä¾‹åˆ†é…æ ‡ç­¾
labels = np.random.choice(
    attack_types,
    size=n_samples,
    p=[0.6, 0.25, 0.15]  # 60% æ­£å¸¸, 25% FTPæ”»å‡», 15% SSHæ”»å‡»
)
```

**è¾“å‡º**: æ ‡ç­¾æ•°ç»„
```python
['BENIGN', 'BENIGN', 'FTP-Patator', 'BENIGN', 'SSH-Patator', ...]
```

#### æ­¥éª¤ 2: ç”Ÿæˆè½½è·å­—èŠ‚

**æ ¹æ®æ ‡ç­¾ç±»å‹ç”Ÿæˆä¸åŒæ¨¡å¼çš„æ•°æ®**:

```python
for label in labels:
    if label == "BENIGN":
        # æ­£å¸¸æµé‡ï¼šè¾ƒä½çš„å­—èŠ‚å€¼ï¼Œæ¨¡æ‹Ÿ HTTP/HTTPS
        payload = np.random.randint(0, 128, size=n_bytes)
        
    elif label == "FTP-Patator":
        # FTP æš´åŠ›æ”»å‡»ï¼šåŒ…å«ç‰¹å®šæ¨¡å¼
        payload = np.random.randint(32, 127, size=n_bytes)  # ASCII å¯æ‰“å°å­—ç¬¦
        payload[:4] = [70, 84, 80, 32]  # "FTP " çš„ ASCII
        
    else:  # SSH-Patator
        # SSH æš´åŠ›æ”»å‡»ï¼šåŒ…å« SSH åè®®ç‰¹å¾
        payload = np.random.randint(0, 255, size=n_bytes)
        payload[:4] = [83, 83, 72, 45]  # "SSH-" çš„ ASCII
```

**è¾“å‡º**: è½½è·æ•°ç»„
```python
# BENIGN ç¤ºä¾‹
[45, 67, 89, 12, 34, 56, 78, 90, ...]  # å€¼åœ¨ 0-127

# FTP-Patator ç¤ºä¾‹
[70, 84, 80, 32, 117, 115, 101, 114, ...]  # å‰4å­—èŠ‚æ˜¯ "FTP "

# SSH-Patator ç¤ºä¾‹
[83, 83, 72, 45, 234, 156, 78, 201, ...]  # å‰4å­—èŠ‚æ˜¯ "SSH-"
```

#### æ­¥éª¤ 3: æ„å»º DataFrame

```python
# æ„å»ºåˆ—å­—å…¸
columns = {
    f"payload_byte_{i+1}": payload_array[:, i] 
    for i in range(n_bytes)
}
columns["attack_label"] = labels

# åˆ›å»º DataFrame
df = pd.DataFrame(columns)
```

**è¾“å‡º**: DataFrame ç»“æ„
```
   payload_byte_1  payload_byte_2  ...  payload_byte_64  attack_label
0              45              67  ...               23        BENIGN
1              70              84  ...               89  FTP-Patator
2              83              83  ...              156  SSH-Patator
...
```

#### æ­¥éª¤ 4: å±•ç¤ºæ•°æ®æ‘˜è¦

**å‡½æ•°**: `show_data_summary(df)`

**è¾“å‡ºç¤ºä¾‹**:
```
ğŸ“Š æ•°æ®æ‘˜è¦:
   - æ€»æ ·æœ¬æ•°: 50
   - å­—èŠ‚åˆ—æ•°: 64

ğŸ“ˆ æ ‡ç­¾åˆ†å¸ƒ:
   BENIGN         :  30 ( 60.0%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   FTP-Patator    :  13 ( 26.0%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
   SSH-Patator    :   7 ( 14.0%) â–ˆâ–ˆâ–ˆ

ğŸ“‹ å‰ 5 æ¡è®°å½• (éƒ¨åˆ†åˆ—):
   attack_label  payload_byte_1  payload_byte_2  payload_byte_3  payload_byte_4  payload_byte_5
0        BENIGN              45              67              89              12              34
1  FTP-Patator              70              84              80              32             117
2  SSH-Patator              83              83              72              45             234
3        BENIGN              23              56              78              90              11
4        BENIGN              67              89              12              34              56
```

#### æ­¥éª¤ 5: å±•ç¤ºåå…­è¿›åˆ¶è½¬æ¢

**å‡½æ•°**: `show_hex_conversion(df, n_examples=3)`

**å¤„ç†**:
```python
# è·å–å‰ 16 å­—èŠ‚
bytes_data = [int(row[col]) for col in payload_cols[:16]]

# è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
hex_str = "".join(f"{b:02x}" for b in bytes_data)

# æ¯ 4 å­—ç¬¦åˆ†ç»„ï¼ˆPCAP_encoder çš„ "every4" æ ¼å¼ï¼‰
hex_grouped = " ".join(hex_str[j:j+4] for j in range(0, len(hex_str), 4))
```

**è¾“å‡ºç¤ºä¾‹**:
```
ğŸ”„ Payload å­—èŠ‚ â†’ åå…­è¿›åˆ¶å­—ç¬¦ä¸²è½¬æ¢ç¤ºä¾‹:

   æ ·æœ¬ 1 (BENIGN):
   åŸå§‹å­—èŠ‚ (å‰8ä¸ª): [ 45  67  89  12  34  56  78  90 ...]
   åå…­è¿›åˆ¶å­—ç¬¦ä¸²:   2d43 590c 2238 4e5a 1122 3344 5566 7788

   æ ·æœ¬ 2 (FTP-Patator):
   åŸå§‹å­—èŠ‚ (å‰8ä¸ª): [ 70  84  80  32 117 115 101 114 ...]
   åå…­è¿›åˆ¶å­—ç¬¦ä¸²:   4654 5020 7573 6572 6e61 6d65 3a20

   æ ·æœ¬ 3 (SSH-Patator):
   åŸå§‹å­—èŠ‚ (å‰8ä¸ª): [ 83  83  72  45 234 156  78 201 ...]
   åå…­è¿›åˆ¶å­—ç¬¦ä¸²:   5353 482d ea9c 4ec9 a1b2 c3d4 e5f6
```

#### æ­¥éª¤ 6: ä¿å­˜ä¸º Parquet

**å‡½æ•°**: `save_as_parquet(df, output_dir)`

```python
output_path = output_dir / "demo_payload_bytes.parquet"
df.to_parquet(output_path, index=False)
```

**è¾“å‡º**:
```
ğŸ’¾ æ•°æ®å·²ä¿å­˜:
   è·¯å¾„: /path/to/data/demo/demo_payload_bytes.parquet
   å¤§å°: 12.3 KB
```

### è¾“å‡º

#### æ–‡ä»¶
- **è·¯å¾„**: `../data/demo/demo_payload_bytes.parquet`
- **æ ¼å¼**: Parquet
- **å¤§å°**: ~10-20 KB (50 æ ·æœ¬ Ã— 64 å­—èŠ‚)

#### æ•°æ®ç»“æ„
```python
DataFrame columns:
- payload_byte_1 to payload_byte_64: int (0-255)
- attack_label: str ('BENIGN', 'FTP-Patator', 'SSH-Patator')
```

### ä½¿ç”¨æ–¹æ³•

```bash
# è¿è¡Œè„šæœ¬
python demo_minimal_data.py

# äº¤äº’å¼æ¼”ç¤ºï¼ŒæŒ‰ Enter é”®é€æ­¥æŸ¥çœ‹
[Step 1/4] ç”Ÿæˆæ¨¡æ‹Ÿç½‘ç»œæµé‡æ•°æ®
>>> æŒ‰ Enter é”®ç»§ç»­...

[Step 2/4] æŸ¥çœ‹æ•°æ®æ‘˜è¦å’Œæ ‡ç­¾åˆ†å¸ƒ
>>> æŒ‰ Enter é”®ç»§ç»­...

[Step 3/4] æŸ¥çœ‹å­—èŠ‚åˆ°åå…­è¿›åˆ¶çš„è½¬æ¢è¿‡ç¨‹
>>> æŒ‰ Enter é”®ç»§ç»­...

[Step 4/4] ä¿å­˜æ•°æ®ä¸º Parquet æ ¼å¼
>>> æŒ‰ Enter é”®ç»§ç»­...
```

### åç»­æ­¥éª¤å»ºè®®

```
ğŸ¯ åç»­æ­¥éª¤:

   1. æŸ¥çœ‹æ•°æ®æ ‡ç­¾åˆ†å¸ƒ:
      python inspect_labels.py --data ../data/demo/demo_payload_bytes.parquet

   2. è¿è¡Œç«¯åˆ°ç«¯æ¼”ç¤º:
      python demo_pipeline.py

   3. ä½¿ç”¨ç¼–ç å™¨+åˆ†ç±»å¤´è¯„ä¼° (éœ€è¦é¢„è®­ç»ƒæƒé‡):
      python eval_with_encoder_head.py --data ../data/demo/demo_payload_bytes.parquet --sample 50
```

---

## è„šæœ¬ 2: demo_pipeline.py

### åŠŸèƒ½

ç«¯åˆ°ç«¯æ¼”ç¤º PCAP_encoder çš„å®Œæ•´å·¥ä½œæµç¨‹ï¼Œä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹æ¨ç†ï¼Œå±•ç¤ºæ¯ä¸ªæ­¥éª¤çš„è¾“å…¥å’Œè¾“å‡ºã€‚

### è¾“å…¥

#### é€‰é¡¹ A: ä½¿ç”¨ç”Ÿæˆçš„æ¼”ç¤ºæ•°æ®
```bash
python demo_pipeline.py --n-samples 10
```

#### é€‰é¡¹ B: ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®
```bash
python demo_pipeline.py --data /path/to/data.parquet --n-samples 50
```

#### é€‰é¡¹ C: ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
```bash
python demo_pipeline.py --use-pretrained --n-samples 20
```

#### å‘½ä»¤è¡Œå‚æ•°

```python
--use-pretrained      # ä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼ˆéœ€è¦ weights.pthï¼‰
--data PATH           # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆParquet æ ¼å¼ï¼‰
--n-samples N         # æ¼”ç¤ºä½¿ç”¨çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ 10ï¼‰
--model-name NAME     # T5 æ¨¡å‹åç§°ï¼ˆé»˜è®¤ t5-smallï¼‰
```

### å¤„ç†æµç¨‹

```mermaid
graph TB
    A[æ£€æŸ¥ä¾èµ–] --> B{æ•°æ®æ¥æº}
    B -->|æœ‰æ•°æ®æ–‡ä»¶| C[åŠ è½½ Parquet]
    B -->|æ— æ•°æ®æ–‡ä»¶| D[ç”Ÿæˆæ¼”ç¤ºæ•°æ®]
    
    C --> E[æ„å»ºæ–‡æœ¬å­—æ®µ]
    D --> E
    
    E --> F[å­—èŠ‚ â†’ åå…­è¿›åˆ¶]
    F --> G[T5 åˆ†è¯ç¼–ç ]
    G --> H[åŠ è½½ T5 æ¨¡å‹]
    H --> I{æœ‰é¢„è®­ç»ƒæƒé‡?}
    I -->|æ˜¯| J[åŠ è½½æƒé‡]
    I -->|å¦| K[éšæœºåˆå§‹åŒ–]
    
    J --> L[åˆ›å»ºåˆ†ç±»å¤´]
    K --> L
    
    L --> M[ç¼–ç  + åˆ†ç±»]
    M --> N[å±•ç¤ºé¢„æµ‹ç»“æœ]
    
    style A fill:#e1f5ff
    style E fill:#fff4e1
    style M fill:#e8f5e9
    style N fill:#f3e5f5
```

#### æ­¥éª¤ 1: æ£€æŸ¥ä¾èµ–

**å‡½æ•°**: `check_dependencies()`

```python
# æ£€æŸ¥å¿…è¦çš„åº“
required = ['transformers', 'torch', 'pandas']

for lib in required:
    try:
        import lib
        print(f"   âœ… {lib} {lib.__version__}")
    except ImportError:
        print(f"   âŒ {lib} æœªå®‰è£…")
```

**è¾“å‡º**:
```
ğŸ” æ£€æŸ¥ä¾èµ–...
   âœ… transformers 4.39.1
   âœ… torch 2.2.2+cu118
   âœ… pandas 2.2.3
```

#### æ­¥éª¤ 2: åŠ è½½æˆ–ç”Ÿæˆæ•°æ®

**å‡½æ•°**: `load_or_generate_data(data_path, n_samples)`

##### é€‰é¡¹ A: åŠ è½½ç°æœ‰æ•°æ®

```python
if data_path and data_path.exists():
    df = pd.read_parquet(data_path)
    if len(df) > n_samples:
        df = df.sample(n=n_samples, random_state=42)
```

**è¾“å…¥**: Parquet æ–‡ä»¶
```
   payload_byte_1  payload_byte_2  ...  attack_label
0              45              67  ...        BENIGN
1              70              84  ...  FTP-Patator
...
```

##### é€‰é¡¹ B: ç”Ÿæˆæ¼”ç¤ºæ•°æ®

```python
else:
    # ç”Ÿæˆç®€å•çš„æ¼”ç¤ºæ•°æ®
    attack_types = ["BENIGN", "FTP-Patator", "SSH-Patator"]
    labels = np.random.choice(attack_types, size=n_samples, p=[0.5, 0.3, 0.2])
    
    # ç”Ÿæˆ 32 å­—èŠ‚çš„è½½è·
    payload_data = np.random.randint(0, 256, size=(n_samples, 32))
    
    columns = {f"payload_byte_{i+1}": payload_data[:, i] for i in range(32)}
    columns["attack_label"] = labels
    
    df = pd.DataFrame(columns)
```

**è¾“å‡º**:
```
ğŸ“‚ ä»æ–‡ä»¶åŠ è½½æ•°æ®: /path/to/data.parquet
   å·²åŠ è½½ 10 æ¡è®°å½•
```
æˆ–
```
ğŸ”§ ç”Ÿæˆæ¼”ç¤ºæ•°æ®...
   å·²ç”Ÿæˆ 10 æ¡æ¼”ç¤ºè®°å½•
```

#### æ­¥éª¤ 3: æ„å»ºæ–‡æœ¬å­—æ®µ

**å‡½æ•°**: `build_text_fields(df, question)`

**å¤„ç†**:
```python
# 1. è·å–æ‰€æœ‰ payload åˆ—
payload_cols = sorted(
    [c for c in df.columns if c.startswith("payload_byte_")],
    key=lambda x: int(x.split("_")[-1])
)

# 2. è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
byte_array = df[payload_cols].to_numpy(dtype=np.uint16)
contexts = []
for row in byte_array:
    hex_str = "".join(f"{int(b):02x}" for b in row)
    # æ¯4å­—ç¬¦åˆ†ç»„ï¼ˆPCAP_encoder æ ¼å¼ï¼‰
    hex_grouped = " ".join(hex_str[i:i+4] for i in range(0, len(hex_str), 4))
    contexts.append(hex_grouped)

# 3. æ„é€ é—®é¢˜
questions = [question] * len(contexts)

# 4. æ ‡ç­¾ç¼–ç 
labels, uniques = pd.factorize(df["attack_label"], sort=True)
```

**è¾“å…¥**: DataFrame
```
   payload_byte_1  payload_byte_2  ...  attack_label
0              69             112  ...        BENIGN
```

**è¾“å‡º**: æ–‡æœ¬å­—æ®µ
```python
questions = ["Classify the network packet", ...]
contexts = ["456c 2dc9 a1b2 c3d4 ...", ...]
labels = [0, 1, 2, 0, 1, ...]  # ç¼–ç åçš„æ ‡ç­¾
label_names = ["BENIGN", "FTP-Patator", "SSH-Patator"]
```

**å±•ç¤º**:
```
ğŸ”„ å°†è½½è·å­—èŠ‚è½¬æ¢ä¸ºåå…­è¿›åˆ¶ä¸Šä¸‹æ–‡...
   é—®é¢˜æ¨¡æ¿: "Classify the network packet"
   ä¸Šä¸‹æ–‡é•¿åº¦: 95 å­—ç¬¦
   æ ‡ç­¾æ˜ å°„: {0: 'BENIGN', 1: 'FTP-Patator', 2: 'SSH-Patator'}
```

#### æ­¥éª¤ 4: å±•ç¤ºæ ·æœ¬è½¬æ¢

**å‡½æ•°**: `show_sample_conversion(df, questions, contexts, n=2)`

**è¾“å‡º**:
```
ğŸ“‹ æ ·æœ¬è½¬æ¢è¯¦æƒ…:

   â”Œâ”€ æ ·æœ¬ 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚ æ ‡ç­¾: BENIGN
   â”‚ åŸå§‹å­—èŠ‚ (å‰8): [69, 112, 45, 201, 161, 178, 195, 212]
   â”‚ é—®é¢˜: Classify the network packet...
   â”‚ ä¸Šä¸‹æ–‡: 456c 2dc9 a1b2 c3d4 e5f6 0718 293a 4b5c...
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   â”Œâ”€ æ ·æœ¬ 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   â”‚ æ ‡ç­¾: FTP-Patator
   â”‚ åŸå§‹å­—èŠ‚ (å‰8): [70, 84, 80, 32, 117, 115, 101, 114]
   â”‚ é—®é¢˜: Classify the network packet...
   â”‚ ä¸Šä¸‹æ–‡: 4654 5020 7573 6572 6e61 6d65 3a20 6164...
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
```

#### æ­¥éª¤ 5: åˆ†è¯ç¼–ç 

**å‡½æ•°**: `tokenize_inputs(questions, contexts, tokenizer, max_length=128)`

**å¤„ç†**:
```python
# T5 çš„è¾“å…¥æ ¼å¼ï¼šquestion + context
inputs = [f"question: {q} context: {c}" for q, c in zip(questions, contexts)]

# åˆ†è¯
encoded = tokenizer(
    inputs,
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)
```

**è¾“å…¥**: æ–‡æœ¬åˆ—è¡¨
```python
[
    "question: Classify the network packet context: 456c 2dc9 a1b2 ...",
    "question: Classify the network packet context: 4654 5020 7573 ...",
    ...
]
```

**è¾“å‡º**: Token IDs
```python
{
    'input_ids': Tensor([[1, 822, 19, 8, ...], [1, 822, 19, 8, ...], ...]),
    'attention_mask': Tensor([[1, 1, 1, 1, ...], [1, 1, 1, 1, ...], ...])
}
```

**å±•ç¤º**:
```
ğŸ”¤ åˆ†è¯ç¼–ç ...
   è¾“å…¥åºåˆ—æ•°: 10
   Token åºåˆ—å½¢çŠ¶: torch.Size([10, 128])
   æœ€å¤§åºåˆ—é•¿åº¦: 128

   ç¬¬ä¸€ä¸ªæ ·æœ¬çš„åˆ†è¯ç»“æœ:
   Tokens (å‰20): ['â–question', ':', 'â–Classify', 'â–the', 'â–network', 'â–packet', 'â–context', ':', 'â–', '4', '5', '6', 'c', 'â–', '2', 'd', 'c', '9', 'â–', 'a']
```

#### æ­¥éª¤ 6: åŠ è½½æ¨¡å‹

**å‡½æ•°**: `load_model(model_name, weights_path, device)`

**å¤„ç†**:
```python
from transformers import T5ForConditionalGeneration

# åŠ è½½ T5 æ¨¡å‹
model = T5ForConditionalGeneration.from_pretrained(model_name)

# å¦‚æœæœ‰é¢„è®­ç»ƒæƒé‡
if weights_path and weights_path.exists():
    state = torch.load(weights_path, map_location="cpu", weights_only=True)
    model.load_state_dict(state, strict=False)

# æå–ç¼–ç å™¨
encoder = model.encoder.to(device)

# å†»ç»“å‚æ•°
for p in encoder.parameters():
    p.requires_grad = False
```

**è¾“å‡º**:
```
ğŸ¤– åŠ è½½ T5 æ¨¡å‹: t5-small
   åŠ è½½é¢„è®­ç»ƒæƒé‡: ../models/weights.pth
   âœ… æƒé‡åŠ è½½æˆåŠŸ
   ç¼–ç å™¨éšè—ç»´åº¦: 512
   ç¼–ç å™¨å±‚æ•°: 6
   è®¾å¤‡: cuda
```
æˆ–
```
ğŸ¤– åŠ è½½ T5 æ¨¡å‹: t5-small
   âš ï¸  ä½¿ç”¨éšæœºåˆå§‹åŒ–æƒé‡ï¼ˆä»…ç”¨äºæ¼”ç¤ºæµç¨‹ï¼‰
   ç¼–ç å™¨éšè—ç»´åº¦: 512
   ç¼–ç å™¨å±‚æ•°: 6
   è®¾å¤‡: cpu
```

#### æ­¥éª¤ 7: åˆ›å»ºåˆ†ç±»å¤´

```python
num_classes = len(label_names)  # 3
hidden_size = model.config.d_model  # 512 (t5-small) æˆ– 768 (t5-base)

head = nn.Linear(hidden_size, num_classes).to(device)
```

**è¾“å‡º**:
```
   åˆ†ç±»å¤´: Linear(512 -> 3)
```

#### æ­¥éª¤ 8: ç¼–ç å’Œåˆ†ç±»

**å‡½æ•°**: `encode_and_classify(encoder, head, encodings, labels, device)`

**å¤„ç†**:
```python
with torch.no_grad():
    # 1. ç¼–ç 
    outputs = encoder(
        input_ids=input_ids, 
        attention_mask=attention_mask, 
        return_dict=True
    )
    hidden = outputs.last_hidden_state[:, 0, :]  # å–ç¬¬ä¸€ä¸ª token çš„è¡¨ç¤º
    
    # 2. åˆ†ç±»
    logits = head(hidden)
    
    # 3. é¢„æµ‹
    probs = torch.softmax(logits, dim=1).cpu().numpy()
    preds = probs.argmax(axis=1)
```

**æ•°æ®æµ**:
```
Token IDs [10, 128]
    â†“
T5 Encoder
    â†“
Hidden States [10, 128, 512]
    â†“
å–ç¬¬ä¸€ä¸ª token [:, 0, :]
    â†“
Representation [10, 512]
    â†“
Linear Classifier
    â†“
Logits [10, 3]
    â†“
Softmax
    â†“
Probabilities [10, 3]
    â†“
Argmax
    â†“
Predictions [10]
```

**è¾“å‡º**:
```
ğŸ§  ç¼–ç  + åˆ†ç±»æ¨ç†...
   1. é€šè¿‡ T5 ç¼–ç å™¨...
      éšè—è¡¨ç¤ºå½¢çŠ¶: torch.Size([10, 512])
   2. é€šè¿‡çº¿æ€§åˆ†ç±»å¤´...
      Logits å½¢çŠ¶: torch.Size([10, 3])
```

#### æ­¥éª¤ 9: å±•ç¤ºé¢„æµ‹ç»“æœ

**å‡½æ•°**: `show_predictions(df, labels, preds, probs, label_names)`

**è¾“å‡º**:
```
ğŸ“Š é¢„æµ‹ç»“æœ:

   æ ·æœ¬   â”‚    çœŸå®æ ‡ç­¾     â”‚    é¢„æµ‹æ ‡ç­¾     â”‚   ç½®ä¿¡åº¦   â”‚ æ­£ç¡® 
   â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€
     1    â”‚     BENIGN      â”‚     BENIGN      â”‚   85.3%    â”‚  âœ…  
     2    â”‚  FTP-Patator    â”‚  FTP-Patator    â”‚   72.1%    â”‚  âœ…  
     3    â”‚  SSH-Patator    â”‚  FTP-Patator    â”‚   45.6%    â”‚  âŒ  
     4    â”‚     BENIGN      â”‚     BENIGN      â”‚   91.2%    â”‚  âœ…  
     5    â”‚  FTP-Patator    â”‚  FTP-Patator    â”‚   68.9%    â”‚  âœ…  
     6    â”‚     BENIGN      â”‚  SSH-Patator    â”‚   52.3%    â”‚  âŒ  
     7    â”‚  SSH-Patator    â”‚  SSH-Patator    â”‚   78.4%    â”‚  âœ…  
     8    â”‚     BENIGN      â”‚     BENIGN      â”‚   88.7%    â”‚  âœ…  
     9    â”‚  FTP-Patator    â”‚     BENIGN      â”‚   55.1%    â”‚  âŒ  
    10    â”‚     BENIGN      â”‚     BENIGN      â”‚   93.5%    â”‚  âœ…  
   â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€

   å‡†ç¡®ç‡: 70.0% (7/10)

   ğŸ“ æ³¨æ„:
   - å¦‚æœæœªä½¿ç”¨é¢„è®­ç»ƒæƒé‡ï¼Œå‡†ç¡®ç‡æ¥è¿‘éšæœºçŒœæµ‹æ˜¯æ­£å¸¸çš„
   - è¿™åªæ˜¯æ¼”ç¤ºæµç¨‹ï¼Œä¸ä»£è¡¨çœŸå®æ¨¡å‹æ€§èƒ½
```

### è¾“å‡º

#### ç»ˆç«¯è¾“å‡º
- æ¯ä¸ªæ­¥éª¤çš„è¯¦ç»†ä¿¡æ¯
- æ ·æœ¬è½¬æ¢ç¤ºä¾‹
- é¢„æµ‹ç»“æœè¡¨æ ¼

#### å­¦ä¹ è¦ç‚¹
- æ•°æ®æ ¼å¼è½¬æ¢è¿‡ç¨‹
- T5 æ¨¡å‹çš„è¾“å…¥æ ¼å¼
- ç¼–ç å™¨çš„å·¥ä½œæ–¹å¼
- åˆ†ç±»å¤´çš„ä½œç”¨

### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºç¡€æ¼”ç¤ºï¼ˆç”Ÿæˆæ•°æ® + éšæœºæƒé‡ï¼‰
python demo_pipeline.py --n-samples 10

# ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
python demo_pipeline.py --use-pretrained --n-samples 20

# ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®
python demo_pipeline.py --data ../data/demo/demo_payload_bytes.parquet --n-samples 50

# ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹
python demo_pipeline.py --model-name t5-base --n-samples 10
```

---

## è„šæœ¬ 3: eval_with_encoder_head.py

### åŠŸèƒ½

ä½¿ç”¨é¢„è®­ç»ƒçš„ T5 ç¼–ç å™¨å’Œçº¿æ€§åˆ†ç±»å¤´è¯„ä¼°æ¨¡å‹åœ¨ CIC-IDS2017 æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹ã€‚

### è¾“å…¥

#### æ•°æ®æ–‡ä»¶
- **æ ¼å¼**: Parquet æ–‡ä»¶
- **å¿…éœ€åˆ—**: `payload_byte_1`, `payload_byte_2`, ..., `attack_label`

#### é¢„è®­ç»ƒæƒé‡
- **è·¯å¾„**: `../models/weights.pth`
- **æ¥æº**: QA æˆ– Denoiser è®­ç»ƒçš„æœ€ä½³æ¨¡å‹

#### å‘½ä»¤è¡Œå‚æ•°

```python
--data PATH           # æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆParquetï¼‰
--weights PATH        # é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆé»˜è®¤ ../models/weights.pthï¼‰
--model-name NAME     # T5 æ¨¡å‹åç§°ï¼ˆé»˜è®¤ t5-baseï¼‰
--max-bytes N         # ä½¿ç”¨çš„æœ€å¤§å­—èŠ‚æ•°ï¼ˆé»˜è®¤ 1500ï¼‰
--sample N            # ä½¿ç”¨çš„æ ·æœ¬æ•°é‡ï¼ˆ0 = å…¨éƒ¨ï¼‰
--batch-size N        # æ‰¹å¤§å°ï¼ˆé»˜è®¤ 16ï¼‰
--epochs N            # è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ 10ï¼‰
--lr FLOAT            # å­¦ä¹ ç‡ï¼ˆé»˜è®¤ 1e-3ï¼‰
--seed N              # éšæœºç§å­ï¼ˆé»˜è®¤ 42ï¼‰
```

### å¤„ç†æµç¨‹

```mermaid
graph TB
    A[åŠ è½½ Parquet æ•°æ®] --> B[é‡‡æ ·æ•°æ®]
    B --> C[å­—èŠ‚ â†’ åå…­è¿›åˆ¶]
    C --> D[æ„å»ºæ–‡æœ¬å­—æ®µ]
    D --> E[T5 åˆ†è¯]
    E --> F[æ•°æ®é›†åˆ’åˆ†]
    F --> G[Train 60%]
    F --> H[Val 20%]
    F --> I[Test 20%]
    
    G --> J[åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨]
    J --> K[åˆ›å»ºåˆ†ç±»å¤´]
    K --> L[è®­ç»ƒåˆ†ç±»å¤´]
    
    L --> M{éªŒè¯}
    M -->|ç»§ç»­| L
    M -->|æœ€ä½³| N[ä¿å­˜æ¨¡å‹]
    
    H --> M
    I --> O[æµ‹è¯•è¯„ä¼°]
    N --> O
    
    O --> P[è®¡ç®—æŒ‡æ ‡]
    P --> Q[F1, AUC, Accuracy]
    
    style A fill:#e1f5ff
    style D fill:#fff4e1
    style L fill:#e8f5e9
    style Q fill:#f3e5f5
```

#### æ­¥éª¤ 1: åŠ è½½æ•°æ®

**å‡½æ•°**: `load_payload_df(path, max_bytes, sample, seed)`

```python
# è¯»å– Parquet æ–‡ä»¶
df = pd.read_parquet(path)

# é‡‡æ ·ï¼ˆå¦‚æœæŒ‡å®šï¼‰
if sample > 0:
    df = df.sample(n=min(sample, len(df)), random_state=seed)

# è·å– payload åˆ—
payload_cols = [f'payload_byte_{i}' for i in range(1, max_bytes + 1) 
                if f'payload_byte_{i}' in df.columns]
```

**è¾“å…¥**: Parquet æ–‡ä»¶
```
   payload_byte_1  payload_byte_2  ...  payload_byte_1500  attack_label
0              69             112  ...                 23        BENIGN
1              70              84  ...                 89  FTP-Patator
...
```

**è¾“å‡º**: é‡‡æ ·åçš„ DataFrame
```
ğŸ“‚ åŠ è½½æ•°æ®: /path/to/data.parquet
   æ€»æ ·æœ¬æ•°: 10000
   é‡‡æ ·å: 1000
   Payload åˆ—æ•°: 1500
   æ ‡ç­¾ç±»åˆ«: ['BENIGN', 'FTP-Patator', 'SSH-Patator', ...]
```

#### æ­¥éª¤ 2: æ„å»ºæ–‡æœ¬å­—æ®µ

**å‡½æ•°**: `build_text_fields(df, payload_cols, question)`

**å¤„ç†**:
```python
# 1. æå–å­—èŠ‚æ•°ç»„
byte_array = df[payload_cols].to_numpy(dtype=np.uint8)

# 2. è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
contexts = []
for row in byte_array:
    hex_str = ''.join(f'{b:02x}' for b in row)
    # æ¯4å­—ç¬¦åˆ†ç»„
    hex_grouped = ' '.join(hex_str[i:i+4] for i in range(0, len(hex_str), 4))
    contexts.append(hex_grouped)

# 3. æ„é€ é—®é¢˜
questions = [question] * len(contexts)

# 4. æ ‡ç­¾ç¼–ç 
labels, uniques = pd.factorize(df['attack_label'], sort=True)
```

**è¾“å…¥**: DataFrame + Payload åˆ—
```
   payload_byte_1  payload_byte_2  ...  attack_label
0              69             112  ...        BENIGN
```

**è¾“å‡º**: æ–‡æœ¬å­—æ®µ
```python
questions = ["Classify the network packet", ...]
contexts = ["456c 2dc9 a1b2 c3d4 e5f6 ...", ...]  # é•¿åº¦ ~3000 å­—ç¬¦
labels = [0, 1, 2, 0, ...]
label_names = ["BENIGN", "FTP-Patator", "SSH-Patator", ...]
```

#### æ­¥éª¤ 3: åˆ†è¯ç¼–ç 

**å‡½æ•°**: `tokenize(tokenizer, questions, contexts, max_length)`

```python
inputs = [f"question: {q} context: {c}" for q, c in zip(questions, contexts)]

encodings = tokenizer(
    inputs,
    padding=True,
    truncation=True,
    max_length=max_length,
    return_tensors="pt"
)
```

**è¾“å‡º**:
```python
{
    'input_ids': Tensor([1000, 512]),      # 1000 æ ·æœ¬ï¼Œæ¯ä¸ªæœ€å¤š 512 tokens
    'attention_mask': Tensor([1000, 512])
}
```

#### æ­¥éª¤ 4: æ•°æ®é›†åˆ’åˆ†

**å‡½æ•°**: `make_loaders(encodings, labels, batch_size, seed, train_frac=0.6, val_frac=0.2)`

```python
# åˆ†å±‚æŠ½æ ·ï¼Œä¿æŒç±»åˆ«æ¯”ä¾‹
train_idx, temp_idx = train_test_split(
    indices, 
    train_size=train_frac, 
    stratify=labels, 
    random_state=seed
)

val_idx, test_idx = train_test_split(
    temp_idx, 
    train_size=val_frac/(1-train_frac), 
    stratify=labels[temp_idx], 
    random_state=seed
)
```

**è¾“å‡º**:
```
ğŸ“Š æ•°æ®é›†åˆ’åˆ†:
   è®­ç»ƒé›†: 600 æ ·æœ¬ (60.0%)
   éªŒè¯é›†: 200 æ ·æœ¬ (20.0%)
   æµ‹è¯•é›†: 200 æ ·æœ¬ (20.0%)
   
   ç±»åˆ«åˆ†å¸ƒ (è®­ç»ƒé›†):
   - BENIGN: 360 (60%)
   - FTP-Patator: 150 (25%)
   - SSH-Patator: 90 (15%)
```

#### æ­¥éª¤ 5: åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨

**å‡½æ•°**: `load_encoder(model_name, weights_path, device)`

```python
# åŠ è½½ T5 æ¨¡å‹
model = T5ForConditionalGeneration.from_pretrained(model_name)

# åŠ è½½é¢„è®­ç»ƒæƒé‡
if weights_path.exists():
    state = torch.load(weights_path, map_location="cpu", weights_only=True)
    model.load_state_dict(state, strict=False)

# æå–ç¼–ç å™¨å¹¶å†»ç»“
encoder = model.encoder.to(device)
for p in encoder.parameters():
    p.requires_grad = False
```

**è¾“å‡º**:
```
ğŸ¤– åŠ è½½æ¨¡å‹:
   æ¨¡å‹: t5-base
   é¢„è®­ç»ƒæƒé‡: ../models/weights.pth
   âœ… æƒé‡åŠ è½½æˆåŠŸ
   ç¼–ç å™¨éšè—ç»´åº¦: 768
   å‚æ•°å·²å†»ç»“
```

#### æ­¥éª¤ 6: åˆ›å»ºåˆ†ç±»å¤´

```python
num_classes = len(np.unique(labels))
hidden_size = encoder.config.d_model

head = nn.Linear(hidden_size, num_classes).to(device)
optimizer = torch.optim.Adam(head.parameters(), lr=lr)
```

**è¾“å‡º**:
```
ğŸ¯ åˆ†ç±»å¤´:
   è¾“å…¥ç»´åº¦: 768
   è¾“å‡ºç±»åˆ«æ•°: 3
   å¯è®­ç»ƒå‚æ•°: 2,307
```

#### æ­¥éª¤ 7: è®­ç»ƒåˆ†ç±»å¤´

**å‡½æ•°**: `run_epoch(loader, encoder, head, device, train=True, optimizer=None)`

```python
for epoch in range(epochs):
    # è®­ç»ƒ
    train_loss, train_acc = run_epoch(
        train_loader, encoder, head, device, 
        train=True, optimizer=optimizer
    )
    
    # éªŒè¯
    val_loss, val_acc = run_epoch(
        val_loader, encoder, head, device, 
        train=False
    )
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        best_epoch = epoch
        torch.save(head.state_dict(), 'best_head.pth')
```

**è®­ç»ƒå¾ªç¯ç»†èŠ‚**:
```python
def run_epoch(loader, encoder, head, device, train, optimizer=None):
    if train:
        head.train()
    else:
        head.eval()
    
    total_loss = 0
    correct = 0
    total = 0
    
    for batch in loader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        # ç¼–ç ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
        with torch.no_grad():
            outputs = encoder(input_ids, attention_mask, return_dict=True)
            hidden = outputs.last_hidden_state[:, 0, :]  # [batch, 768]
        
        # åˆ†ç±»
        logits = head(hidden)  # [batch, num_classes]
        loss = F.cross_entropy(logits, labels)
        
        # åå‘ä¼ æ’­ï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
        if train:
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
        
        # ç»Ÿè®¡
        total_loss += loss.item()
        preds = logits.argmax(dim=1)
        correct += (preds == labels).sum().item()
        total += labels.size(0)
    
    avg_loss = total_loss / len(loader)
    accuracy = correct / total
    
    return avg_loss, accuracy
```

**è¾“å‡º**:
```
ğŸ‹ï¸ è®­ç»ƒåˆ†ç±»å¤´:

Epoch  1/10: Train Loss=0.8234, Acc=0.6500 | Val Loss=0.7123, Acc=0.7100
Epoch  2/10: Train Loss=0.6543, Acc=0.7350 | Val Loss=0.6234, Acc=0.7550
Epoch  3/10: Train Loss=0.5432, Acc=0.7850 | Val Loss=0.5678, Acc=0.7900
...
Epoch 10/10: Train Loss=0.2345, Acc=0.9150 | Val Loss=0.3456, Acc=0.8750

âœ… æœ€ä½³æ¨¡å‹: Epoch 8, Val Loss=0.3123, Val Acc=0.8850
```

#### æ­¥éª¤ 8: æµ‹è¯•è¯„ä¼°

```python
# åŠ è½½æœ€ä½³æ¨¡å‹
head.load_state_dict(torch.load('best_head.pth'))

# æµ‹è¯•
test_loss, test_acc = run_epoch(
    test_loader, encoder, head, device, 
    train=False
)

# è¯¦ç»†è¯„ä¼°
all_preds = []
all_labels = []
all_probs = []

for batch in test_loader:
    with torch.no_grad():
        outputs = encoder(batch['input_ids'], batch['attention_mask'])
        hidden = outputs.last_hidden_state[:, 0, :]
        logits = head(hidden)
        probs = F.softmax(logits, dim=1)
        
        all_preds.append(logits.argmax(dim=1))
        all_labels.append(batch['labels'])
        all_probs.append(probs)

all_preds = torch.cat(all_preds).cpu().numpy()
all_labels = torch.cat(all_labels).cpu().numpy()
all_probs = torch.cat(all_probs).cpu().numpy()
```

#### æ­¥éª¤ 9: è®¡ç®—æŒ‡æ ‡

```python
from sklearn.metrics import f1_score, roc_auc_score, classification_report, confusion_matrix

# F1 Score
f1_macro = f1_score(all_labels, all_preds, average='macro')
f1_weighted = f1_score(all_labels, all_preds, average='weighted')

# AUC (One-vs-Rest)
auc_macro = roc_auc_score(all_labels, all_probs, multi_class='ovr', average='macro')

# å‡†ç¡®ç‡
accuracy = (all_preds == all_labels).mean()

# æ··æ·†çŸ©é˜µ
cm = confusion_matrix(all_labels, all_preds)

# åˆ†ç±»æŠ¥å‘Š
report = classification_report(all_labels, all_preds, target_names=label_names)
```

**è¾“å‡º**:
```
ğŸ“Š æµ‹è¯•é›†è¯„ä¼°ç»“æœ:

æ€»ä½“æŒ‡æ ‡:
   å‡†ç¡®ç‡ (Accuracy):     88.50%
   F1 Score (Macro):      87.23%
   F1 Score (Weighted):   88.12%
   AUC (Macro):           92.45%

æ··æ·†çŸ©é˜µ:
                 BENIGN  FTP-Patator  SSH-Patator
   BENIGN           108            4            8
   FTP-Patator        5           42            3
   SSH-Patator        2            1           27

åˆ†ç±»æŠ¥å‘Š:
                precision    recall  f1-score   support

        BENIGN       0.94      0.90      0.92       120
   FTP-Patator       0.89      0.84      0.87        50
   SSH-Patator       0.71      0.90      0.79        30

      accuracy                           0.89       200
     macro avg       0.85      0.88      0.86       200
  weighted avg       0.89      0.89      0.89       200
```

### è¾“å‡º

#### æ¨¡å‹æ–‡ä»¶
- **best_head.pth**: æœ€ä½³åˆ†ç±»å¤´æƒé‡

#### è¯„ä¼°æŠ¥å‘Š
- å‡†ç¡®ç‡ã€F1ã€AUC
- æ··æ·†çŸ©é˜µ
- åˆ†ç±»æŠ¥å‘Š

#### æ—¥å¿—æ–‡ä»¶
- è®­ç»ƒè¿‡ç¨‹æ—¥å¿—
- æ¯ä¸ª epoch çš„æŒ‡æ ‡

### ä½¿ç”¨æ–¹æ³•

```bash
# åŸºç¡€è¯„ä¼°
python eval_with_encoder_head.py \
    --data ../data/demo/demo_payload_bytes.parquet \
    --sample 100

# ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
python eval_with_encoder_head.py \
    --data /path/to/CIC-IDS2017.parquet \
    --weights ../models/weights.pth \
    --sample 1000 \
    --epochs 20

# å®Œæ•´è¯„ä¼°ï¼ˆå…¨éƒ¨æ•°æ®ï¼‰
python eval_with_encoder_head.py \
    --data /path/to/CIC-IDS2017.parquet \
    --weights ../models/weights.pth \
    --sample 0 \
    --batch-size 32 \
    --epochs 50
```

---

## è„šæœ¬ 4: evaluate_cic_ids.py

### åŠŸèƒ½

CIC-IDS2017 æ•°æ®é›†è¯„ä¼°çš„åˆå§‹æ¨¡æ¿ï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ HuggingFace æ ¼å¼çš„é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œåˆ†ç±»è¯„ä¼°ã€‚

### è¾“å…¥

#### æ•°æ®æ–‡ä»¶
- **æ ¼å¼**: Parquet æ–‡ä»¶ï¼ˆCIC-IDS2017 æ ¼å¼ï¼‰
- **æ¥æº**: `nids-datasets` åº“æˆ–æ‰‹åŠ¨ä¸‹è½½

#### é¢„è®­ç»ƒæ¨¡å‹
- **æ ¼å¼**: HuggingFace æ¨¡å‹ç›®å½•
- **åŒ…å«**: `config.json`, `pytorch_model.bin`, `tokenizer_config.json`

### å¤„ç†æµç¨‹

```mermaid
graph LR
    A[Parquet æ–‡ä»¶] --> B[load_data]
    B --> C[prepare_data]
    C --> D[å­—èŠ‚ â†’ åå…­è¿›åˆ¶]
    D --> E[æ ‡ç­¾ç¼–ç ]
    E --> F[load_model]
    F --> G[HF Model + Tokenizer]
    G --> H[evaluate_model]
    H --> I[åˆ†è¯ + æ¨ç†]
    I --> J[è®¡ç®— F1 & AUC]
    
    style A fill:#e1f5ff
    style G fill:#e8f5e9
    style J fill:#f3e5f5
```

#### ä¸»è¦å‡½æ•°

##### 1. load_data(parquet_file)

```python
def load_data(parquet_file):
    """ä»æŒ‡å®šçš„ Parquet æ–‡ä»¶ä¸­è¯»å–æ•°æ®"""
    df = pd.read_parquet(parquet_file)
    return df
```

##### 2. prepare_data(df)

```python
def prepare_data(df):
    """å°†åŸå§‹å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„æ–‡æœ¬æ ¼å¼"""
    # è·å– payload åˆ—
    payload_cols = [f'payload_byte_{i}' for i in range(1, 1501) 
                    if f'payload_byte_{i}' in df.columns]
    
    # è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²
    context = df[payload_cols].applymap(lambda x: format(int(x), '02x')).agg(''.join, axis=1)
    
    # ç”Ÿæˆé—®é¢˜
    question = ['Classify the network packet'] * len(df)
    
    # æ ‡ç­¾æ˜ å°„
    label_mapping = {label: idx for idx, label in enumerate(df['attack_label'].unique())}
    df['class'] = df['attack_label'].map(label_mapping)
    
    return context, question, df['class'], df['attack_label']
```

##### 3. load_model(model_path)

```python
def load_model(model_path):
    """åŠ è½½ HuggingFace æ ¼å¼çš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨"""
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    return model, tokenizer
```

##### 4. evaluate_model(model, tokenizer, context, question, labels)

```python
def evaluate_model(model, tokenizer, context, question, labels):
    """åœ¨ç»™å®šæ•°æ®ä¸Šè¿è¡Œæ¨¡å‹å¹¶è®¡ç®— F1 å’Œ AUC æŒ‡æ ‡"""
    # åˆ†è¯
    inputs = tokenizer(
        list(zip(question, context)), 
        padding=True, 
        truncation=True, 
        return_tensors="pt"
    )
    
    # æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
    
    # é¢„æµ‹
    predictions = outputs.logits.argmax(dim=-1).detach().cpu().numpy()
    
    # è®¡ç®—æŒ‡æ ‡
    f1 = f1_score(labels, predictions, average='macro')
    
    try:
        auc = roc_auc_score(
            labels, 
            outputs.logits.detach().cpu().numpy(), 
            multi_class='ovr', 
            average='macro'
        )
    except ValueError:
        auc = float('nan')
    
    return f1, auc
```

### è¾“å‡º

```
F1 Macro: 0.8723
AUC: 0.9245
```

### ä½¿ç”¨æ–¹æ³•

```python
# ä¿®æ”¹æ•°æ®è·¯å¾„
parquet_file = 'CIC-IDS2017/Network_Flows+Packet_Fields+Payload_Bytes_File_1.parquet'

# ä¿®æ”¹æ¨¡å‹è·¯å¾„
model_path = 'models/weights.pth'

# è¿è¡Œ
python evaluate_cic_ids.py
```

### æ³¨æ„äº‹é¡¹

- æ­¤è„šæœ¬æ˜¯æ¨¡æ¿ï¼Œéœ€è¦æ ¹æ®å®é™…ç¯å¢ƒè°ƒæ•´è·¯å¾„
- éœ€è¦å®‰è£… `nids-datasets` åº“ï¼ˆå¯é€‰ï¼‰
- æ¨¡å‹éœ€è¦æ˜¯ HuggingFace æ ¼å¼ï¼ˆä¸ `eval_with_encoder_head.py` ä¸åŒï¼‰

---

## è„šæœ¬ 5: data_downloads.py

### åŠŸèƒ½

ä¸‹è½½ CIC-IDS2017 æ•°æ®é›†çš„è¾…åŠ©è„šæœ¬ã€‚

### è¾“å…¥

- **æ•°æ®é›†åç§°**: CIC-IDS2017
- **ä¸‹è½½æº**: å®˜æ–¹ç½‘ç«™æˆ–é•œåƒ

### å¤„ç†æµç¨‹

```python
# ä½¿ç”¨ nids-datasets åº“
from nids_datasets import Dataset

# ä¸‹è½½æ•°æ®é›†
dataset = Dataset.load('CIC-IDS2017')

# ä¿å­˜ä¸º Parquet
dataset.to_parquet('CIC-IDS2017.parquet')
```

### è¾“å‡º

- **æ–‡ä»¶**: `CIC-IDS2017.parquet`
- **å¤§å°**: å‡  GB
- **æ ¼å¼**: Parquet

### ä½¿ç”¨æ–¹æ³•

```bash
python data_downloads.py
```

---

## è„šæœ¬ 6: inspect_labels.py

### åŠŸèƒ½

æ£€æŸ¥æ•°æ®é›†çš„æ ‡ç­¾åˆ†å¸ƒå’Œç»Ÿè®¡ä¿¡æ¯ã€‚

### è¾“å…¥

```bash
python inspect_labels.py --data /path/to/data.parquet
```

### å¤„ç†æµç¨‹

```python
# è¯»å–æ•°æ®
df = pd.read_parquet(data_path)

# ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
label_counts = df['attack_label'].value_counts()

# è®¡ç®—ç™¾åˆ†æ¯”
label_pcts = df['attack_label'].value_counts(normalize=True) * 100

# å±•ç¤º
for label, count in label_counts.items():
    pct = label_pcts[label]
    print(f"{label:20s}: {count:6d} ({pct:5.2f}%)")
```

### è¾“å‡º

```
ğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:

   æ€»æ ·æœ¬æ•°: 10000

   æ ‡ç­¾ç»Ÿè®¡:
   BENIGN              :   6000 (60.00%)
   FTP-Patator         :   2500 (25.00%)
   SSH-Patator         :   1500 (15.00%)

   ç±»åˆ«æ•°: 3
   æ˜¯å¦å¹³è¡¡: å¦ï¼ˆæœ€å¤§/æœ€å°æ¯”ä¾‹ = 4.00ï¼‰
```

### ä½¿ç”¨æ–¹æ³•

```bash
# æ£€æŸ¥æ¼”ç¤ºæ•°æ®
python inspect_labels.py --data ../data/demo/demo_payload_bytes.parquet

# æ£€æŸ¥ CIC-IDS2017 æ•°æ®
python inspect_labels.py --data /path/to/CIC-IDS2017.parquet
```

---

## å®Œæ•´å·¥ä½œæµç¨‹

### æ¨èä½¿ç”¨æµç¨‹

```mermaid
graph TB
    A[å¼€å§‹] --> B{æœ‰ CIC-IDS2017 æ•°æ®?}
    B -->|å¦| C[è¿è¡Œ demo_minimal_data.py]
    B -->|æ˜¯| D[è¿è¡Œ data_downloads.py]
    
    C --> E[è¿è¡Œ inspect_labels.py]
    D --> E
    
    E --> F[è¿è¡Œ demo_pipeline.py]
    F --> G{æœ‰é¢„è®­ç»ƒæƒé‡?}
    
    G -->|å¦| H[å…ˆè®­ç»ƒ PCAP_encoder]
    G -->|æ˜¯| I[è¿è¡Œ eval_with_encoder_head.py]
    
    H --> I
    I --> J[åˆ†æç»“æœ]
    J --> K[ç»“æŸ]
    
    style C fill:#e1f5ff
    style F fill:#fff4e1
    style I fill:#e8f5e9
    style J fill:#f3e5f5
```

### æ­¥éª¤è¯¦è§£

#### 1. å‡†å¤‡æ•°æ®

```bash
# é€‰é¡¹ A: ç”Ÿæˆæ¼”ç¤ºæ•°æ®ï¼ˆå¿«é€Ÿï¼‰
python demo_minimal_data.py

# é€‰é¡¹ B: ä¸‹è½½çœŸå®æ•°æ®ï¼ˆå®Œæ•´ï¼‰
python data_downloads.py
```

#### 2. æ£€æŸ¥æ•°æ®

```bash
python inspect_labels.py --data ../data/demo/demo_payload_bytes.parquet
```

#### 3. è¿è¡Œæ¼”ç¤º

```bash
# ç†è§£æµç¨‹
python demo_pipeline.py --n-samples 10

# ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
python demo_pipeline.py --use-pretrained --n-samples 20
```

#### 4. å®Œæ•´è¯„ä¼°

```bash
# è®­ç»ƒå’Œè¯„ä¼°
python eval_with_encoder_head.py \
    --data ../data/demo/demo_payload_bytes.parquet \
    --weights ../models/weights.pth \
    --sample 100 \
    --epochs 20
```

#### 5. åˆ†æç»“æœ

- æŸ¥çœ‹å‡†ç¡®ç‡ã€F1ã€AUC
- åˆ†ææ··æ·†çŸ©é˜µ
- è¯†åˆ«é”™è¯¯åˆ†ç±»çš„æ ·æœ¬

---

## æ•°æ®æµè½¬æ€»è§ˆ

### å®Œæ•´æ•°æ®æµ

```mermaid
graph TB
    subgraph "æ•°æ®å‡†å¤‡"
        A1[demo_minimal_data.py] --> A2[ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®]
        A3[data_downloads.py] --> A4[ä¸‹è½½ CIC-IDS2017]
        A2 --> A5[Parquet æ–‡ä»¶]
        A4 --> A5
    end
    
    subgraph "æ•°æ®æ£€æŸ¥"
        A5 --> B1[inspect_labels.py]
        B1 --> B2[æ ‡ç­¾åˆ†å¸ƒæŠ¥å‘Š]
    end
    
    subgraph "æµç¨‹æ¼”ç¤º"
        A5 --> C1[demo_pipeline.py]
        C1 --> C2[å­—èŠ‚ â†’ åå…­è¿›åˆ¶]
        C2 --> C3[T5 åˆ†è¯]
        C3 --> C4[T5 ç¼–ç å™¨]
        C4 --> C5[åˆ†ç±»å¤´]
        C5 --> C6[é¢„æµ‹ç»“æœ]
    end
    
    subgraph "å®Œæ•´è¯„ä¼°"
        A5 --> D1[eval_with_encoder_head.py]
        D1 --> D2[æ•°æ®åˆ’åˆ†]
        D2 --> D3[åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨]
        D3 --> D4[è®­ç»ƒåˆ†ç±»å¤´]
        D4 --> D5[æµ‹è¯•è¯„ä¼°]
        D5 --> D6[æ€§èƒ½æŒ‡æ ‡]
    end
    
    style A5 fill:#e1f5ff
    style C6 fill:#fff4e1
    style D6 fill:#f3e5f5
```

### æ•°æ®æ ¼å¼å˜åŒ–

| é˜¶æ®µ | è¾“å…¥æ ¼å¼ | è¾“å‡ºæ ¼å¼ | ç¤ºä¾‹ |
|------|----------|----------|------|
| **æ•°æ®ç”Ÿæˆ** | é…ç½®å‚æ•° | Parquet | `{payload_byte_1: 69, ..., attack_label: 'BENIGN'}` |
| **å­—èŠ‚æå–** | Parquet | Numpy æ•°ç»„ | `[69, 112, 45, 201, ...]` |
| **åå…­è¿›åˆ¶è½¬æ¢** | Numpy æ•°ç»„ | å­—ç¬¦ä¸² | `"456c 2dc9 a1b2 c3d4 ..."` |
| **æ–‡æœ¬æ„å»º** | åå…­è¿›åˆ¶å­—ç¬¦ä¸² | æ–‡æœ¬ | `"question: Classify... context: 456c..."` |
| **åˆ†è¯** | æ–‡æœ¬ | Token IDs | `[1, 822, 19, 8, ...]` |
| **ç¼–ç ** | Token IDs | Hidden States | `Tensor([batch, seq, 768])` |
| **è¡¨ç¤ºæå–** | Hidden States | å‘é‡ | `Tensor([batch, 768])` |
| **åˆ†ç±»** | å‘é‡ | Logits | `Tensor([batch, num_classes])` |
| **é¢„æµ‹** | Logits | ç±»åˆ« | `1` (FTP-Patator) |

---

## æ€»ç»“

### æ¨¡å—åŠŸèƒ½æ€»ç»“

| è„šæœ¬ | åŠŸèƒ½ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| `demo_minimal_data.py` | ç”Ÿæˆæ¼”ç¤ºæ•°æ® | æ—  | Parquet æ–‡ä»¶ |
| `demo_pipeline.py` | ç«¯åˆ°ç«¯æ¼”ç¤º | Parquetï¼ˆå¯é€‰ï¼‰ | é¢„æµ‹ç»“æœ |
| `eval_with_encoder_head.py` | å®Œæ•´è¯„ä¼° | Parquet + æƒé‡ | æ€§èƒ½æŒ‡æ ‡ |
| `evaluate_cic_ids.py` | CIC-IDS è¯„ä¼°æ¨¡æ¿ | Parquet + HF æ¨¡å‹ | F1 & AUC |
| `data_downloads.py` | ä¸‹è½½æ•°æ®é›† | æ—  | CIC-IDS2017 |
| `inspect_labels.py` | æ£€æŸ¥æ ‡ç­¾ | Parquet | æ ‡ç­¾åˆ†å¸ƒ |

### å…³é”®æŠ€æœ¯ç‚¹

1. **æ•°æ®æ ¼å¼è½¬æ¢**: å­—èŠ‚ â†’ åå…­è¿›åˆ¶ â†’ æ–‡æœ¬
2. **T5 æ¨¡å‹ä½¿ç”¨**: åˆ†è¯ã€ç¼–ç ã€è¡¨ç¤ºæå–
3. **è¿ç§»å­¦ä¹ **: å†»ç»“ç¼–ç å™¨ + è®­ç»ƒåˆ†ç±»å¤´
4. **è¯„ä¼°æŒ‡æ ‡**: Accuracy, F1, AUC, Confusion Matrix

### ä½¿ç”¨å»ºè®®

1. **åˆå­¦è€…**: ä» `demo_minimal_data.py` å’Œ `demo_pipeline.py` å¼€å§‹
2. **ç ”ç©¶è€…**: ä½¿ç”¨ `eval_with_encoder_head.py` è¿›è¡Œå®Œæ•´è¯„ä¼°
3. **å¼€å‘è€…**: å‚è€ƒ `evaluate_cic_ids.py` é›†æˆåˆ°è‡ªå·±çš„ç³»ç»Ÿ

### å¸¸è§é—®é¢˜

**Q1: æ²¡æœ‰é¢„è®­ç»ƒæƒé‡æ€ä¹ˆåŠï¼Ÿ**
- ä½¿ç”¨ `demo_pipeline.py` ç†è§£æµç¨‹ï¼ˆéšæœºæƒé‡ï¼‰
- å…ˆè®­ç»ƒ PCAP_encoderï¼ˆPart 1ï¼‰è·å–æƒé‡

**Q2: æ•°æ®é›†å¤ªå¤§æ€ä¹ˆåŠï¼Ÿ**
- ä½¿ç”¨ `--sample` å‚æ•°é‡‡æ ·
- ä» `demo_minimal_data.py` ç”Ÿæˆçš„å°æ•°æ®å¼€å§‹

**Q3: å¦‚ä½•æé«˜æ€§èƒ½ï¼Ÿ**
- ä½¿ç”¨æ›´å¤§çš„ T5 æ¨¡å‹ï¼ˆt5-base, t5-largeï¼‰
- å¢åŠ è®­ç»ƒè½®æ•°
- è°ƒæ•´å­¦ä¹ ç‡
- ä½¿ç”¨æ›´å¤šæ•°æ®

**Q4: å¦‚ä½•åœ¨è‡ªå·±çš„æ•°æ®ä¸Šæµ‹è¯•ï¼Ÿ**
- ç¡®ä¿æ•°æ®æ ¼å¼ç¬¦åˆè¦æ±‚ï¼ˆpayload_byte_* åˆ— + attack_label åˆ—ï¼‰
- ä½¿ç”¨ `inspect_labels.py` æ£€æŸ¥æ•°æ®
- è¿è¡Œ `eval_with_encoder_head.py` è¯„ä¼°
