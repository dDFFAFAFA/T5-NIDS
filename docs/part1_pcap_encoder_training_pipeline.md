# Part 1: PCAP_encoder é¢„è®­ç»ƒæ¨¡å‹å®Œæ•´è®­ç»ƒé“¾è·¯

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ•´ä½“æ¶æ„](#æ•´ä½“æ¶æ„)
- [é˜¶æ®µ 1: æ•°æ®é¢„å¤„ç†](#é˜¶æ®µ-1-æ•°æ®é¢„å¤„ç†)
- [é˜¶æ®µ 2: QA æ¨¡å‹è®­ç»ƒ](#é˜¶æ®µ-2-qa-æ¨¡å‹è®­ç»ƒ)
- [é˜¶æ®µ 3: Denoiser æ¨¡å‹è®­ç»ƒ](#é˜¶æ®µ-3-denoiser-æ¨¡å‹è®­ç»ƒ)
- [é˜¶æ®µ 4: åˆ†ç±»ä»»åŠ¡å¾®è°ƒ](#é˜¶æ®µ-4-åˆ†ç±»ä»»åŠ¡å¾®è°ƒ)
- [æ•°æ®æµè½¬æ€»è§ˆ](#æ•°æ®æµè½¬æ€»è§ˆ)

---

## æ¦‚è¿°

PCAP_encoder æ˜¯ä¸€ä¸ªåŸºäº T5 æ¨¡å‹çš„ç½‘ç»œæµé‡è¡¨ç¤ºå­¦ä¹ æ¡†æ¶ï¼Œé€šè¿‡ä¸¤ç§è‡ªç›‘ç£é¢„è®­ç»ƒä»»åŠ¡ï¼ˆé—®ç­”å’Œå»å™ªï¼‰å­¦ä¹ æ•°æ®åŒ…çš„è¯­ä¹‰è¡¨ç¤ºï¼Œæœ€ç»ˆç”¨äºä¸‹æ¸¸çš„åˆ†ç±»ä»»åŠ¡ã€‚

### æ ¸å¿ƒç†å¿µ

å°†ç½‘ç»œæ•°æ®åŒ…è§†ä¸º"æ–‡æœ¬"ï¼Œåˆ©ç”¨ NLP é¢†åŸŸçš„é¢„è®­ç»ƒæŠ€æœ¯ï¼š
- **é—®ç­”ä»»åŠ¡ (QA)**: ä»åå…­è¿›åˆ¶æ•°æ®åŒ…ä¸­æå–ç‰¹å®šå­—æ®µå€¼
- **å»å™ªä»»åŠ¡ (Denoiser)**: é‡å»ºè¢«ç ´åçš„æ•°æ®åŒ…
- **ä¸‹æ¸¸åº”ç”¨**: ä½¿ç”¨å­¦åˆ°çš„è¡¨ç¤ºè¿›è¡Œæµé‡åˆ†ç±»

---

## æ•´ä½“æ¶æ„

```mermaid
graph TB
    A[åŸå§‹ PCAP æ–‡ä»¶] --> B[æ•°æ®é¢„å¤„ç†]
    B --> C[QA æ•°æ®é›† .parquet]
    B --> D[Denoiser æ•°æ®é›† .parquet]
    
    C --> E[QA æ¨¡å‹è®­ç»ƒ]
    D --> F[Denoiser æ¨¡å‹è®­ç»ƒ]
    
    E --> G[é¢„è®­ç»ƒæƒé‡ weights.pth]
    F --> G
    
    G --> H[åˆ†ç±»ä»»åŠ¡å¾®è°ƒ]
    H --> I[æœ€ç»ˆåˆ†ç±»æ¨¡å‹]
    
    style A fill:#e1f5ff
    style C fill:#fff4e1
    style D fill:#fff4e1
    style G fill:#e8f5e9
    style I fill:#f3e5f5
```

---

## é˜¶æ®µ 1: æ•°æ®é¢„å¤„ç†

### 1.1 QA æ•°æ®é›†ç”Ÿæˆ

#### è„šæœ¬ä½ç½®
```
Preprocess/FromPCAPtoQADataset.py
```

#### è¾“å…¥
- **æ ¼å¼**: PCAP æ–‡ä»¶ï¼ˆåŸå§‹ç½‘ç»œæµé‡æ•è·ï¼‰
- **æ¥æº**: Wiresharkã€tcpdump ç­‰å·¥å…·æ•è·çš„ç½‘ç»œæµé‡
- **å†…å®¹**: åŒ…å«å®Œæ•´çš„ç½‘ç»œæ•°æ®åŒ…ï¼ˆEthernetã€IPã€TCP/UDP/ICMP ç­‰å±‚ï¼‰

#### å¤„ç†æµç¨‹

```mermaid
graph LR
    A[PCAP æ–‡ä»¶] --> B[Scapy è§£æ]
    B --> C[æ•°æ®åŒ…å¯¹è±¡]
    C --> D[åŒ¿ååŒ–å¤„ç†]
    D --> E[ç§»é™¤è½½è·]
    E --> F[æå–å­—æ®µ]
    F --> G[åå…­è¿›åˆ¶è½¬æ¢]
    G --> H[ç”Ÿæˆé—®ç­”å¯¹]
    H --> I[Parquet æ–‡ä»¶]
    
    style A fill:#e1f5ff
    style I fill:#fff4e1
```

##### æ­¥éª¤ 1: è¯»å– PCAP æ–‡ä»¶

**å‡½æ•°**: `read_pcap_header(input_path)`

```python
# æµå¼è¯»å–ï¼Œé¿å…å†…å­˜æº¢å‡º
from scapy.all import PcapReader

with PcapReader(pcap_file) as pcap_reader:
    for pkt in pcap_reader:
        # å¤„ç†æ¯ä¸ªæ•°æ®åŒ…
```

**è¾“å‡º**: Scapy æ•°æ®åŒ…å¯¹è±¡åˆ—è¡¨

##### æ­¥éª¤ 2: åŒ¿ååŒ–å¤„ç†

**å‡½æ•°**: `modify_IPv4packets(pkt)`, `modify_IPv6packets(pkt)`

**ç›®çš„**:
- éšç§ä¿æŠ¤ï¼šé¿å…æš´éœ²çœŸå® IP åœ°å€
- æ³›åŒ–èƒ½åŠ›ï¼šé˜²æ­¢æ¨¡å‹è®°ä½ç‰¹å®š IP
- æ•°æ®å¢å¼ºï¼šåŒä¸€ä¸ªåŒ…æ¯æ¬¡å¤„ç† IP éƒ½ä¸åŒ

**å¤„ç†å†…å®¹**:
```python
# IPv4 åŒ¿ååŒ–
pkt[IP].src = generate_rnd_IP()  # éšæœº IPï¼Œå¦‚ "192.168.1.100"
pkt[IP].dst = generate_rnd_IP()
pkt[IP].ttl = random.randint(1, 255)  # éšæœº TTL

# IPv6 åŒ¿ååŒ–
pkt[IPv6].src = generate_rnd_IPv6()  # å¦‚ "2001:0db8:85a3:..."
pkt[IPv6].dst = generate_rnd_IPv6()
pkt[IPv6].hlim = random.randint(1, 255)  # éšæœº Hop Limit
```

**è¾“å…¥**: åŸå§‹æ•°æ®åŒ…
```
IP src=10.0.0.1 dst=10.0.0.2 ttl=64
```

**è¾“å‡º**: åŒ¿ååŒ–æ•°æ®åŒ…
```
IP src=192.168.45.123 dst=172.16.89.201 ttl=128
```

##### æ­¥éª¤ 3: ç§»é™¤è½½è·ï¼ˆå¯é€‰ï¼‰

**å‡½æ•°**: `remove_payload(pkt)`

**é…ç½®**: `PAYLOAD = False`ï¼ˆé»˜è®¤ç§»é™¤ï¼‰

**ç›®çš„**:
- å‡å°‘å™ªå£°ï¼šHTTPS ç­‰åŠ å¯†æµé‡çš„è½½è·æ˜¯éšæœºçš„
- èšç„¦åè®®ï¼šåè®®å¤´éƒ¨åŒ…å«æ›´å¤šç»“æ„åŒ–ä¿¡æ¯
- å‡å°æ•°æ®ï¼šåŠ å¿«è®­ç»ƒé€Ÿåº¦

**å¤„ç†é€»è¾‘**:
```python
if TCP in pkt:
    del pkt[TCP].payload  # ç§»é™¤ TCP è½½è·
elif UDP in pkt:
    del pkt[UDP].payload  # ç§»é™¤ UDP è½½è·
# ICMP ä¸ç§»é™¤ï¼ˆè½½è·é€šå¸¸ä¸åŠ å¯†ï¼‰
```

**è¾“å…¥**: å®Œæ•´æ•°æ®åŒ…ï¼ˆå«è½½è·ï¼‰
```
Ethernet / IP / TCP / Raw(load='GET /index.html HTTP/1.1...')
```

**è¾“å‡º**: ä»…åè®®å¤´
```
Ethernet / IP / TCP
```

##### æ­¥éª¤ 4: æå–å­—æ®µ

**å‡½æ•°**: `pkt2dict(pkt)`

**å¤„ç†**: åˆ©ç”¨ Scapy çš„ `show2()` æ–¹æ³•è§£ææ•°æ®åŒ…

**è¾“å…¥**: Scapy æ•°æ®åŒ…å¯¹è±¡

**è¾“å‡º**: åµŒå¥—å­—å…¸
```python
{
    'Ethernet': {
        'dst': 'ff:ff:ff:ff:ff:ff',
        'src': '00:11:22:33:44:55',
        'type': '0x800'
    },
    'IP': {
        'version': 4,
        'ihl': 5,
        'tos': 0,
        'len': 60,
        'id': 7238,
        'flags': 'DF',
        'frag': 0,
        'ttl': 64,
        'proto': 'tcp',
        'chksum': '0x1c46',
        'src': '192.168.1.1',
        'dst': '192.168.1.2'
    },
    'TCP': {
        'sport': 443,
        'dport': 54321,
        'seq': 1234567890,
        'ack': 987654321,
        'dataofs': 5,
        'flags': 'PA',
        'window': 65535,
        'chksum': '0x5a3c',
        'urgptr': 0
    }
}
```

##### æ­¥éª¤ 5: åå…­è¿›åˆ¶è½¬æ¢

**å‡½æ•°**: `convert_hexadecimal(dict_pkt, pkt)`

**ç›®çš„**: å°†å­—æ®µå€¼è½¬æ¢ä¸ºåå…­è¿›åˆ¶ï¼Œä¸æ•°æ®åŒ…çš„åŸå§‹è¡¨ç¤ºä¸€è‡´

**è½¬æ¢è§„åˆ™**:
```python
# IP åœ°å€: 192.168.1.1 -> c0a8 0101
'192.168.1.1' -> 'c0a80101'

# ç«¯å£å·: 443 -> 01bb
443 -> '01bb'

# åºåˆ—å·: 1234567890 -> 499602d2
1234567890 -> '499602d2'

# æ ‡å¿—ä½: 'PA' -> 18
'PA' -> '18'
```

**è¾“å‡º**: åå…­è¿›åˆ¶å­—å…¸
```python
{
    'IP': {
        'src': 'c0a80101',
        'dst': 'c0a80102',
        'ttl': '40',
        'chksum': '1c46',
        ...
    },
    'TCP': {
        'sport': '01bb',
        'dport': 'd431',
        ...
    }
}
```

##### æ­¥éª¤ 6: ç”Ÿæˆæ•°æ®åŒ…åå…­è¿›åˆ¶å­—ç¬¦ä¸²

**å‡½æ•°**: `bytes_hex(pkt).decode()`

**å¤„ç†**: å°†æ•´ä¸ªæ•°æ®åŒ…è½¬æ¢ä¸ºåå…­è¿›åˆ¶å­—ç¬¦ä¸²

**è¾“å…¥**: Scapy æ•°æ®åŒ…å¯¹è±¡

**è¾“å‡º**: åŸå§‹åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼ˆæ— ç©ºæ ¼ï¼‰
```
4500003c1c46400040067c46c0a80101c0a8010201bbd431499602d23ade32b15018ffff5a3c0000
```

##### æ­¥éª¤ 7: æ ¼å¼åŒ–åå…­è¿›åˆ¶å­—ç¬¦ä¸²

**é…ç½®**: `PKT_FORMAT = "every4"`ï¼ˆé»˜è®¤ï¼‰

**ä¸‰ç§æ ¼å¼**:

| æ ¼å¼ | ç¤ºä¾‹ | è¯´æ˜ |
|------|------|------|
| `every4` | `4500 003c 1c46 4000` | æ¯ 4 å­—ç¬¦ä¸€ç»„ï¼ˆæ¨èï¼‰ |
| `every2` | `45 00 00 3c 1c 46 40 00` | æ¯ 2 å­—ç¬¦ä¸€ç»„ï¼ˆå­—èŠ‚çº§ï¼‰ |
| `noSpace` | `4500003c1c464000` | æ— åˆ†éš”ï¼ˆæœ€ç´§å‡‘ï¼‰ |

**è¾“å‡º**: æ ¼å¼åŒ–çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²
```
4500 003c 1c46 4000 4006 7c46 c0a8 0101 c0a8 0102 01bb d431 4996 02d2 3ade 32b1 5018 ffff 5a3c 0000
```

##### æ­¥éª¤ 8: ç”Ÿæˆé—®ç­”å¯¹

**å‡½æ•°**: `create_list_questions()`, ä¸»å¾ªç¯

**é—®é¢˜æ¨¡æ¿æ–‡ä»¶**: `questionsQA.txt`

**é—®é¢˜ç¤ºä¾‹**:
```
What is the source IP?
What is the destination IP?
What is the source port?
What is the destination port?
What is the TTL?
What is the TCP flags?
What is the IP checksum?
...
```

**ç”Ÿæˆé€»è¾‘**:
```python
for pkt_dict, hex_string in zip(list_dict_values, list_dict_hex):
    for layer in pkt_dict:
        for field in pkt_dict[layer]:
            question = f"What is the {layer}.{field}?"
            context = hex_string  # å®Œæ•´æ•°æ®åŒ…åå…­è¿›åˆ¶
            answer = pkt_dict[layer][field]  # å­—æ®µçš„åå…­è¿›åˆ¶å€¼
            
            # æ·»åŠ åˆ°æ•°æ®é›†
            qa_pairs.append({
                'question': question,
                'context': context,
                'answer': answer,
                'pkt_field': f"{layer}.{field}"
            })
```

**è¾“å‡ºç¤ºä¾‹**:

| question | context | answer | pkt_field |
|----------|---------|--------|-----------|
| What is the source IP? | 4500 003c 1c46 ... | c0a8 0101 | IP.src |
| What is the destination IP? | 4500 003c 1c46 ... | c0a8 0102 | IP.dst |
| What is the source port? | 4500 003c 1c46 ... | 01bb | TCP.sport |
| What is the TCP flags? | 4500 003c 1c46 ... | 18 | TCP.flags |

##### æ­¥éª¤ 9: ä¿å­˜ä¸º Parquet

**è¾“å‡ºæ–‡ä»¶**: `Train_QA.parquet`, `Test_QA.parquet`

**æ•°æ®æ ¼å¼**:
```python
DataFrame columns:
- question: str      # é—®é¢˜
- context: str       # æ•°æ®åŒ…åå…­è¿›åˆ¶å­—ç¬¦ä¸²
- answer: str        # ç­”æ¡ˆï¼ˆåå…­è¿›åˆ¶ï¼‰
- pkt_field: str     # å­—æ®µç±»å‹ï¼ˆç”¨äºåˆ†æï¼‰
```

**æ–‡ä»¶å¤§å°**: å–å†³äº PCAP æ–‡ä»¶å¤§å°ï¼Œé€šå¸¸å‡  MB åˆ°å‡  GB

#### å®Œæ•´æ•°æ®æµ

```
åŸå§‹ PCAP
    â†“
[Scapy è§£æ]
    â†“
æ•°æ®åŒ…å¯¹è±¡: Ethernet/IP/TCP/...
    â†“
[åŒ¿ååŒ–] IP: 10.0.0.1 â†’ 192.168.45.123
    â†“
[ç§»é™¤è½½è·] Ethernet/IP/TCP/Raw â†’ Ethernet/IP/TCP
    â†“
[æå–å­—æ®µ] {'IP': {'src': '192.168.45.123', ...}, 'TCP': {...}}
    â†“
[åå…­è¿›åˆ¶è½¬æ¢] {'IP': {'src': 'c0a82d7b', ...}, 'TCP': {...}}
    â†“
[ç”Ÿæˆåå…­è¿›åˆ¶å­—ç¬¦ä¸²] "4500 003c 1c46 4000 ..."
    â†“
[ç”Ÿæˆé—®ç­”å¯¹]
    question: "What is the source IP?"
    context: "4500 003c 1c46 4000 ..."
    answer: "c0a8 2d7b"
    â†“
Parquet æ–‡ä»¶
```

---

### 1.2 Denoiser æ•°æ®é›†ç”Ÿæˆ

#### è„šæœ¬ä½ç½®
```
Preprocess/FromPCAPtoDenoiserDataset.py
```

#### è¾“å…¥
- **æ ¼å¼**: PCAP æ–‡ä»¶ï¼ˆåŒ QA æ•°æ®é›†ï¼‰

#### å¤„ç†æµç¨‹

ä¸ QA æ•°æ®é›†ç±»ä¼¼ï¼Œä½†æ›´ç®€å•ï¼š

```mermaid
graph LR
    A[PCAP æ–‡ä»¶] --> B[Scapy è§£æ]
    B --> C[åŒ¿ååŒ–å¤„ç†]
    C --> D[ç§»é™¤è½½è·]
    D --> E[ç”Ÿæˆåå…­è¿›åˆ¶]
    E --> F[åˆ†é…å»å™ªé—®é¢˜]
    F --> G[Parquet æ–‡ä»¶]
    
    style A fill:#e1f5ff
    style G fill:#fff4e1
```

##### ä¸»è¦å·®å¼‚

1. **ä¸æå–å­—æ®µ**: åªéœ€è¦å®Œæ•´çš„åå…­è¿›åˆ¶å­—ç¬¦ä¸²
2. **é—®é¢˜æ¨¡æ¿**: ä» `questionsDenoiser.txt` éšæœºé€‰æ‹©

**é—®é¢˜ç¤ºä¾‹**:
```
Reconstruct the original packet
Denoise this network packet
Fix the corrupted packet header
Restore the packet to its original form
```

##### è¾“å‡ºæ ¼å¼

**æ–‡ä»¶**: `Train_Denoiser.parquet`, `Test_Denoiser.parquet`

**æ•°æ®æ ¼å¼**:
```python
DataFrame columns:
- question: str      # å»å™ªæŒ‡ä»¤
- context: str       # æ•°æ®åŒ…åå…­è¿›åˆ¶å­—ç¬¦ä¸²ï¼ˆè®­ç»ƒæ—¶ä¼šè¢«ç ´åï¼‰
```

**ç¤ºä¾‹**:

| question | context |
|----------|---------|
| Reconstruct the original packet | 4500 003c 1c46 4000 4006 7c46 c0a8 0101 ... |
| Denoise this network packet | 4500 0028 a3f2 4000 4006 9960 c0a8 0165 ... |

#### æ•°æ®ç ´åï¼ˆåœ¨è®­ç»ƒæ—¶åŠ¨æ€è¿›è¡Œï¼‰

ç ´åç­–ç•¥åœ¨ `Core/classes/dataset_for_denoiser.py` ä¸­å®ç°ï¼š

```python
# ç ´åç‡ (Corruption Rate)
CR = 15  # 15% çš„ token è¢«ç ´å

# ç ´åæ–¹æ³•
åŸå§‹: "4500 003c 1c46 4000 4006 7c46"
ç ´å: "4500 <extra_id_0> 1c46 <extra_id_1> 4006 7c46"

# T5 çš„å»å™ªæ ¼å¼
è¾“å…¥: "4500 <extra_id_0> 1c46 <extra_id_1> 4006 7c46"
ç›®æ ‡: "<extra_id_0> 003c <extra_id_1> 4000 <extra_id_2>"
```

---

## é˜¶æ®µ 2: QA æ¨¡å‹è®­ç»ƒ

### 2.1 è®­ç»ƒè„šæœ¬

#### ä½ç½®
```
2.Training/QA/train.py
Experiments/4_QA_model_training/T5QandA.sh
```

### 2.2 è¾“å…¥

#### æ•°æ®æ–‡ä»¶
- **è®­ç»ƒé›†**: `1.Datasets/QA/Train_QA.parquet`
- **æµ‹è¯•é›†**: `1.Datasets/QA/Test_QA.parquet`

#### é…ç½®å‚æ•°ï¼ˆæ¥è‡ª `.sh` è„šæœ¬ï¼‰

```bash
# æ¨¡å‹é…ç½®
MODEL_NAME="T5-base"              # T5 æ¨¡å‹åç§°
TOKENIZER_NAME="T5-base"          # åˆ†è¯å™¨åç§°
BOTTLENECK="mean"                 # ç“¶é¢ˆå±‚ç±»å‹: mean/first/last

# è®­ç»ƒé…ç½®
BATCH_SIZE=4                      # æ‰¹å¤§å°
EPOCHS=2                          # è®­ç»ƒè½®æ•°
LR=0.0005                         # å­¦ä¹ ç‡
MAX_QST_LENGTH=512                # é—®é¢˜+ä¸Šä¸‹æ–‡æœ€å¤§é•¿åº¦
MAX_ANS_LENGTH=32                 # ç­”æ¡ˆæœ€å¤§é•¿åº¦
PERC=1                            # ä½¿ç”¨æ•°æ®é›†çš„ç™¾åˆ†æ¯” (1-100)
SEED=43                           # éšæœºç§å­

# è¾“å…¥æ ¼å¼
INPUT_FORMAT="every4"             # åå…­è¿›åˆ¶æ ¼å¼
```

### 2.3 è®­ç»ƒæµç¨‹

```mermaid
graph TB
    A[Parquet æ•°æ®] --> B[QA_Dataset åŠ è½½]
    B --> C[æ•°æ®é›†åˆ’åˆ†]
    C --> D[Train 80%]
    C --> E[Val 20%]
    
    D --> F[T5 Tokenizer]
    E --> F
    
    F --> G[Token IDs]
    G --> H[T5 Encoder-Decoder]
    
    H --> I[Teacher Forcing]
    I --> J[è®¡ç®— Loss]
    J --> K[åå‘ä¼ æ’­]
    K --> L[æ›´æ–°æƒé‡]
    
    L --> M{éªŒè¯}
    M -->|ç»§ç»­| H
    M -->|æœ€ä½³| N[ä¿å­˜æ¨¡å‹]
    
    style A fill:#fff4e1
    style N fill:#e8f5e9
```

#### æ­¥éª¤ 1: æ•°æ®åŠ è½½

**ç±»**: `Core/classes/dataset_for_QA.py` - `QA_Dataset`

```python
# åŠ è½½ Parquet æ–‡ä»¶
dataset_obj = QA_Dataset(opts, tokenizer_obj)
dataset_obj.load_dataset(
    input_path="Train_QA.parquet",
    test_path="Test_QA.parquet",
    format="every4"
)

# æ•°æ®æ ¼å¼åŒ–
# ç§»é™¤ç©ºæ ¼
context = context.replace(" ", "")  # "4500003c1c46..." 

# é‡æ–°åˆ†ç»„ï¼ˆæ ¹æ® formatï¼‰
if format == "every4":
    context = ' '.join([context[i:i+4] for i in range(0, len(context), 4)])
    # "4500 003c 1c46 ..."
```

**è¾“å…¥**: Parquet æ–‡ä»¶
```python
{
    'question': 'What is the source IP?',
    'context': '4500003c1c464000...',  # æ— ç©ºæ ¼
    'answer': 'c0a80101',
    'pkt_field': 'IP.src'
}
```

**è¾“å‡º**: æ ¼å¼åŒ–æ•°æ®
```python
{
    'question': 'What is the source IP?',
    'context': '4500 003c 1c46 4000 ...',  # æ¯4å­—ç¬¦ä¸€ç»„
    'answer': 'c0a80101'
}
```

#### æ­¥éª¤ 2: æ•°æ®é›†åˆ’åˆ†

**æ–¹æ³•**: `split_train_val_test(percentage)`

```python
# ä½¿ç”¨æŒ‡å®šç™¾åˆ†æ¯”çš„æ•°æ®
num_rows = int(len(data) * percentage / 100)
data_sampled = data.sample(n=num_rows, random_state=seed)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›† (80/20)
train_data, val_data = train_test_split(
    data_sampled, 
    test_size=0.2, 
    random_state=seed
)
```

**è¾“å‡º**:
- è®­ç»ƒé›†: 80% çš„æ•°æ®
- éªŒè¯é›†: 20% çš„æ•°æ®
- æµ‹è¯•é›†: ç‹¬ç«‹çš„ Test_QA.parquet

#### æ­¥éª¤ 3: åˆ†è¯ç¼–ç 

**ç±»**: `Core/classes/tokenizer.py` - `QA_Tokenizer_T5`

**T5 è¾“å…¥æ ¼å¼**:
```python
# é—®é¢˜ + ä¸Šä¸‹æ–‡
input_text = f"question: {question} context: {context}"

# ç¤ºä¾‹
"question: What is the source IP? context: 4500 003c 1c46 4000 ..."
```

**åˆ†è¯è¿‡ç¨‹**:
```python
# ç¼–ç è¾“å…¥
question_tokenized = tokenizer.tokenize_question(question, context)
# è¿”å›:
{
    'input_ids': [1, 822, 19, 8, 1391, 2465, 58, ...],  # Token IDs
    'attention_mask': [1, 1, 1, 1, 1, 1, 1, ...]        # æ³¨æ„åŠ›æ©ç 
}

# ç¼–ç ç­”æ¡ˆ
answer_tokenized = tokenizer.tokenize_answer(answer)
# è¿”å›:
{
    'input_ids': [3, 75, 632, 505, 3, 632, 3, 2, 3, 4, 3, 2, 1],
    'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```

**è¾“å‡º**: æ¨¡å‹è¾“å…¥æ ¼å¼
```python
{
    'input_ids': Tensor([1, 822, 19, 8, ...]),           # ç¼–ç å™¨è¾“å…¥
    'attention_mask': Tensor([1, 1, 1, 1, ...]),         # ç¼–ç å™¨æ³¨æ„åŠ›
    'labels': Tensor([3, 75, 632, 505, ...]),            # è§£ç å™¨ç›®æ ‡
    'decoder_attention_mask': Tensor([1, 1, 1, ...]),    # è§£ç å™¨æ³¨æ„åŠ›
    'decoder_input_ids': Tensor([0, 3, 75, 632, ...])    # è§£ç å™¨è¾“å…¥
}
```

#### æ­¥éª¤ 4: æ¨¡å‹å®šä¹‰

**ç±»**: `Core/classes/T5_model.py` - `T5_PCAP_translator`

```python
# åŠ è½½é¢„è®­ç»ƒ T5 æ¨¡å‹
from transformers import T5ForConditionalGeneration

model = T5ForConditionalGeneration.from_pretrained("T5-base")

# T5-base æ¶æ„
- Encoder: 12 å±‚ Transformer
- Decoder: 12 å±‚ Transformer
- Hidden size: 768
- Attention heads: 12
- Parameters: ~220M
```

**ç“¶é¢ˆå±‚é…ç½®**:
```python
# ç”¨äºæå–æ•°æ®åŒ…è¡¨ç¤º
if bottleneck == "mean":
    # å¹³å‡æ± åŒ–æ‰€æœ‰ token
    representation = hidden_states.mean(dim=1)  # [batch, 768]
elif bottleneck == "first":
    # ä½¿ç”¨ç¬¬ä¸€ä¸ª token (ç±»ä¼¼ BERT [CLS])
    representation = hidden_states[:, 0, :]     # [batch, 768]
elif bottleneck == "last":
    # ä½¿ç”¨æœ€åä¸€ä¸ª token
    representation = hidden_states[:, -1, :]    # [batch, 768]
```

#### æ­¥éª¤ 5: è®­ç»ƒå¾ªç¯

**æ–¹æ³•**: `start_training()`

```python
for epoch in range(num_epochs):
    for batch in train_loader:
        # å‰å‘ä¼ æ’­
        outputs = model(
            input_ids=batch['input_ids'],
            attention_mask=batch['attention_mask'],
            labels=batch['labels'],
            decoder_attention_mask=batch['decoder_attention_mask']
        )
        
        # è®¡ç®—æŸå¤± (Cross-Entropy)
        loss = outputs.loss
        
        # åå‘ä¼ æ’­
        accelerator.backward(loss)
        
        # æ›´æ–°å‚æ•°
        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        
    # éªŒè¯
    val_loss = validation_batch(model, val_loader)
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_loss < best_loss:
        save_model(model, "best_model")
```

**æŸå¤±å‡½æ•°**: Cross-Entropy Loss
```python
# T5 ä½¿ç”¨ Teacher Forcing
# ç»™å®šè¾“å…¥ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ª token

è¾“å…¥åºåˆ—: "question: What is the source IP? context: 4500 003c ..."
ç›®æ ‡åºåˆ—: "c0a8 0101"

# æ¨¡å‹é¢„æµ‹æ¯ä¸ªä½ç½®çš„ token æ¦‚ç‡åˆ†å¸ƒ
# æŸå¤± = -log P(æ­£ç¡® token)
```

#### æ­¥éª¤ 6: éªŒè¯å’Œä¿å­˜

**éªŒè¯æŒ‡æ ‡**:
```python
# 1. éªŒè¯æŸå¤±
val_loss = average_loss_on_validation_set

# 2. å‡†ç¡®ç‡ï¼ˆæŒ‰å­—æ®µç±»å‹ï¼‰
accuracy_by_field = {
    'IP.src': 0.95,
    'IP.dst': 0.94,
    'TCP.sport': 0.92,
    'TCP.dport': 0.91,
    ...
}

# 3. æ•´ä½“å‡†ç¡®ç‡
overall_accuracy = correct_predictions / total_predictions
```

**æ¨¡å‹ä¿å­˜**:
```python
# ä¿å­˜è·¯å¾„
output_path = "results/TrainingQA/Denoiser0450K_QA_Hard_mean_seed43_512/
               task-supervised_lr-0.0005_epochs-20_batch-24/seed_43/best_model"

# ä¿å­˜å†…å®¹
- pytorch_model.bin       # æ¨¡å‹æƒé‡
- config.json             # æ¨¡å‹é…ç½®
- training_args.bin       # è®­ç»ƒå‚æ•°
```

### 2.4 è¾“å‡º

#### è®­ç»ƒå¥½çš„æ¨¡å‹
- **ä½ç½®**: `results/.../best_model/`
- **æ ¼å¼**: PyTorch æ¨¡å‹æ–‡ä»¶
- **å¤§å°**: ~220M (T5-base)

#### è®­ç»ƒæ—¥å¿—
```
Epoch 1/2:
  Train Loss: 2.345
  Val Loss: 1.987
  Accuracy: 0.856

Epoch 2/2:
  Train Loss: 1.654
  Val Loss: 1.543
  Accuracy: 0.912

Best model saved at epoch 2
```

#### æ€§èƒ½æŒ‡æ ‡
```python
{
    'best_val_loss': 1.543,
    'best_epoch': 2,
    'accuracy_by_field': {
        'IP.src': 0.95,
        'IP.dst': 0.94,
        'TCP.sport': 0.92,
        ...
    },
    'overall_accuracy': 0.912
}
```

---

## é˜¶æ®µ 3: Denoiser æ¨¡å‹è®­ç»ƒ

### 3.1 è®­ç»ƒè„šæœ¬

#### ä½ç½®
```
2.Training/Denoiser/train.py
Experiments/3_denoiser_training/T5denoiser.sh
```

### 3.2 è¾“å…¥

#### æ•°æ®æ–‡ä»¶
- **è®­ç»ƒé›†**: `1.Datasets/denoiser/Train_Denoiser.parquet`
- **æµ‹è¯•é›†**: `1.Datasets/denoiser/Test_Denoiser.parquet`

#### é…ç½®å‚æ•°

```bash
# æ¨¡å‹é…ç½®
MODEL_NAME="T5-base"
TOKENIZER_NAME="T5-base"
BOTTLENECK="mean"

# è®­ç»ƒé…ç½®
BATCH_SIZE=2                      # Denoiser æ‰¹å¤§å°é€šå¸¸æ›´å°
EPOCHS=2
LR=0.0005
MAX_QST_LENGTH=512
MAX_ANS_LENGTH=512                # Denoiser ç­”æ¡ˆæ›´é•¿ï¼ˆå®Œæ•´æ•°æ®åŒ…ï¼‰
PERC=1
SEED=43

# Denoiser ç‰¹æœ‰å‚æ•°
CR=(0 15 30)                      # ç ´åç‡ (Corruption Rate)
```

### 3.3 è®­ç»ƒæµç¨‹

#### æ­¥éª¤ 1: æ•°æ®åŠ è½½

**ç±»**: `Core/classes/dataset_for_denoiser.py` - `Denoiser_Dataset`

```python
dataset_obj = Denoiser_Dataset(opts, tokenizer_obj, corruption_rate=15)
dataset_obj.load_dataset(
    input_path="Train_Denoiser.parquet",
    test_path="Test_Denoiser.parquet",
    format="every4"
)
```

#### æ­¥éª¤ 2: åŠ¨æ€æ•°æ®ç ´å

**åœ¨ `__getitem__` ä¸­å®ç°**:

```python
def __getitem__(self, idx):
    question = self.questions[idx]  # "Reconstruct the original packet"
    context = self.context[idx]     # "4500 003c 1c46 4000 ..."
    
    # åŠ¨æ€ç ´å
    corrupted_context, target = self.corrupt(context, corruption_rate=15)
    
    # è¿”å›
    return {
        'input': f"{question} {corrupted_context}",
        'target': target
    }
```

**ç ´åç¤ºä¾‹**:

```python
# åŸå§‹æ•°æ®åŒ…
original = "4500 003c 1c46 4000 4006 7c46 c0a8 0101 c0a8 0102"

# ç ´å 15% çš„ token
corrupted = "4500 <extra_id_0> 1c46 4000 <extra_id_1> 7c46 c0a8 0101 <extra_id_2> 0102"

# T5 ç›®æ ‡æ ¼å¼
target = "<extra_id_0> 003c <extra_id_1> 4006 <extra_id_2> c0a8 <extra_id_3>"
```

**ç ´åç‡å¯¹æ¯”**:

| CR | åŸå§‹ | ç ´åå |
|----|------|--------|
| 0% | `4500 003c 1c46 4000` | `4500 003c 1c46 4000` (æ— ç ´å) |
| 15% | `4500 003c 1c46 4000` | `4500 <extra_id_0> 1c46 4000` |
| 30% | `4500 003c 1c46 4000` | `<extra_id_0> 003c <extra_id_1> 4000` |

#### æ­¥éª¤ 3: è®­ç»ƒ

**ä¸ QA è®­ç»ƒç±»ä¼¼**ï¼Œä½†ç›®æ ‡æ˜¯é‡å»ºå®Œæ•´æ•°æ®åŒ…ï¼š

```python
# è¾“å…¥
input_text = "Reconstruct the original packet 4500 <extra_id_0> 1c46 ..."

# ç›®æ ‡
target_text = "<extra_id_0> 003c <extra_id_1> 4006 ..."

# è®­ç»ƒ
loss = model(input_ids, labels=target_ids).loss
```

**æŸå¤±å‡½æ•°**: åŒæ ·æ˜¯ Cross-Entropy Loss

### 3.4 è¾“å‡º

#### è®­ç»ƒå¥½çš„æ¨¡å‹
- **ä½ç½®**: `results/.../best_model/`
- **èƒ½åŠ›**: èƒ½å¤Ÿé‡å»ºè¢«ç ´åçš„æ•°æ®åŒ…

#### æ€§èƒ½æŒ‡æ ‡
```python
{
    'best_val_loss': 0.876,
    'reconstruction_accuracy': 0.934,  # é‡å»ºå‡†ç¡®ç‡
    'token_accuracy': 0.967            # Token çº§å‡†ç¡®ç‡
}
```

---

## é˜¶æ®µ 4: åˆ†ç±»ä»»åŠ¡å¾®è°ƒ

### 4.1 æ¦‚è¿°

ä½¿ç”¨é¢„è®­ç»ƒçš„ QA æˆ– Denoiser æ¨¡å‹çš„ç¼–ç å™¨ï¼Œæ·»åŠ åˆ†ç±»å¤´è¿›è¡Œå¾®è°ƒã€‚

### 4.2 æ¶æ„

```mermaid
graph LR
    A[æ•°æ®åŒ…åå…­è¿›åˆ¶] --> B[T5 Tokenizer]
    B --> C[Token IDs]
    C --> D[T5 Encoder å†»ç»“]
    D --> E[Hidden States]
    E --> F[Bottleneck Layer]
    F --> G[Linear Classifier]
    G --> H[Class Logits]
    
    style D fill:#e8f5e9
    style G fill:#f3e5f5
```

### 4.3 è¾“å…¥

#### æ•°æ®æ ¼å¼
```python
# åˆ†ç±»æ•°æ®é›†
{
    'payload_byte_1': 69,
    'payload_byte_2': 112,
    ...,
    'payload_byte_N': 45,
    'attack_label': 'FTP-Patator'  # æˆ– 'BENIGN', 'SSH-Patator', ...
}
```

#### é¢„è®­ç»ƒæƒé‡
- **è·¯å¾„**: `results/.../best_model/pytorch_model.bin`
- **æ¥æº**: QA æˆ– Denoiser è®­ç»ƒçš„æœ€ä½³æ¨¡å‹

### 4.4 è®­ç»ƒæµç¨‹

#### æ­¥éª¤ 1: æ•°æ®è½¬æ¢

```python
# å­—èŠ‚æ•°ç»„ â†’ åå…­è¿›åˆ¶å­—ç¬¦ä¸²
bytes_array = [69, 112, 45, 201, ...]
hex_string = ''.join(f'{b:02x}' for b in bytes_array)
# "456c2dc9..."

# æ ¼å¼åŒ–
hex_formatted = ' '.join(hex_string[i:i+4] for i in range(0, len(hex_string), 4))
# "456c 2dc9 ..."

# æ„é€ è¾“å…¥
input_text = f"question: Classify the network packet context: {hex_formatted}"
```

#### æ­¥éª¤ 2: åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨

```python
# åŠ è½½å®Œæ•´çš„ T5 æ¨¡å‹
model = T5ForConditionalGeneration.from_pretrained(pretrained_path)

# æå–ç¼–ç å™¨
encoder = model.encoder

# å†»ç»“ç¼–ç å™¨å‚æ•°
for param in encoder.parameters():
    param.requires_grad = False
```

#### æ­¥éª¤ 3: æ·»åŠ åˆ†ç±»å¤´

```python
# çº¿æ€§åˆ†ç±»å™¨
num_classes = len(unique_labels)  # å¦‚ 3: BENIGN, FTP-Patator, SSH-Patator
hidden_size = 768  # T5-base çš„éšè—ç»´åº¦

classifier = nn.Linear(hidden_size, num_classes)

# å®Œæ•´æ¨¡å‹
class ClassificationModel(nn.Module):
    def __init__(self, encoder, classifier, bottleneck='mean'):
        super().__init__()
        self.encoder = encoder
        self.classifier = classifier
        self.bottleneck = bottleneck
    
    def forward(self, input_ids, attention_mask):
        # ç¼–ç 
        outputs = self.encoder(input_ids, attention_mask)
        hidden = outputs.last_hidden_state  # [batch, seq_len, 768]
        
        # ç“¶é¢ˆå±‚
        if self.bottleneck == 'mean':
            representation = hidden.mean(dim=1)  # [batch, 768]
        elif self.bottleneck == 'first':
            representation = hidden[:, 0, :]
        
        # åˆ†ç±»
        logits = self.classifier(representation)  # [batch, num_classes]
        return logits
```

#### æ­¥éª¤ 4: è®­ç»ƒåˆ†ç±»å¤´

```python
# åªè®­ç»ƒåˆ†ç±»å¤´
optimizer = Adam(classifier.parameters(), lr=1e-3)

for epoch in range(epochs):
    for batch in train_loader:
        # å‰å‘ä¼ æ’­
        logits = model(batch['input_ids'], batch['attention_mask'])
        
        # è®¡ç®—æŸå¤±
        loss = CrossEntropyLoss()(logits, batch['labels'])
        
        # åå‘ä¼ æ’­ï¼ˆåªæ›´æ–°åˆ†ç±»å¤´ï¼‰
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

### 4.5 è¾“å‡º

#### è®­ç»ƒå¥½çš„åˆ†ç±»æ¨¡å‹
- **ç¼–ç å™¨**: å†»ç»“çš„é¢„è®­ç»ƒ T5 ç¼–ç å™¨
- **åˆ†ç±»å¤´**: è®­ç»ƒå¥½çš„çº¿æ€§å±‚

#### æ€§èƒ½æŒ‡æ ‡
```python
{
    'accuracy': 0.923,
    'f1_macro': 0.915,
    'f1_weighted': 0.921,
    'confusion_matrix': [[45, 2, 1], [3, 38, 2], [1, 2, 42]],
    'classification_report': {
        'BENIGN': {'precision': 0.92, 'recall': 0.94, 'f1': 0.93},
        'FTP-Patator': {'precision': 0.90, 'recall': 0.88, 'f1': 0.89},
        'SSH-Patator': {'precision': 0.93, 'recall': 0.93, 'f1': 0.93}
    }
}
```

---

## æ•°æ®æµè½¬æ€»è§ˆ

### å®Œæ•´æ•°æ®æµå›¾

```mermaid
graph TB
    subgraph "é˜¶æ®µ 1: æ•°æ®é¢„å¤„ç†"
        A1[åŸå§‹ PCAP] --> A2[Scapy è§£æ]
        A2 --> A3[åŒ¿ååŒ– + ç§»é™¤è½½è·]
        A3 --> A4{æ•°æ®é›†ç±»å‹}
        A4 -->|QA| A5[æå–å­—æ®µ + ç”Ÿæˆé—®ç­”]
        A4 -->|Denoiser| A6[ç”Ÿæˆåå…­è¿›åˆ¶]
        A5 --> A7[QA.parquet]
        A6 --> A8[Denoiser.parquet]
    end
    
    subgraph "é˜¶æ®µ 2: QA è®­ç»ƒ"
        A7 --> B1[QA_Dataset]
        B1 --> B2[T5 Tokenizer]
        B2 --> B3[T5 Model]
        B3 --> B4[è®­ç»ƒå¾ªç¯]
        B4 --> B5[QA æƒé‡]
    end
    
    subgraph "é˜¶æ®µ 3: Denoiser è®­ç»ƒ"
        A8 --> C1[Denoiser_Dataset]
        C1 --> C2[åŠ¨æ€ç ´å]
        C2 --> C3[T5 Tokenizer]
        C3 --> C4[T5 Model]
        C4 --> C5[è®­ç»ƒå¾ªç¯]
        C5 --> C6[Denoiser æƒé‡]
    end
    
    subgraph "é˜¶æ®µ 4: åˆ†ç±»å¾®è°ƒ"
        D1[åˆ†ç±»æ•°æ®] --> D2[è½¬æ¢ä¸ºåå…­è¿›åˆ¶]
        D2 --> D3[T5 Tokenizer]
        B5 -.åŠ è½½.-> D4[T5 Encoder å†»ç»“]
        C6 -.åŠ è½½.-> D4
        D3 --> D4
        D4 --> D5[Linear Classifier]
        D5 --> D6[è®­ç»ƒåˆ†ç±»å¤´]
        D6 --> D7[æœ€ç»ˆåˆ†ç±»æ¨¡å‹]
    end
    
    style A1 fill:#e1f5ff
    style A7 fill:#fff4e1
    style A8 fill:#fff4e1
    style B5 fill:#e8f5e9
    style C6 fill:#e8f5e9
    style D7 fill:#f3e5f5
```

### æ•°æ®æ ¼å¼å˜åŒ–

| é˜¶æ®µ | è¾“å…¥æ ¼å¼ | è¾“å‡ºæ ¼å¼ | æ•°æ®ç¤ºä¾‹ |
|------|----------|----------|----------|
| **PCAP è§£æ** | äºŒè¿›åˆ¶ PCAP | Scapy å¯¹è±¡ | `Ethernet/IP/TCP` |
| **åŒ¿ååŒ–** | Scapy å¯¹è±¡ | Scapy å¯¹è±¡ | `IP.src: 10.0.0.1 â†’ 192.168.1.1` |
| **å­—æ®µæå–** | Scapy å¯¹è±¡ | å­—å…¸ | `{'IP': {'src': '192.168.1.1'}}` |
| **åå…­è¿›åˆ¶è½¬æ¢** | å­—å…¸ | åå…­è¿›åˆ¶å­—å…¸ | `{'IP': {'src': 'c0a80101'}}` |
| **QA ç”Ÿæˆ** | åå…­è¿›åˆ¶å­—å…¸ + å­—ç¬¦ä¸² | Parquet | `{question, context, answer}` |
| **Denoiser ç”Ÿæˆ** | åå…­è¿›åˆ¶å­—ç¬¦ä¸² | Parquet | `{question, context}` |
| **åˆ†è¯** | æ–‡æœ¬ | Token IDs | `[1, 822, 19, 8, ...]` |
| **ç¼–ç ** | Token IDs | Hidden States | `Tensor([batch, seq, 768])` |
| **ç“¶é¢ˆå±‚** | Hidden States | è¡¨ç¤ºå‘é‡ | `Tensor([batch, 768])` |
| **åˆ†ç±»** | è¡¨ç¤ºå‘é‡ | Logits | `Tensor([batch, num_classes])` |

### å…³é”®è½¬æ¢ç‚¹

#### 1. PCAP â†’ åå…­è¿›åˆ¶å­—ç¬¦ä¸²

```
äºŒè¿›åˆ¶æ•°æ®åŒ…
    â†“
Scapy è§£æ
    â†“
Python å¯¹è±¡: Ethernet/IP/TCP
    â†“
bytes_hex()
    â†“
"4500003c1c464000..."
    â†“
æ ¼å¼åŒ– (every4)
    â†“
"4500 003c 1c46 4000 ..."
```

#### 2. åå…­è¿›åˆ¶å­—ç¬¦ä¸² â†’ Token IDs

```
"question: What is the source IP? context: 4500 003c ..."
    â†“
T5 Tokenizer
    â†“
[1, 822, 19, 8, 1391, 2465, 58, 2625, 58, 314, 305, 305, 3, 632, ...]
```

#### 3. Token IDs â†’ æ•°æ®åŒ…è¡¨ç¤º

```
Token IDs: [1, 822, 19, ...]
    â†“
T5 Encoder (12 å±‚ Transformer)
    â†“
Hidden States: [batch, seq_len, 768]
    â†“
Bottleneck (mean pooling)
    â†“
Representation: [batch, 768]
```

#### 4. æ•°æ®åŒ…è¡¨ç¤º â†’ åˆ†ç±»ç»“æœ

```
Representation: [batch, 768]
    â†“
Linear Classifier
    â†“
Logits: [batch, num_classes]
    â†“
Softmax
    â†“
Probabilities: [0.05, 0.92, 0.03]
    â†“
Argmax
    â†“
Predicted Class: 1 (FTP-Patator)
```

---

## æ€»ç»“

### æ ¸å¿ƒæµç¨‹

1. **æ•°æ®é¢„å¤„ç†**: PCAP â†’ Parquet (QA + Denoiser)
2. **QA è®­ç»ƒ**: å­¦ä¹ ä»åå…­è¿›åˆ¶ä¸­æå–å­—æ®µ
3. **Denoiser è®­ç»ƒ**: å­¦ä¹ é‡å»ºè¢«ç ´åçš„æ•°æ®åŒ…
4. **åˆ†ç±»å¾®è°ƒ**: ä½¿ç”¨å­¦åˆ°çš„è¡¨ç¤ºè¿›è¡Œåˆ†ç±»

### å…³é”®æŠ€æœ¯

- **T5 æ¨¡å‹**: Text-to-Text æ¡†æ¶
- **è‡ªç›‘ç£å­¦ä¹ **: æ— éœ€å¤§é‡æ ‡æ³¨æ•°æ®
- **è¿ç§»å­¦ä¹ **: é¢„è®­ç»ƒ â†’ å¾®è°ƒ
- **æ•°æ®å¢å¼º**: åŒ¿ååŒ–ã€åŠ¨æ€ç ´å

### è¾“å…¥è¾“å‡ºæ€»ç»“

| é˜¶æ®µ | è¾“å…¥ | è¾“å‡º |
|------|------|------|
| æ•°æ®é¢„å¤„ç† | PCAP æ–‡ä»¶ | Parquet æ–‡ä»¶ (QA + Denoiser) |
| QA è®­ç»ƒ | QA.parquet | é¢„è®­ç»ƒæƒé‡ (weights.pth) |
| Denoiser è®­ç»ƒ | Denoiser.parquet | é¢„è®­ç»ƒæƒé‡ (weights.pth) |
| åˆ†ç±»å¾®è°ƒ | åˆ†ç±»æ•°æ® + é¢„è®­ç»ƒæƒé‡ | åˆ†ç±»æ¨¡å‹ |
